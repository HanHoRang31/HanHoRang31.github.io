[{"categories":null,"contents":"","date":"Mar 05","permalink":"https://HanHoRang31.github.io/projects/a_project/","tags":null,"title":"HanHoRang Git Repo"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 5주차 시간에는 모임장님께서 쿠버네티스 보안을 주제로 학습 내용을 공유해 주셨다. 이번 블로그 글에서는 쿠버네티스 보안에 대해 스터디한 내용을 공유하고자 한다.\nKubernetes 4C Layer 쿠버네티스 공식문서에 따르면 쿠버네티스 보안은 4계층(클라우드/ 클러스터 / 컨테이너 / 코드)으로 구성되며, 각 계층에 대해 보안 관점이 필요하다고 한다.\nhttps://kubernetes.io/docs/concepts/security/overview/\nCloud, Infra : 클라우드 계층에서는 쿠버네티스 클러스터가 실행되는 기반 인프라에 초점을 맞춘다. 이 계층에서는 가상 머신, 네트워크, 스토리지 및 기타 자원에 대한 보안을 강화하고, 클라우드 제공 업체의 보안 도구 및 기능을 활용하는 것으로 초점이 맞춰져 있다. 인프라 보호 대상과 클라우드 제공 업체(AWS) 의 제공 보안은 다음의 표를 참고하자. 보호 대상 AWS EKS에서의 보안 기능 API 서버에 대한 네트워크 액세스 클러스터 구축시 액세스 지점에 대해 Elastic Load Balancer를 구성한다. 이를 통해 API 서버에 대한 액세스를 VPC 내부로 제한하거나 인터넷 게이트웨이를 통해 인터넷에서도 접근이 가능하도록 설정할 수 있다. 또한, AWS Identity and Access Management (IAM)을 사용하여 사용자와 역할에 대한 접근 제어를 구성할 수 있다. 노드에 대한 네트워크 액세스 VPC 서비스를 이용하여 노드에 대한 액세스를 VPC 내부로 제한하거나 인터넷 게이트웨이를 통해 인터넷에서도 접근이 가능하도록 설정할 수 있다. 또한 보안 그룹을 통해 IP 주소 범위에 대한 인바운드 및 아운바운드 트래픽을 제어할 수 있다. 클라우드 제공 업체 API 에 대한 쿠버네티스 액세스 IAM 권한의 접근 키를 다룬다. 접근 키가 탈취되면 해당 리소스에 대한 제어가 탈취된다. 공식 문서에서는 최소 권한 원칙을 액세스 권한을 부여할 것을 권고한다. etcd에 대한 액세스 etcd 데이터베이스가 EKS 완전 관리형 컨트롤 플레인 내부에 숨겨져 있으며, 사용자는 etcd에 직접 액세스할 수 없다. etcd 암호화 기본적으로 EKS 클러스터를 생성할 때, etcd 데이터를 암호화하는 옵션을 선택할 수 있다. 이렇게 설정하면, 클러스터의 etcd 데이터는 AWS Key Management Service (KMS)에서 제공하는 고객 관리형 키 (CMK)를 사용하여 암호화되어 기밀성이 보장된다. Cluster: 클러스터 계층에서는 쿠버네티스 클러스터 자체의 보안에 중점을 둔다. 클러스터 상의 중요한 서비스 A와 보안이 취약한 서비스 B에 대해 격리를 위한 계층이라고 보면 된다. 쿠버네티스에서는 격리에 방법으로 다양한 방법을 제안한다. 대표적으로 RBAC 인증, 네트워크 정책, 파드 보안 표준, 인그래스용 TLS 등이 있다. 항목 설명 RBAC 인증 역할(Role) 및 클러스터 역할(ClusterRole)을 사용하여 쿠버네티스 사용자와 서비스 계정에 권한을 부여하는 Role-Based Access Control 방식이다. 이를 통해 세분화된 권한 제어로 클러스터의 보안을 강화할 수 있다. 네트워크 정책 쿠버네티스 네트워크 정책은 특정 파드, 네임스페이스, 또는 IP 범위와 같은 소스로부터 들어오거나 나가는 트래픽을 허용하거나 차단하는 규칙을 정의한다. 이를 통해 네트워크 보안을 강화하고, 민감한 데이터를 처리하는 파드에 대한 접근을 제한할 수 있다. 파드 보안 표준 파드 보안 표준은 컨테이너와 파드가 안전하게 실행되도록 하는데 도움이 되는 일련의 보안 지침 및 구성이다. 예를 들어, 보안 컨텍스트(Security Context), 네트워크 폴리시, 리소스 제한, 파드 안티-어피니티 등을 사용하여 파드의 보안을 강화할 수 있다. 인그래스용 TLS 쿠버네티스 인그래스를 사용하여 클러스터 외부에서 내부 서비스로의 요청을 중앙 집중식으로 관리할 때, TLS(Transport Layer Security)를 사용하여 클라이언트와 서버 간 통신을 암호화하여 데이터를 보호할 수 있다. 인증서와 개인 키를 제공하고 호스트 이름과 인증서의 일치 여부를 확인해야 한다. Container: 컨테이너 계층에서는 실행되는 워크로드에 직접적으로 영향을 주는 컨테이너에 초점을 맞춘다. 이 계층에서는 컨테이너 이미지 보안, 리소스 격리, 시크릿 관리 및 네트워크 정책을 포함된다. 여기에서는 보안 컨텍스트, 네트워크 폴리시, 컨테이너 런타임의 보안 기능(이미지 스캔) 등을 사용하여 컨테이너의 보안을 강화한다. 항목 설명 보안 컨텍스트 (Security Context) 쿠버네티스에서 컨테이너 또는 파드에 적용되는 보안 설정을 정의하는데 사용된다. 보안 컨텍스트를 사용하여 컨테이너의 파일 시스템에 대한 액세스 권한, 프로세스 ID, 사용자 ID, 그룹 ID 등을 제어할 수 있다. 이를 통해 컨테이너와 파드가 안전하게 실행되도록 할 수 있으며, 호스트 시스템과 다른 컨테이너로부터 분리되어 보안을 강화할 수 있다. 컨테이너 런타임의 보안 기능 컨테이너 런타임(예: Docker, containerd, CRI-O 등)은 컨테이너를 실행하고 관리하는데 사용되는 소프트웨어이다. 컨테이너 런타임은 다양한 보안 기능을 제공하여 컨테이너의 보안을 강화할 수 있다. 예를 들어, 컨테이너 런타임은 네임스페이스를 사용하여 컨테이너 프로세스를 격리할 수 있으며, cgroups을 사용하여 리소스 사용량을 제한하고, seccomp, AppArmor, SELinux와 같은 보안 프로파일을 적용하여 컨테이너의 시스템 호출을 제한할 수 있다. Code: 코드 계층에서는 애플리케이션의 소스 코드 및 구성에 초점을 맞춘다. 이 계층에서는 애플리케이션의 취약점 및 보안 결함을 찾고 수정하여 워크로드의 보안을 향상시켜야 한다. 방법으로는 TLS 액세스, 통신 포트 제한, 종속성 보안, 정적 및 동적 소스 코드 분석, 의존성 스캔 및 안전한 코딩 기법 등이 있다. 항목 설명 통신 포트 제한 통신 포트 제한은 서버, 컨테이너, 애플리케이션 등이 사용하는 네트워크 포트를 제한하여 보안을 강화하는 방법이다. 불필요한 포트를 차단하고, 필요한 포트에 대해서만 허용하면 악의적인 공격자가 시스템에 액세스하는 것을 방지할 수 있다. 종속성 보안 종속성 보안은 애플리케이션에서 사용하는 외부 라이브러리 및 패키지에 대한 보안을 관리하는 방법이다. 취약한 종속성을 사용하면 시스템이 공격에 노출될 수 있다. 따라서, 종속성을 최신 상태로 유지하고, 보안 취약점이 발견될 경우 적절한 조치를 취하는 것이 중요하다. 정적 및 동적 소스 코드 분석 정적 소스 코드 분석은 코드를 실행하지 않고 소스 코드를 검사하여 보안 취약점을 찾는 방법이다. 동적 소스 코드 분석은 애플리케이션을 실행하면서 코드를 분석하여 보안 취약점을 찾는 방법이다. 이러한 분석 방법들을 사용하여 애플리케이션 코드의 보안 취약점을 발견하고 수정할 수 있다. 의존성 스캔 및 안전한 코딩 기법 의존성 스캔은 애플리케이션의 종속성에 대한 보안 취약점을 찾기 위한 도구를 사용하는 것이다. 안전한 코딩 기법은 개발자가 코드를 작성할 때 고려해야 하는 보안 지침 및 원칙이다. 이러한 기법을 사용하면 애플리케이션의 보안을 강화하고, 취약점을 줄일 수 있다. 보안적으로 신경써야할 요소가 많다. 위의 설명한 보안 기능들을 전부 다루면 좋겠지만 내용이 방대하다. 이번 블로그 글에서는 보안 툴인 kubescape를 이용하여 앞서 4C에서 Cluster, Container, Code 계층의 보안 검증 과정을 다뤄보겠다.\nKubescape 쿠버네티스 클러스터 보안 설정을 평가하고 검증하는 오픈 소스이다. NSA와 MITRE의 Kubernetes Hardening Guidance를 기반으로 작동하며 웹 대시보드를 통해 검증 및 취약점 점검을 할 수 있다. 또한, 도커 레지스트리, 이미지 취약점 스캔이 가능하다. 23년 4월 기준으로 업데이트가 계속 진행 중이며 공식 문서 또한 정리가 잘 되어 있다.\n그렇지만 제한적인 요소도 존재한다. 클러스터를 ARMO 웹 대시보드로 연결해야한다는 점으로 온프레미스에서 도입시 고려해야 한다. 또한 프리티어 기준 워크 노드가 10개로 제한된다.\n보안 기능을 전부 사용하려면 Operator 설치가 동반된다. 아키텍처는 다음과 같이 구성된다.\nhttps://github.com/kubescape/helm-charts/blob/master/charts/kubescape-cloud-operator/README.md\nMaster Gateway: ARMO 백엔드에서 실행 중인 마스터 게이트웨이이며 사용자가 등록한 모든 게이트웨이에 메시지를 브로드캐스트하여 모든 클러스터 게이트웨이에 런타임 작업을 전달한다. In-cluster Gateway: 클러스터 내에서 다른 구성 요소와 통신하기 위해 마스터 게이트웨이와 연결되며, 웹소켓(Websocket)을 사용하여 등록된다. 브로드캐스트 메세지를 전달받아 다른 컴포넌트에 전달한다. Operator : 트리거 엔진으로 게이트웨이에서 잔달받은 작업을 실행하거나 스케줄링하는 역할을 담당한다. Kubevuln : 컨테이너 이미지 취약점을 스캔하는 컴포넌트이다. 취약점 스캔은 grype을 통해 진행한다. Kubescape : 클러스터 내부 검증을 스캔하는 컴포넌트이다. Kollector : kubernetes API Server와 통신하여 클러스터 정보와 변경 정보를 확인하며 정보를 백엔드 CloudEndpoint로 전달한다. 설치 배포 환경 : kops 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스\n먼저 https://cloud.armosec.io/ 회원가입이 필요하다. 회원가입 이후 회원 ID 값에 맞게 helm 설치 명령어가 나온다. 복사하여 클러스터에 설치하자.\n설치 이후 Verfiy installation 버튼을 누르면 아래와 같이 연결이 안된다고 나오나 kops 클러스터에서 설치했을 때의 버그같다.\n아래와 같이 로그 확인 및 대시보드를 확인하면 정상적으로 등록이 되어 있다.\n1 kubectl -n kubescape logs -f $(kubectl -n kubescape get pods | grep kollector | awk \u0026#39;{print $1}\u0026#39;) 기능 확인 대시보드를 접속하면 왼쪽의 메뉴를 통해서 보안 검증이 가능하다.\nCompliance : 쿠버네티스 준수 정책 검증 Vulnerabilities : 컨테이너 파드 취약점 점검 RBAC Visualizer : RBAC 시각화 Repository Scanning : 레파지토리 스캐닝 Registry Scanning : 도커 레지스트리 스캐닝 Compliance 쿠버네티스 모범 사례를 체계화하여 수백 개의 항목들을 통해 클러스터 검증시켜주는 기능이다. 검증 항목은 MITRA 와 NSA 에서 제시하는 보안 가이드라인이며 웹 대시보드에서 프레임워크별 제어가 가능하다.\n또한, 보안이 필요한 항목에는 fix버튼을 통해 구성적으로 확인이 가능하다.\n아래는 필자 클러스터 환경에서 high 레벨 항목이며 몇 가지 항목 원인은 다음과 같다.\nC-0057(Privileged container) : 호스트 시스템의 모든 기능을 포함하는 컨테이너가 있어서 발생한 항목이다. 파드 구성 중 spec.container.securityContext.privileged == true 일 때 발생한다. 필자의 경우 볼륨 관리 파드(ebs-csi-node) 에서 발생했다. C-0045(Writable hostPath mount) : 쓰기 가능한 hostPath 볼륨으로 컨테이너를 생성했을 때 발생하는 알람이다. 설명에서는 이를 통해 호스트 볼륨의 정보를 얻을 수 있다 한다. 파드 구성 중 mount.readOnly == false 이 됐을 때 발생한다. C-0015 (kubernetes secret list) : 쿠버네티스 사용자가 시크릿에 대해 접근이 가능할 때 발생하는 알람이다. C-0041(HostNetwork access) : 호스트 네트워크에 연결할 경우 발생하는 알람이다. 필자의 경우 external-dns 파드에서 발생했다. 항목을 살펴보니 대부분 add-on 파드들에 대해 발생한 경고이다. 이러한 경우 ignore 로 알람 무시가 가능하다.\n각 모범 사례 기준이 엄격하고 서드 파티 레벨의 애플리케이션의 구성 정보 확인시 검증 과정에서 유용하게 사용할 수 있을 것 같다.\nVulnerabilities 취약점 점검으로 배포 컨테이너들에 대해 취약점 점검을 진행한다. 취약점 점검은 grype 엔진을 통해 진행되며 CVE(Common Vulnerabilities and Exposures, 미국 NSA, CISA에서 제공하는 보안 가이드라인) 식별자로 검사가 진행된다.\n위와 마찬가지로 필자의 클러스터에서 CRITICAL한 레벨의 취약점을 살펴보겠다.\nGHSA-r48q-9g5r-8q2h(CVE-2022-1996) : emicklei/go-restful의 버전 3.8.0 이전에서 발견된 취약점이다. 사용자가 제어할 수 있는 키를 통해 권한을 우회할 수 있는 보안 문제가 있다고 한다. 한 가지 의문은 kubescape 내 파드인 kollector 에서 발생하는 것인데, 깃허브 이슈와 커밋에도 없는 사항이라 깃허브 이슈로 등록했다.\n그 외에도 metrics-server, coredns 파드에서도 발생했으며 관련 깃 이슈를 공유한다. 대부분 해당 패키지 버전 업데이트로 피드백하며 업데이트하는 것 같다.\nRBAC Visualizer 쿠버네티스 role 에 부여된 오브젝트 접근 제어 권한에 대해 시각화 기능을 제공해준다. 운영적으로 매력적인 기능이다. 아래 그림처럼 쿠버네티스 ServiceAccount 별로 접근 제어 목록을 한 눈에 확인할 수 있어 불필요한 권한 및 접근 제어에 대해 검증이 가능할 것 같다.\nRepository scanning 코드 관리 저장소인 깃허브, 깃랩 등의 레파지토리에 스캐닝 검사를 시켜준다.\n레파지토리 등록은 공식문서를 참고하자. 필자는 mac 환경에서 진행했다.\n깃허브 토큰을 환경 변수로 등록하고, kubescape CLI 를 통해 취약점 검사 및 웹 대시보드로 정보를 전달한다.\n1 export GITHUB_TOKEN=my-access-token Location 은 저장소 URL 이다. kubescape CLI 를 통해 검사하면 웹 대시보드에서 확인이 가능하다. 아래 화면은 필자의 깃허브 레파지토리를 등록해서 검사하였다.\n레파지토리의 코드와 헬름 차트의 value 값들에도 취약점 검사가 진행된다.\n필자의 레파지토리의 경우 C-0009(Resource limits) 항목이 많아 확인해보니 resource limit 설정 문제였다. 테스트 환경에서 진행함으로 리소스 제한을 풀었지만 운영 환경에서 도입시 파드 구성 환경에 따라 설정해야겠다.\nRegistry Scanning 이미지 레파지토리에도 검사가 가능하다. 하버에서도 이미지 취약점 점검이 가능하는 것으로 알고 있는데 차이점을 확인해보겠다. 기능 확인을 위해 하버를 클러스터에 설치하여 스캐닝 기능을 테스트하였다. 등록시 스캐닝 주기와 태그 개수를 설정할 수 있다.\n시간별로 이미지 레파지토리에 취약 점검이 가능하다. 마찬가지로 grype 엔진을 통해 이미지 취약점 점검을 진행한다.\nINTEGRATIONS kubescape 툴은 Code, CI / CD 에도 활용이 가능하다.\nhttps://github.com/kubescape/kubescape\n공식문서를 확인하니 jenkins, gitlab CI / CD 에서도 job 스케쥴링으로 취약점 점검이 가능하다. CI / CD 파이프라인 구축 이후 고려해보도록 하자.\n3rd party 애플리케이션 연동을 통해 알람 구성도 쉽게 가능하다. 슬랙에서도 알람 설정이 쉽게 가능한데, 스캐닝과 점검 레벨에 따라 알람 설정이 가능하다.\n마치며 이번 블로그 글에서는 쿠버네티스 보안 계층과 보안 툴인 kubescape를 살펴보았다. 개인적으로 쿠버네티스의 보안 생태계를 경험하여 역량 향상에 도움이 되었다. 필자가 생각하기에 kubescape 툴은 미국 가이드라인의 사례나 취약점 점검과 grype 엔진을 통합해서 평가해주는 operator 툴 느낌이 강했다. 비즈니스 모델에 따라 달라지겠지만 비슷한 operator 툴을 대안으로 찾으면 좋을 것 같다.\n","date":"Apr 07","permalink":"https://HanHoRang31.github.io/post/pkos-5-security/","tags":["KANS","kops","cloud","AWS","kubernetes","security","kubescape"],"title":"[PKOS] 쿠버네티스 보안과 Kubescape DeepDive"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 스터디 4주차 시간에는 쿠버네티스 모니터링과 로깅 시스템을 구축하여 기능들을 살펴보았다. 이번 블로그 글에서는 모니터링 시스템에 대해 심화 학습한 내용들을 공유하고자 한다.\n모니터링은 어떤 대상을 감시, 감찰한다는 뜻으로 모니터링의 목적은 지속적인 감시, 감찰을 통해 대상의 상태나 가용성, 변화 등을 확인하고 대비하는 것이다. 모니터링의 개념처럼 쿠버네티스 모니터링도 똑같다. 쿠버네티스에서 특정 기간에 측정한 일련의 숫자(메트릭)에 대해 감시와 감찰을 통해 대상의 상태나 가용성 변화를 확인하고 대비한다고 보면 되겠다. 쿠버네티스 모니터링 시스템으로는 Prometheus, InfluxDB, DataDog, 클라우드 프로바이더 등이 있으나 이번 블로그 글에서는 오픈소스 모니터링 시스템인 Prometheus와 기능 확장 시스템인 Thanos를 다루겠다.\nPrometheus 오픈 소스 모니터링 시스템이다. 시계열 데이터 수집, 저장 및 쿼리 기능을 제공하고 다양한 경고 기능을 제공한다. 오픈소스 진영에서 가장 많이 사용하는 모니터링 시스템으로 사실상 거의 표준처럼 사용하고 있다. 아키텍처는 다음과 같다. (공식문서)\nhttps://prometheus.io/docs/introduction/overview/\n빨간 네모로 표시된 것이 프로메테우스 구성 컴포넌트이다.\nPrometheus Server : Prometheus 서버는 메트릭 수집, 저장, 처리 및 쿼리 기능을 수행한다. 메트릭 수집 방식으로 Pull 방식을 기본적으로 사용한다. 해당 서버가 대상 서비스로부터 메트릭을 주기적으로 수집하고, 시계열 데이터베이스(TSDB, HDD/SDD)에 저장한다. 데이터베이스에 저장한 데이터는 쿼리 언어 PromQL을 통해 데이터를 필터링, 집계 시각화하는데 사용한다. Pushgateway : Pushgateway는 Push 방식을 사용하는 일부 유형의 메트릭을 Prometheus에서 수집하기 위한 중간 서버이다. 주로 일회성 작업(예: 배치 작업)으로부터 메트릭을 수집하는 데 사용된다. 작업이 종료되더라도 메트릭이 보존되어 Prometheus 서버가 해당 메트릭을 수집할 수 있게 한다. Alertmanager: Alertmanager는 Prometheus 서버에서 발생한 경고를 관리하고, 사용자에게 알림을 전달하는 컴포넌트이다. Prometheus UI : 내장된 웹 인터페이스로, 사용자가 Prometheus 서버에서 메트릭을 쿼리하고, 시각화된 그래프를 확인할 수 있다. 사용자는 PromQL을 사용하여 원하는 메트릭을 검색하고 분석할 수 있으며, 기본적인 대시보드 및 경고 설정을 관리할 수 있다. 아키텍처를 살펴보았는데 프로메테우스는 단일 노드 시스템으로 설계되어 있어 클러스터링 구조를 직접 지원하지 않는다. 이로인해 확장성과 고가용성에 일부 보완이 필요하다.\n확장성 문제 단일 노드에서 모든 메트릭을 처리하려 할 때 노드의 자원이 고갈되어 성능 저하를 초래할 수 있다. 대규모 인프라에서 많은 수의 메트릭을 수집하고 처리하는 데 있어 성능 저하와 저장소 부족 문제가 발생할 수 있다. 외부 스토리지 연결이 필요하다. 고가용성 문제 단일 노드에서 발생하는 장애나 다운타임이 생겨 프로메테우스 서버가 내려가면 그 시간 동안에는 메트릭을 수집할 수 없다. 볼륨이 AWS EBS 를 사용해도 단일 노드에서만 연결이 가능하다. 연결 노드에 다운 타임이 발생하면 메트릭을 가져올 수 없다. 이러한 문제를 해결하기 위한 도구로 Thanos를 사용할 것이다.\nThanos 프로메테우스의 확장성과 고가용성을 개선하기 위한 시스템이다. 사이트 정문에 대놓고 프로메테우스를 저격하고 있다. 타노스 아키텍처를 통해 어떻게 개선할 수 있는 확인해보겠다.\nhttps://thanos.io/v0.6/thanos/getting-started.md/\n파란 네모가 타노스 구성 컴포넌트이다. 설계 디자인은 공식 문서에서도 참고가 가능하다.\nThanos Sidecar : Prometheus에 연결되어 메트릭 데이터를 쿼리하고 클라우드 스토리지에 업로드한다. 노드마다 사이드카가 연결되며 외부 스토리지 저장을 통해 확장성을 개선시키는 역할의 컴포넌트이다. Thanos Store Gateway : 외부 스토리지에 메트릭 데이터를 읽어 Thanos Query로 전달한다. 해당 컴포넌트를 통해 외부 스토리지에서 과거 데이터도 쿼리할 수 있게 된다. Thanos Query : 사용자 쿼리를 요청 처리하며 짧은 시간의 데이터는 타노스 사이드카에서 가져오며, 오래된 데이터는 스토어 게이트웨이를 통해 외부 스토리지에서 가져온다. Prometheus Query API를 구현하여 사용자가 기존의 Prometheus 쿼리를 그대로 사용할 수 있게 한다. 프로메테우스단에서 고가용성을 제공해주는 컴포넌트이다. 통합 데이터간의 중복 제거 (de-duplication) 기능을 기본으로 제공하여 여러 프로메테우스 및 원격 스토리지의 메트릭 데이터를 통합하여 쿼리할 수 있게 해준다. 한 가지 주의할 점은 Thanos query 도 고가용성을 보장해줘야 한다. 공식 문서에 따르면 타노스 구성 파드들은 샤딩 수단을 제공하지 않아, 모두 수평적 확장이 가능하다. 타노스 배포시 쿼리 파드 개수를 2개 이상으로 조절하여 가용성을 보장시키자.\nhttps://observability.thomasriley.co.uk/prometheus/using-thanos/high-availability/\nhttps://thanos.io/tip/thanos/design.md/#metric-sources\nThanos Compactor : 타노스 쿼리와는 별개의 프로세스로, 객체 스토리지 버킷만 가리키며 여러 개의 작은 블록을 더 큰 블록으로 지속적으로 통합시켜주는 컴포넌트이다. 블록을 통합시키면 데이터가 압축하게 되므로 버킷의 총 스토리지 크기, 스토어 노드의 로드 및 버킷에서 쿼리 데이터를 가져오는 데 필요한 요청 수가 크게 줄어든다. Thanos Ruler : 프로메테우스 인스턴스들로부터 알림 규칙 정보를 가져와 통합하고, 프로메테우스와 함께 작동하는 외부 알림 시스템에게 알림을 전송하는 역할의 컴포넌트이다. 연계 배포 배포 환경 : kops 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스\n프로메테우스 \u0026amp; 타노스를 연계하여 배포한다. 배포를 위해 헬름 차트를 사용할 예정이며 프로메테우스 배포는 kube-prometheus-stack(그라파나, 추가 메트릭 자동 구성) 차트를 사용할 것이다. 또한 타노스는 bitnami/thanos 차트를 사용할 것이며 타노스 외부스토리지는 MinIO를 배포하여 연결할 것이다. 전체 배포 순서는 다음과 같다. 구성 차트는 필자의 깃허브에서 참고가 가능하다.\nMInIO 배포 kube-promethes-statck 설정 \u0026amp; 배포 타노스 설정 \u0026amp; 배포 그라파나 설정 및 대시보드 확인 1. MinIO 배포 타노스 외부 스토리지로 MinIO를 설정할 것이다. 이를 위한 사전 작업으로 MinIO를 먼저 배포하겠다.\n차트 가져오기\n1 2 3 helm repo add minio https://charts.bitnami.com/bitnami helm repo update helm fetch minio/minio --untar --version 12.2.1 차트 설정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # values-minio.yaml mode: distributed auth: rootUser: admin rootPassword: \u0026#34;admin1234\u0026#34; statefulset: replicaCount: 4 zones: 1 drivesPerNode: 1 provisioning: config: - name: region options: name: ap-northeast-2 ingress: enabled: true hostname: minio.hanhorang.link path: /* annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}, {\u0026#34;HTTPS\u0026#34;:9090}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: \u0026#34;$ACM arn \u0026#34; persistence: storageClass: \u0026#34;kops-csi-1-21\u0026#34; 분산스토리지 모드로 설정 (테스트환경 노드 4개) 노드당 파드 하나를 할당 Ingress(네트워크) : AWS ALB 설정 persistence(볼륨) : AWS gp2 기본 스토리지 클래스 설정 배포\n1 2 kubectl create ns minio helm install minio minio/minio -f values-minio.yaml -n minio --version 12.2.1 버킷 생성 및 접근 키 발급\n타노스에서 minio 버킷에 접근하기 위한 접근 키를 발급받자\nminio 도메인 접속\n어드민 계정은 차트에서 admin / admin1234 로 설정되어 있다. 로그인을 하자.\n로그인이 완료되면 다음과 같은 화면을 확인할 수 있다.\n버킷 생성 후 버킷 접근을 위한 액세스 키 발급이 필요하다. 왼쪽 메뉴 [Access Keys] 에서 키를 발급받자.\nAccess Key 발급 후 MINIO 동작 권한을 등록해야 한다. 생성한 키를 클릭하면 정책 입력 칸이 나온다. 아래 정책을 입력하도록 하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;admin:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::*\u0026#34; ] } ] } 필자의 경우 접근 키는 다음과 같이 생성되었다.\naccess_key : aajl91wFPCRVmfWR\nsecret_key: SfP4woqjY3fcyh1cuwF1CNQFEe6hs4X6\n발급받은 키를 기반으로 Secret을 생성하자.\n1 2 3 4 5 6 7 #minio-key.yaml type: s3 config: bucket: thanos endpoint: minio.hanhorang.link access_key: aajl91wFPCRVmfWR secret_key: SfP4woqjY3fcyh1cuwF1CNQFEe6hs4X6 1 2 kubectl create ns monitoring kubectl create secret generic thanos-minio-secret -n monitoring --from-file=minio-key.yaml 2. kube-promethes-statck 설정 \u0026amp; 배포 프로메테우스 배포 및 사이드 카에 타노스 연동을 위한 설정을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm fetch prometheus-community/kube-prometheus-stack --untar --version 45.7.1 # 사용 리전의 인증서 ARN 확인 CERT_ARN=`aws acm list-certificates --query \u0026#39;CertificateSummaryList[].CertificateArn[]\u0026#39; --output text` echo \u0026#34;alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN\u0026#34; KOPS_CLUSTER_NAME=\u0026#34;hanhorang.link\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # values-kube-prometheus-stack.yaml cat \u0026lt;\u0026lt;EOT \u0026gt; ./values-kube-prometheus-stack.yaml alertmanager: enabled: false grafana: defaultDashboardsTimezone: Asia/Seoul adminPassword: admin1234 ingress: enabled: true ingressClassName: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; hosts: - grafana.$KOPS_CLUSTER_NAME paths: - /* prometheus: # 사이드카 노출 서비스 설정 thanosService: enabled: true ingress: enabled: true ingressClassName: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; hosts: - prometheus.$KOPS_CLUSTER_NAME paths: - /* prometheusSpec: podMonitorSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false retention: 5d retentionSize: \u0026#34;10GiB\u0026#34; scrapeInterval: \u0026#34;15s\u0026#34; # alert 관련 설정으로 주석 처리 # evaluationInterval: 15s # 가용성 설정 replicas: 3 # 타노스 설정 thanos: image: \u0026#34;quay.io/thanos/thanos:v0.27.0\u0026#34; objectStorageConfig: key: minio-key.yaml name: thanos-minio-secret version: v0.27.0 # 볼륨 설정 storageSpec: {} ## Using PersistentVolumeClaim ## # volumeClaimTemplate: # spec: # storageClassName: gluster # accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] # resources: # requests: # storage: 50Gi # selector: {} EOT 알람을 사용하지 않음으로 alertmanager false로 설정하였다. 헬름 차트를 보면 prometheus.thanos 에 설정하는 부분이 있는데 여기서 설정하는 것이 아니다! 원격 스토리지 접근에 대한 오류가 발생하므로 prometheus.prometheusSpec.thanos 에 앞서 생성한 시크릿 키를 입력하자. (위에 차트 그대로 입력하면 문제없습니다.) 1 2 3 4 kubectl create ns monitoring helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.7.1 \\ -f values-kube-prometheus-stack.yaml --namespace monitoring 배포 이후 타노스 사이드카 연동을 확인하자.\n1 kubectl describe pods prometheus-kube-prometheus-stack-prometheus-0 -n monitoring 성공이다!\n3. 타노스 설정 \u0026amp; 배포 타노스 사이드카를 제외한 컴포넌트를 설치하고 thnaos query 가 프로메테우스 사이드카로, store gateway가 원격 스토리지인 minio 로 연동할 수 있도록 설정해야 한다. 차트부터 가져오도록 하자.\n1 2 3 helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm fetch bitnami/thanos --untar --version 12.3.2 타노스 연동을 위해 설정을 진행한다. 메트릭을 가져오기 위해 버킷 정보와 프로메테우스 배포시 같이 배포된 타노스 사이드카 서비스 주소를 입력한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 objstoreConfig: |- type: s3 config: bucket: monitoring endpoint: minio.minio.svc.cluster.local:9000 access_key: aajl91wFPCRVmfWR secret_key: SfP4woqjY3fcyh1cuwF1CNQFEe6hs4X6 insecure: true querier: stores: - kube-prometheus-stack-thanos-discovery.monitoring.svc.cluster.local:10901 - thanos-storegateway.monitoring.svc.cluster.local:10901 replicaCount: 2 ingress: enabled: true hostname: thanos.hanhorang.link ingressClassName: \u0026#34;alb\u0026#34; annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:955963799952:certificate/7569648c-bfd5-4860-b2c1-16ef02acbb58 alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; path : /* bucketweb: enabled: true compactor: enabled: true storegateway: enabled: true ruler: enabled: false objstoreConfig.config.endpoint 를 서비스 DNS로 대체했다. ALB 도메인 입력시 Timeout 으로 파드가 올라가지 않기 때문이다. querier.store 에 쿼리할 대상을 등록한다. 대상으로 타노스 사이드카의 서비스 주소와 스토어게이트웨이를 등록한다. 배포\n1 2 3 helm install thanos bitnami/thanos --version 12.3.2 \\ -f values-thanos.yaml --namespace monitoring 배포 완료 후 타노스 쿼리 호스트 도메인을 통해 접속하자. Store와 Status/Target를 확인하여 사이드카 연동을 확인한다.\n타노스가 정상적으로 배포된 것을 확인하였다. 배포 이후에는 프로메테우스 서버를 2개 이상 띄어서 프로메테우스 서버가 고가용성을 갖도록 구성하자. (앞서 프로메테우스 배포시 프로메테우스 서버를 3개를 배포하였다)\n3개의 프로메테우스 서버가 서로 독립적으로 메트릭을 수집한다. 타노스 쿼리는 프로메테우스에 등록된 사이드카를 통해 메트릭을 통합 수집한다. 이 때 고가용성이 보장되는데 하나의 프로메테우스가 다운타임이 가진다한들 다른 프로메테우스 서버에서 메트릭 수집 및 집계를 수행할 수 있기때문이다. 물론 중복 중복된 메트릭에 대해선 타노스 내 Use Deduplication 기능을 통해 소거가 가능하다. 중복 메트릭 설정은 프로메테우스 라벨 설정을 통해 가능하나 자동으로 설정이 되어 생략하겠다.\n4. 그라파나 설정 그라파나는 시각화 대시보드이다. 앞서 구축한 모니터링 시스템을 기반으로 메트릭 수집 파이프라인을 구성하고 대시보드를 확인하겠다. 그라파나 도메인에 접속하여 로그인을 진행한다. (초기 아이디: admin, 비밀번호: admin1234)\n먼저, 수집 메트릭 URL을 프로메테우스에서 타노스 쿼리로 수정할 것이다. 왼쪽 하단의 톱니바퀴 메뉴에서 Configuration에 들어간 다음 프로메테우스 설정 URL을 thanos-query:9090 으로 수정하자.\n바꾸고 나서 대시보드를 확인하면 정상적으로 작동하는 것을 확인할 수 있다.\n마치며 kube-prometheus-stack 자체적으로도 프로메테우스 고가용성을 보장한다. 하지만 이렇게 구성한 프로메테우스 HA는 여전히 중복 데이터와 쿼리 집계, 확장성에 대한 보완 요소가 있다. 이를 해결하기 위해 Thanos을 소개하였고 연동 방법과 구성 요소를 확인하였다.\n참고 https://aws.amazon.com/ko/blogs/opensource/improving-ha-and-long-term-storage-for-prometheus-using-thanos-on-eks-with-s3/\nhttps://velog.io/@seokbin/Kube-Prometheus-Thanos-구성#4-프로메테우스-ha-구성\n","date":"Apr 01","permalink":"https://HanHoRang31.github.io/post/pkos2-4-monitoring/","tags":["KANS","kops","cloud","AWS","kubernetes","monitoring","prometheus","thanos"],"title":"[PKOS] 고가용성 모니터링 시스템 구축하기 with 프로메테우스, 타노스"},{"categories":null,"contents":" 로깅 (Loki \u0026amp; Promtail ) 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. Logging? 애플리케이션 실행 중 발생하는 이벤트, 작업, 오류 등의 정보를 기록하는 프로세스이다. 로깅의 주요 목적은 프로그램의 실행 상태를 추적하고, 문제 발생 시 원인을 찾기고, 내부 감사를 기록하기 위함이다. 로그 파일은 시간 순서대로 저장되며, 대부분의 경우 텍스트 파일 또는 데이터베이스에 저장된다.\n컨테이너화된 애플리케이션에 가장 쉽고 가장 널리 사용되는 로깅 방법은 표준 출력(stdout)과 표준 에러(stderr) 스트림에 작성하는 것이다. 이를 이용하면 로깅 명령어를 통해 조회가 가능하다.\n1 2 3 4 5 6 # 로그 확인 예 kubectl logs metrics-server-5f65d889cd-znqw5 -n kube-system I0328 00:14:31.072509 1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key) I0328 00:14:31.477085 1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController .. 쿠버네티스 환경에서도 컨테이너 엔진이나 런타임이 제공하는 기본적인 로깅 기능이 있으나 충분하지 않다. 컨테이너가 크래시되거나, 파드가 축출되거나, 노드가 종료된 경우에도 애플리케이션의 로그에 접근할 수 없기 때문이다.\n따라서, 쿠버네티스에서 로그는 노드, 파드 또는 컨테이너와는 독립적으로 별도의 스토리지와 라이프사이클을 가져야 한다. 이 개념을 클러스터-레벨-로깅 이라 하며 이를 위해 별도의 벡엔드 솔루션이 필요하다. 쿠버네티스에 사용할 수 있는 로깅 솔루션은 3가가지 오픈소스 프로젝트를 결합한 PLG 스택(Promtail, Loki, Grafana) 또는 ELK(Elasticsearch, Logstash, Kibana)있다. 이번 블로그 글에서는 PLG 스택을 알아볼 것이며 로깅 시스템인 Loki 와 로그 수집 에이전트인 Promtail 을 설치하하여 클러스터-레벨-로깅을 테스트해보겠다.\nLoki Loki는 Grafana Labs에서 개발한 경량 로깅 시스템으로, 쿠버네티스 환경에서 메타데이터를 기반으로 로그를 수집하고 빠르게 처리할 수 있다. 그리고 Prometheus와 호환되는 레이블 기반 질의 및 필터링 기능을 제공하며, Grafana와 통합을 통해 로깅 데이터를 대시보드에서 확인이 가능하다. 로깅 수집 에이전트인 Promtail을 사용하여 로깅을 수집하며 이를 통해 쿠버네티스 리소스(노드, 파드 또는 컨테이너)와는 독립적으로 별도의 스토리지와 라이프사이클을 가진다.\nhttps://grafana.com/blog/2018/12/12/loki-prometheus-inspired-open-source-logging-for-cloud-natives/\n메타데이터 인덱싱?\nhttps://grafana.com/oss/loki/\n로키는 메타데이터를 인덱싱을 통해 경량화 및 빠른 쿼리 성능을 가진다. 메타인덱싱 원리는 위의 그림과 같은데 로그 전체 텍스트를 저장하는 것이 아니라 타임스탬프와 라벨을 묶어 인덱스(index) 로 그 외 나머지 텍스트를 청크(chunk)로 나눠 저장된다.\n인덱스: 로그 시간과 레이블을 묶어 해싱을 통해 고유한 식별자를 만든다. 이 식별자를 통해 로그 스트림을 참조하고 검색하는데 사용된다. 인덱스는 일반적으로 NoSQL DB에 저장하는데 Key 값에는 인덱스를 values 값에는 해당 청크의 데이터를 저장한다. 청크: 청크는 실제 로그 데이터이다. 청크는 데이터를 압축 및 저장하기 위해 여러 압축 알고리즘을 사용할 수 있으며, 기본적으로 압축이 적용되어 저장 공간을 최적화합니다. 일반적으로 오브젝트 스토리지에 저장한다. 아키텍처 https://grafana.com/blog/2018/12/12/loki-prometheus-inspired-open-source-logging-for-cloud-natives/\n그림에서 화살표 빨강은 로그 Write, 파랑은 Read를 의미한다. 또한, 각 구성 컴포넌트들은 HA를 지원하여 컴포넌트 내부 구성 하나에 장애가 발생하더라도 서비스가 중단되지 않는다.\nYour Jobs : 로그 수집 에이전트로 사용자 정의에 맞게 로그를 수집하여 로키 서버(Distributor)에 전달한다. 로그 수집 에이전트로 Promtail를 사용하나 fluent, fluentbit 과 호환이 가능하다. Distributor(디스트리뷰터): Distributor는 로그 데이터를 수신하고, 해당 데이터를 인제스터(Ingester)에 분산시키는 역할을 한다. 또한, 레이블의 해시 값을 사용하여 데이터를 적절한 인제스터에 전달하며 로드밸런싱을 통해 로그 데이터를 여러 인제스터에 고르게 분산시켜준다. Ingester(인제스터): Ingester는 Distributor로부터 로그 데이터를 받아서, 로그 스트림을 청크로 나누고 압축한 후 저장시킨다. 인제스터는 메모리 또는 영구 스토리지에 청크를 저장할 수 있으며, 쿼리어(Querier)에게 저장된 청크에 대한 질의 결과를 제공한다. 청크가 일정 시간 또는 크기에 도달하면 인제스터는 이를 영구 스토리지에 저장시킨다. Querier(쿼리어): Querier는 사용자로부터 질의를 받아 처리한다. 질의를 처리할 때, 쿼리어는 인덱스를 사용하여 관련된 청크를 찾고, 인제스터 및 영구 스토리지에서 해당 청크를 가져와 질의 결과를 반환한다. loki 버전 3.0 이상부터는 loki, loki-distributed가 통합되었다.\nhttps://grafana.com/docs/loki/latest/getting-started/\nLoki Write component : 로그 데이터를 수신하고 저장시켜주는 컴포넌트이다. 앞서 아키텍처의 빨간 flow를 담당하는 Distributor와 Ingester 로 구성되어 있다. Loki Read component: 로그 데이터를 조회하고 처리하는데 사용된다. 앞서 아키텍처의 파랑 flow를 담당하는 Querier와 Query Frontend 로 구성되어 있다. gateway : Loki 구성요소에 대한 프록시 서버이다. 로그를 까보면 NGINX 게이트웨이가 설치되며 로드밸런싱 기능을 수행하여 각 컴포넌트에 트래픽을 분산시킨다. 설치 설치 환경 : kops 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스\n설치는 다음과 같이 진행할 예정이다.\nLoki (helm grafana/loki 4.8.0) Promtail (helm grafana/promtail 6.9.2) 사족이지만, 로그 저장소로 mongoDB를 활용하려 했으나 안 된다! 로키 호환 저장소가 정해져있기 때문이다. 온프로미스에서 구성시 참고하자.\n본 블로그에서는 S3에 오브젝트 데이터를 DynamoDB에 인덱스 데이터를 저장시키겠다. (230402. DynamoDB 인덱스 연동문제로 S3에 인덱스, 오브젝트 데이터 저장)\nhttps://grafana.com/docs/loki/latest/operations/storage/\nLoki 설치 Loki 저장소로 S3 와 DyanmoDB 스토리지 생성과 IAM role 권한 연결이 필요하다. 본 테스트에서는 S3 이름을 han-loki , DyanmoDB 이름은 loki_ 로 생성하였다. DynamoDB 사용시 주의해야할 점은 다음과 같다.\nDynamoDB 테이블 이름을 헬름 차트의 schema_config.config DynamoDB 파티션 키 \u0026amp; 정렬 키를 (문자열, 바이너리)로 지정해야 한다. IAM 권한은 S3, DyanmoDB에 대한 정책을 부여했다.\nAmazonS3FullAccess AmazonDynamoDBFullAccess 필자는 사용자에 IAM 권한를 부여 후 access-key 와 secret-key를 가져왔다. 해당 키는 밑의 헬름 차트 버킷 연동에서 사용된다. 우선 헬름을 통해 로키 차트를 가져오겠다.\n1 2 3 4 kubectl create ns loki helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm fetch grafana/loki --untar --version 4.8.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 #values-loki.yaml schema_config: configs: - from: 2020-05-15 store: aws object_store: s3 schema: v11 index: prefix: loki period: 0 storage_config: aws: s3: s3://ap-northeast-2/han-loki dynamodb: dynamodb_url: dynamodb://ap-northeast-2 table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: write_scale: enabled: false read_scale: enabled: false chunk_tables_provisioning: write_scale: enabled: false read_scale: enabled: false table_prefix: \u0026#34;loki\u0026#34; tableManager: enabled: true monitoring: lokiCanary: enabled: true selfMonitoring: enabled: false loki: auth_enabled: false storage: bucketNames: chunks: han-loki ruler: han-loki admin: han-loki type: s3 s3: s3: han-loki endpoint: s3.ap-northeast-2.amazonaws.com region: ap-northeast-2 secretAccessKey: {SECRET KEY} accessKeyId: {ACCESS KEY} s3ForcePathStyle: false insecure: false http_config: {} access_key 와 Secret_access_key 노출에 주의하자! schema_config : 인덱스와 청크 데이터에 대한 저장 스키마를 정의한다. storage_config: 데이터 저장할 스토리지 정보를 입력한다. table_manager, tableManager : 테이블 기반 데이터 저장소에 인덱스 및 청크를 지원하는데 버전 호환 문제로 작동이 안되어 dynamodb 에 인덱스가 저장이 안된다. (현재 S3에 저장됨) monitoring.lokicanary: 시스템 검증에 사용된다. true 설정시 별도의 카나리 파드가 생성되어 일정시간마다 테스트 로그를 전달한다. monitoring.selfMonitoring : 대시보드에 Loki 관련 대시보드가 업로드된다하지만, loki: 로키 서버 설정을 정의한다. 최신 버전에는 스토리지 연동을 여기서 하는데 dynamodb에 대한 설정 예시가 없고 테스트가 안되서 s3 만 정의하였다. 1 helm install loki grafana/loki -f values-loki.yaml -n loki --version 4.8.0 파드 배포는 완료되었으나, 로키 스토리지 연동으로 안정화 작업이 필요하다.\n안정화 작업는 다음과 같다.\nstorage_config 스토리지 연동 문제\n1 2 level=error ts=2023-04-01T03:18:30.427197283Z caller=flush.go:144 org_id=self-monitoring msg=\u0026#34;failed to flush\u0026#34; err=\u0026#34;failed to flush chunks: store put chunk: NoCredentialProviders: no valid providers in chain. Deprecated.\\n\\tFor verbose messaging see aws.Config.CredentialsChainVerboseErrors, num_chunks: 1, labels: {app_kubernetes_io_component=\\\u0026#34;read\\\u0026#34;, app_kubernetes_io_instance=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_name=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_part_of=\\\u0026#34;memberlist\\\u0026#34;, cluster=\\\u0026#34;loki\\\u0026#34;, container=\\\u0026#34;loki\\\u0026#34;, controller_revision_hash=\\\u0026#34;loki-read-7749df4969\\\u0026#34;, filename=\\\u0026#34;/var/log/pods/loki_loki-read-2_a8fab5a2-e78b-4cff-9f8d-ba0ee5952a3c/loki/0.log\\\u0026#34;, job=\\\u0026#34;loki/loki-read\\\u0026#34;, namespace=\\\u0026#34;loki\\\u0026#34;, pod=\\\u0026#34;loki-read-2\\\u0026#34;, statefulset_kubernetes_io_pod_name=\\\u0026#34;loki-read-2\\\u0026#34;, stream=\\\u0026#34;stderr\\\u0026#34;}\u0026#34; level=info ts=2023-04-01T03:18:30.427244754Z caller=flush.go:168 msg=\u0026#34;flushing stream\u0026#34; user=self-monitoring fp=33d14d11bfd98f55 immediate=false num_chunks=1 labels=\u0026#34;{app_kubernetes_io_component=\\\u0026#34;read\\\u0026#34;, app_kubernetes_io_instance=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_name=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_part_of=\\\u0026#34;memberlist\\\u0026#34;, cluster=\\\u0026#34;loki\\\u0026#34;, container=\\\u0026#34;loki\\\u0026#34;, controller_revision_hash=\\\u0026#34;loki-read-7749df4969\\\u0026#34;, filename=\\\u0026#34;/var/log/pods/loki_loki-read-2_a8fab5a2-e78b-4cff-9f8d-ba0ee5952a3c/loki/0.log\\\u0026#34;, job=\\\u0026#34;loki/loki-read\\\u0026#34;, namespace=\\\u0026#34;loki\\\u0026#34;, pod=\\\u0026#34;loki-read-2\\\u0026#34;, statefulset_kubernetes_io_pod_name=\\\u0026#34;loki-read-2\\\u0026#34;, stream=\\\u0026#34;stderr\\\u0026#34;}\u0026#34; 공식문서 예제 storage_config 가 최신 기준으로 업데이트된 것 같지 않다. 필자의 경우 배포 yaml로 구성을 설정하니 정상적으로 작동했다.\nS3 디렉토리에 chunk 폴더가 없다? 이슈 에 따르면 멀티테넌트 구성에서 loki를 실행할때 인증이 비활성화하면 기본적으로 fake 폴더에 저장된다고 한다. fake폴더 안에는 정상적으로 chunk가 들어가있는 것을 확인할 수 있으나 다중 클러스터에서 배포시 loki 별로 인증이 필요할 것 같다.\ndynamoDB에 인덱스를 저장하고 싶어요.\n해결해야할 문제다. 현재 S3에 index값이 들어가는데 dynamodb에 옮겨야 한다. 헬름차트에 table-manager 설정이 두개(table-manager, tableManager)여서 설정이 안 먹힌다. 추가 원인으로는 깃허브 이슈( https://github.com/grafana/loki/issues/5070) 설정값( extraArgs)인데 적용이 안된다. 추후 해결시 업데이트하겠다.\n배포를 완료하면 로키 파드가 정상적으로 작동하는 것을 알 수 있다.\nhttp://loki-read-headless.loki.svc.cluster.local:3100\nPromtail 설치 1 2 3 helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm fetch grafana/promtail --untar --version 6.9.2 Promtail 차트에서 따로 설정할 것은 없지만, 로키 연결과 로그 라인 파이프라인 구성 확인을 위해 집어넣는다.\nconfig.clients.url : 로그 수집 후 전달한 로그 서버를 입력한다. 보통 로키 서버 설치시 설정되는 url로 지정된다. config.snippets: 로그 라인 분석과 추출, 필터링하는 스테이지들의 작업을 정의한다. 원하는 로그 데이터 형식을 정의해서 입력하면 된다. scraping 구문은 프로메테우스와 동일하다. 1 helm install promtail grafana/promtail -n loki --version 6.9.2 로그 수집 테스트 Promtail-Loki-Grafana 까지의 로그 수집을 테스트하겠다. 테스트를 위해 nginx 를 배포하고 파드 로그 확인과 그라파나(로키)에서 로그를 확인한다.\n1 2 helm repo add bitnami https://charts.bitnami.com/bitnami helm install nginx bitnami/nginx --version 13.2.23 -f nginx-values.yaml 파드 로그는 파드가 올라간 노드 /var/log/pods 에 저장되어 있다. 파드의 로그를 확인하고 그라파나 대시보드에서 로키가 해당 로그를 긁어오는지 확인하자.\n[Explorer] → Job = default/nginx 설정 후 로그 확인 잘 들어온다! 이어서 파드 라이프사이클과 독립적으로 로그가 관리되는 지 확인하겠다. nginx 파드를 삭제하고 파드 로그와 로키를 확인하겠다.\n1 helm uninstall nginx 아래 디렉토리를 확인하면 nginx 파드가 삭제되어 로그 디렉토리가 삭제된 것을 확인할 수 있다.\n파드가 삭제되었지만 그라파나(로키) 에서 nginx 로그를 확인할 수 있다.\n마치며 그라파나 공식 문서를 참고하니 엔터프라이즈에 대한 지원만 활발한 느낌이다. 오픈소스로 설치시 연동 부분과 최신 버전 호환 문제로 테스트하는 데도 오랜 시간이 걸렸다. 특히 인덱스 데이터를 dynamodb 에 연동해야 했지만 설정 문제로 실패했고 S3에 인덱스, 청크 데이터를 저장시켰다. 이 부분은 공식 문서를 최신 버전으로 업데이트를 하거나 예가 나오면 업데이트하겠다.\n","date":"Apr 01","permalink":"https://HanHoRang31.github.io/post/pkos2-4-logging/","tags":["KANS","kops","cloud","AWS","kubernetes","monitoring","PLG","loki","grafana","promtail"],"title":"[PKOS] 로깅 PLG 스택, 최신 버전(Loki v2.8.0) 배포하기"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 3주차 시간에는 Gitlab 과 ArgoCD를 배포하여 Gitops 시스템을 구축하였다. 이번 블로그 글에서는 GitOPS 시스템에 대한 실습 내용들을 정리하고 공유하고자 한다.\nGitOps는 GIt을 진실의 원천(SSOT, Single Source of Truth) 으로 사용하는 인프라와 애플리케이션 배포 관리 방식이다. 진실의 원천이라는 말은 Git에서만 소스를 관리할 수 있게 하여 단일 진실 원천을 구현한다는 말이다. 쿠버네티스에서는 GItOps를 ArgoCD를 이용하여 깃 저장소에 있는 소스를 정의된 클러스터 환경에 자동으로 반영시켜 준다.\nGitOps 시스템을 구축하면 얻을 수 있는 이점은 다음과 같다.\n버전 관리: Git을 사용하므로 인프라 및 애플리케이션의 모든 변경 사항이 추적되고 버전이 관리된다. 이를 통해 문제 발생 시 이전 상태로 쉽게 되돌릴 수 있다. 디커플링: 코드와 인프라를 분리함으로써 개발자와 운영팀 간의 협업이 쉬워진다. 자동화: 변경 사항이 자동으로 적용되므로 수동 인프라 관리 작업이 줄어들고, 실수를 방지할 수 있다. 보안: Git 저장소에 접근 권한을 제어함으로써 인프라 변경에 대한 보안을 강화할 수 있다. 신속한 피드백 루프: 문제가 발생하면 소스 코드에 대한 변경을 통해 빠르게 해결하고 적용할 수 있다. 이점만 존재하는 것은 아니다, 단점도 존재한다.\n학습 곡선: GitOps 및 관련 도구를 사용하려면 Git, 선언적 인프라 도구 및 오케스트레이션 플랫폼에 대한 지식이 필요하다. 복잡성: GitOps를 사용하면 초기 설정과 관리가 복잡할 수 있다. 적절한 도구와 프로세스를 구축하고 유지 관리하는 데 시간과 노력이 필요할 수 있다. 높은 의존성: GitOps는 Git 저장소에 대한 높은 의존성을 가지며, 저장소 접근이 불가능한 경우 인프라 변경이 제한될 수 있다. 간단하게 예제 시나리오를 구성하여 GitOps 시스템을 구축하고자 한다. 먼저 쿠버네티스 환경에 Gitlab과 ArgoCD를 배포할 것이고, 관리 대상을 지난 시간에 배운 Harbor 배포 차트로 지정할 것이다.\nGitlab 배포 Gitlab은 소스 원격 저장소이다. 흔히 쓰는 깃허브로 생각하면 이해가 빠르다. Gitlab의 차별점은 사설(공식문서에는 offline이라 칭함) 깃랩을 구축할 수 있다는 점인데 직접 헬름 차트를 구성하여 배포하겠다.\n깃랩 차트는 다음과 같이 불러올 수 있다.\n1 2 3 helm repo add gitlab https://charts.gitlab.io/ helm repo update helm fetch gitlab/gitlab --untar --version 6.8.1 깃랩 차트에 부가 옵션이 많다.. 공식 문서를 참고하자. 다음은 필자가 차트를 보고 간략히 정리한 내용이다.\n유료 버전 무료 제공 : 쿠버네티스에서 오프라인 깃랩 설치시 enterprise edtion (유료, 이하 EE라 칭함) 을 무료로 사용할 수 있다. 이유를 찾아보니 고객 유치 전략이라 한다. EE 사용시 고급 보안, 인증, 권한 관리, 진행률 보고, 고급 CI/CD 기능, 멀티 프로젝트 파이프라인 등의 고급 기능을 제공한다. 스토리지 관련 가용성 제공 : Gitlab 자체적으로 도커 레지스트리를 제공할 수 있다. 또한,레지스트리 이미지 및 깃랩 페이지, 등의 데이터등에 대한 저장소 스토리지로 Minio 를 사용한다. 앞선 블로그 글에서 harbor 레지스트리에 대한 고가용성 구축을 다루었는데, 깃랩에서는 자동으로 연동해주는 것 같다. 네트워크 최적화 기능 제공 : gitlab 서버(Gitlay)에 대한 로드밸런싱(Prafect) 기능을 제공하며, Workhorse라는 컴포넌트를 통해 중앙 프록시 및 파일 처리를 관리한다. 보안 : Oauth, 인증 및 권한 관리(gitlab shell) 을 제공한다. Observability 제공 : 그라파나 연동 기능, Tracing 기능을 제공한다. CI / CD 제공 : gitlab-runner 라는 컴포넌트를 통해 CI / CD 기능을 제공한다. 뭐지..? 단순히 깃 저장소를 확장하여 대부분의 addon를 자동으로 연계시켜 제공하다니, 심지어 고가용성, 최적화에 대한 구성도 자동으로 제공해준다. 기능별로 세세히 보고 싶은 마음이 굴뚝같지만, 이번 블로그글에서는 gitops 시스템 대한 내용만 다룬다. 차트에서 다음과 같은 부분을 수정하였다.\nvalues-gitops.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 global: hosts: domain: {도메인 입력} ingress: configureCertmanager: false provider: aws class: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: {ACM arn 입력} alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;gitlab\u0026#34; tls: enabled: false certmanager: install: false nginx-ingress: enabled: false prometheus: install: false gitlab-runner: install: false 헬름 차트에서는 차트 오버라이드가 가능하다. 위의 차트에서 ACM값만 수정해서 설치를 진행하자.\n1 helm install gitlab gitlab/gitlab -f values-gitops.yaml --namespace gitlab --version 6.8.4 설치 후 도메인을 통해 로그인을 진행한다. 초기 admin 계정의 아이디는 root 이며, 비밀번호는 다음의 명령어에서 확인한다.\n1 2 # 웹 root 계정 암호 확인 kubectl get secrets -n gitlab gitlab-gitlab-initial-root-password --template={{.data.password}} | base64 -d ;echo 접속하면 깃랩 프로젝트 화면이 보인다. 이어서 PKOS 스터디에서 진행한 실습 내용을 테스트하겠다. 사용자 계정을 생성하여 토큰을 발급받고, 신규 프로젝트를 파일을 업로드해보자.\nArgoCD 배포 ArgoCD는 Git 리포지토리에 저장된 쿠버네티스 매니페스트와 실제 클러스터 상태를 동기화시켜주는 지속적인 배포(Continuous Delivery, CD) 툴이다. Argo CD를 사용하면 Git 리포지토리를 기반으로 인프라 구성을 코드로 관리할 수 있게 된다. 이는 단일 진실 원천(SSOT)로 자동화, 보안, 버전 관리 측면에서 유용하다.\nArgoCD 배포는 Helm 차트로 진행한다.\n1 2 3 helm repo add argo https://argoproj.github.io/argo-helm helm repo update helm fetch argo/argo-cd --untar --version 5.19.14 ArgoCD 차트 분량도 상당하다. 아키텍처를 보니 이해가 빠른 것 같아서 먼저 공유한다.\nhttps://blog.searce.com/argocd-gitops-continuous-delivery-approach-on-google-kubernetes-engine-2a6b3f6813c0\nargo-cd-server: Argo CD API 서버와 웹 UI를 제공하는 컴포넌트이다. 사용자 인증 및 권한 관리를 처리하며, 클러스터와 Git 리포지토리 간의 동기화를 관리한다. argo-cd-repo-server: Git 리포지토리와 통신하여 사용자가 관리하는 쿠버네티스 매니페스트 파일을 가져오는 컴포넌트이다. 또한, 리포지토리 내의 Helm 차트와 Kustomize 구성을 처리한다. argo-cd-application-controller: Argo CD의 핵심 컴포넌트로, 쿠버네티스 클러스터 상태와 Git 리포지토리 상태를 비교하고 동기화를 수행한다. 클러스터의 실제 상태와 원하는 상태를 일치시키는 작업을 담당한다. argo-cd-dex-server: 인증 프록시 역할을 하는 컴포넌트로, 다양한 OAuth 및 OIDC 프로바이더와 통합하여 Argo CD 인증을 처리한다. argo-cd-redis: 캐싱 및 세션 관리를 위한 Redis 데이터베이스이다. Argo CD는 Redis를 사용하여 성능 향상과 빠른 응답 시간을 제공한다. Kustomize ?\n쿠버네티스 리소스 구성을 커스터마이징을 위한 도구이다. 동일한 구성을 가진 매니패스트에 수정할 부분만 추가해서 오버라이드가 가능하다.\n아래 예는 원본 베이스 앱(my-app) 에 dev, ops 환경별 필요 값을 오버라이드하는 예제이다.\n먼저 베이스가 되는 deployment를 선언하고 Kustomize 구성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # base/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-app image: my-app-image:latest ports: - containerPort: 80 1 2 3 4 5 # base/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml 이제 dev, Ops 환경에 대한 kustomization.yaml 파일을 생성하고 다음 내용을 추가하자.\nDev 환경 설정\n1 2 3 4 5 6 7 # overlays/dev/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - ../../base patchesStrategicMerge: - deployment-patch.yaml 1 2 3 4 5 6 7 8 9 10 11 12 # verlays/dev/deployment-patch.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 2 template: spec: containers: - name: my-app image: my-app-image:dev Ops 환경 설정\n1 2 3 4 5 6 7 # overlays/ops/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - ../../base patchesStrategicMerge: - deployment-patch.yaml 1 2 3 4 5 6 7 8 9 10 11 12 # overlays/ops/kustomization.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 4 template: spec: containers: - name: my-app image: my-app-image:ops 눈치 챘는가? Ops와 dev의 replicas 개수와 이미지 설정만 달랐고 그 부분만 추가했다. 배포는 다음 식으로 진행한다.\n1 kubectl kustomize overlays/dev | kubectl apply -f - 위의 컴포넌트별 차트에서 설정이 가능하다. 추가 설정은 아래 config.param 에서 오버라이드하는 것을 추천한다. 사용 예는 깃허브에 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 # Argo CD configuration parameters ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/argocd-cmd-params-cm.yaml params: ## Controller Properties # -- Number of application status processors controller.status.processors: 20 # -- Number of application operation processors controller.operation.processors: 10 # -- Specifies timeout between application self heal attempts controller.self.heal.timeout.seconds: 5 # -- Repo server RPC call timeout seconds. controller.repo.server.timeout.seconds: 60 깃허브의 사용 예에는 중요한 reSyncPreiod 설정(저장소 동기화 시간) 이 없는 것 같다. 이럴 때는 직접 차트를 확인하여 구성 값을 확인하고 config.params에 추가하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # argo-cd/templates/argocd-application-controller - command: - argocd-application-controller - --metrics-port={{ .Values.controller.containerPorts.metrics }} {{- if .Values.controller.metrics.applicationLabels.enabled }} {{- range .Values.controller.metrics.applicationLabels.labels }} - --metrics-application-labels - {{ . }} {{- end }} {{- end }} {{- with .Values.controller.args.statusProcessors }} - --status-processors - {{ . | quote }} {{- end }} {{- with .Values.controller.args.operationProcessors }} - --operation-processors - {{ . | quote }} {{- end }} {{- with .Values.controller.args.appResyncPeriod }} # config.params 추가 - --app-resync - {{ . | quote }} {{- end }} {{- with .Values.controller.args.appHardResyncPeriod }} - --app-hard-resync - {{ . | quote }} {{- end }} {{- with .Values.controller.args.selfHealTimeout }} - --self-heal-timeout-seconds - {{ . | quote }} {{- end }} 실제 배포는 CLB에 externalDNS 로 진행하였다.\n1 2 3 4 5 6 # values-argocd.yaml server: service: type: LoadBalancer annotations: external-dns.alpha.kubernetes.io/hostname: argocd.\u0026lt;도메인 입력\u0026gt; 배포\n1 2 kubectl create ns argocd helm install argocd argo/argo-cd -f values-argocd.yaml --namespace argocd --version 5.19.14 실제로 배포할 시 고려할 점은 접네트워크 대역이다. ArgoCD는 클러스터를 직접적으로 관리할 수 있기 때문이다. 실제 Devops 팀이나 Admin 사용자가 사용할 것이라 예상한다. 이를 위해 허용된 네트워크 대역에서만 로드밸런서 접근이 가능하도록 설정하는 것이 중요할 것 같다. 로드밸런스 설정 후 적절한 보안 그룹을 생성하여 접근을 제어하도록 하자.\n초기 admin 로그인 정보는 다음의 명령어로 확인이 가능하다.\n1 2 3 #비밀번호 ARGOPW=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) echo $ARGOPW 로그인 이후 ArgoCD에 클러스터와 깃 저장소 등록이 필요하다. ArgoCD UI 나 CLI 를 통해 확인 및 등록이 가능하다. 클러스터는 구축한 클러스터 정보가 기본으로 등록되어 있다. 향후 생산성을 위해 CLI를 통해 확인해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 최신버전 설치 curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 install -m 555 argocd-linux-amd64 /usr/local/bin/argocd chmod +x /usr/local/bin/argocd # 버전 확인 argocd version --short # argocd 서버 로그인 argocd login argocd.$KOPS_CLUSTER_NAME --username admin --password $ARGOPW WARNING: server certificate had error: x509: certificate is valid for localhost, argocd-server, argocd-server.argocd, argocd-server.argocd.svc, argocd-server.argocd.svc.cluster.local, not argocd.hanhorang.link. Proceed insecurely (y/n)? y \u0026#39;admin:login\u0026#39; logged in successfully Context \u0026#39;argocd.hanhorang.link\u0026#39; updated # argocd repo 등록 argocd repo add https://gitlab.hanhorang.link/Horang/test-stg.git --username horang --password PASSWORDa! Repository \u0026#39;https://gitlab.hanhorang.link/Horang/test-stg.git\u0026#39; added # argocd 확인 argocd repo list TYPE NAME REPO INSECURE OCI LFS CREDS STATUS MESSAGE PROJECT git https://gitlab.hanhorang.link/Horang/test-stg.git false false false true Successful argocd cluster list SERVER NAME VERSION STATUS MESSAGE PROJECT https://kubernetes.default.svc in-cluster Unknown Cluster has no applications and is not being monitored. GitOps 구축 구축한 Gilab, ArgoCD 를 통해서 GitOps 시스템을 구축해보겠다. gitops 정의대로 깃 저장소에 있는 헬름 차트가 쿠버네티스 환경에 실시간으로 동기화되는 지 테스트해보겠다. 사용 헬름 차트는 앞서 스토리지 테스트를 위해 구축한 minIO 를 대상으로 진행하겠다. 해당 차트는 필자의 깃허브에서 확인이 가능하다.\n1 2 3 helm repo add minio https://charts.bitnami.com/bitnami helm repo update helm fetch minio/minio --untar --version 12.2.1 먼저, gitlab 저장소에 헬름 차트를 PUSH한다.\n다음은 ArgoCD를 통해 동기화를 진행한다. 동기화 구성은 ArgoCD CRD로 작성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: minio-helm namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: minio server: https://kubernetes.default.svc project: default source: repoURL: https://gitlab.hanhorang.link/Horang/test-stg path: minio/ targetRevision: HEAD helm: valueFiles: - values-minio.yaml syncPolicy: syncOptions: - CreateNamespace=true spec.destination : 동기화 클러스터 대상을 나타낸다 spec.source : 깃허브 저장소를 입력한다. spec. syncpolicy : 동기화 정책을 구성한다. 옵션은 공식 문서를 참고바란다. CreateNamespace=true 옵션은 대상 네임스페이스가 없으면 생성시킨다는 옵션이다. 배포 후 ArgoCD UI에서 확인하자.\n1 kubectl apply -f minio-helm-argo.yaml 배포시 OutofSync 의 상태가 되는데 상단의 Sync App눌러 동기화를 진행한다.\n동기화시 옵션에 따라 세부 동작이 가능하다.\nPRUNE: 리포지토리에 없는 리소스를 삭제함 DRY RUN : 테스트로 실제 변경하지 않음 APPLY ONLY: 리소스 생성 및 수정만 수행하고 삭제하지 않음 FORCE : 강제 적용\nSKIP SCHEMA VALIDATION: 리소스 매니페스트의 JSON 스키마 검증을 건너뛰는 옵션이다. 이 옵션은 매니페스트에 포함된 스키마가 유효하지 않거나 검증되지 않아도 배포를 진행하고자 할 때 사용한다. AUTO-CREATE NAMESPACE: ArgoCD가 리소스를 배포할 네임스페이스가 존재하지 않는 경우 자동으로 해당 네임스페이스를 생성하는 옵션이다. PRUNE LAST: 클러스터에 존재하지 않아야 하는 리소스를 자동으로 제거하여 깔끔한 상태를 유지할 수 있도록 돕는다. APPLY OUT OF SYNC ONLY: ArgoCD가 오직 동기화되지 않은 리소스에 대해서만 kubectl apply 명령을 실행하는 옵션이다. 이렇게 하면 이미 동기화된 리소스는 건드리지 않고, 변경된 리소스에 대해서만 업데이트를 진행한다.\nRESPECT IGNORE DIFFERENCES: ArgoCD가 리소스를 비교할 때, 무시해야 하는 차이점을 존중하도록 설정하는 옵션이다. 이렇게 하면 사용자가 지정한 특정 필드의 변경 사항을 무시하고 동기화 여부를 결정할 수 있다.\nSERVER-SIDE APPLY: ArgoCD가 서버 측에서 리소스를 적용하도록 설정하는 옵션이다. 이 옵션을 사용하면, 리소스의 변경 사항이 서버 측에서 자동으로 병합되어 관리자가 수동으로 병합할 필요가 없다. 이 방식은 클라이언트 측에서 **kubectl apply**를 사용하는 것보다 더 효율적인 리소스 관리를 가능하게 한다.\nREPLACE : 리소스 변경시 기존 리소스를 삭제하고 새로운 리소스를 생성하여 대체한다.\nRETRY : 동기화 실패시 재시도\nSync 후 배포까지 모니터링 후 정상적으로 작동하는 것을 확인할 수 있다.\nArgoCD는 기본적으로 수동적으로 Sync 작업이 필요하다. 자동으로 깃 저장소에 내용으로만 동기화시키려면 self-healing 옵션이 필요하다. App Detail의 Policy 설정에서 활성화하자.\n마치며 이번 글에서는 Gitlab 와 ArgoCD 차트를 분석하여 구성하였고 GitOps 시스템을 구축해보았다. 인프라 관리자 측면에서 SSOT를 구성하면 코드 구성 관리 측면에서 편리해질 것이 느껴진다. 그리고 Gitlab와 ArgoCD 메뉴얼이 잘 정리되어 있다. 확장 기능(메트릭, 알람, 보안) 필요시 메뉴얼을 참고하자!\n","date":"Mar 25","permalink":"https://HanHoRang31.github.io/post/pkos2-3-gitops/","tags":["KANS","kops","cloud","AWS","kubernetes","GitOps","Gitlab","ArgoCD","CI/CD"],"title":"[PKOS] GitOps와 ArgoCD DeepDive"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 2주차 스터디에서는 쿠버네티스의 네트워크와 스토리지를 중점적으로 공부하였다. 분량이 많아 네트워크와 스토리지를 나눠서 블로그를 작성할 예정이다. 이번 블로그 글에서는 로컬 스토리지에 대해 공유하겠다. 일반적으로 로컬 스토리지는 IOPS 성능이 특화되어 있지만 노드에 종속되어 있어 고가용성이나 스토리지 기능에 제한이 있다. 이러한 제한을 없애기 위한 과정으로 로컬 스토리지의 Hostpath, local 볼륨을 마운트하여 테스트를 진행할 것이고, 마지막으로는 로컬 볼륨에서 고가용성과 스토리지 기능(백업)을 가진 Mysql 데이터베이스를 구성하겠다. 추가로 스토리지 성능 측정과 모니터링 과정, QnA를 준비하였다. 본론으로 들어가서, 쿠버네티스에서 스토리지를 사용하는 이유는 무엇일까? 스토리지가 데이터를 저장하는 용도인 것처럼 데이터 저장을 위해서이다. 예를 들어, 데이터베이스같은 애플리케이션을 파드로 운영한다고 가정해보자, 파드 라이프사이클과 별개로 데이터가 보존되어야 한다. 이를 위해 쿠버네티스에서는 PV(Persistent Volume)과 PVC(Persistent Volume Claim) 리소스를 제공한다. 또한, 데이터 관리 방법(데이터 저장 위치, 데이터 공유, 확장성)에 따라 여러 스토리지 볼륨과 기능을 제공한다. 이처럼 데이터를 보존해야 하는 애플리케이션을 상태있는(Stateful) 애플리케이션이라 칭하며 스토리지를 통해 클러스터 내의 컨테이너에 안정적이고 지속적인 데이터를 제공할 수 있다.\n로컬 스토리지 로컬 스토리지는 말 그대로 파드의 스토리지로 서버 내부 볼륨의 스토리지를 사용하는 것이다. AWS EC2는 내부 볼륨인 인스토어 스토어를 사용한다. 로컬 스토리지 구현은 쿠버네티스에서 HostPath, Local 볼륨 마운트로 나뉘어 사용이 가능하다. HostPath 볼륨과 Local 볼륨의 차이는 쿠버네티스 볼륨 리소스(PV) 사용 유무에 따라 구분한다.\n일반적으로 내부 볼륨의 스토리지를 사용하는 만큼, 다른 원격 스토리지와 비교했을때 IOPS 성능이 뛰어나다. cncf 공식사이트에서 IOPS 기준 약 2~3배의 차이가 난다고 하니 성능 필요의 애플리케이션에서는 도입을 고려할 만하다. 그리고 로컬스토리지는 EC2 에 종속되어 있어 고가용성 구성과 백업같은 기능 사용에 추가 구성이 필요하다. 필자는 이를 해결하기 위해 볼륨프로비저닝 플러그인인 Local-path-provisioner 와 백업 솔루션인 Velero를 사용하였다.\n먼저 Local-path-provisioner 플러그인 설치 과정을 살펴볼 것이고, 로컬스토리지 이해를 위해 3가지의 케이스로 나뉘어 테스트를 진행하겠다.\nLocal-path-provisioner PV를 통한 고가용성 테스트 Hospath PV를 통한 고가용성 테스트 파드간 데이터 동기화 구성 local-path-provisioner 볼륨 프로비저닝 플러그인 중 하나로, 로컬 노드의 파일 시스템 경로를 사용하여 PVC(Persistent Volume Claim)를 만들어주는 역할을 한다. PVC를 통해 볼륨을 요청하면 PV가 자동으로 생성되어 연결된다고 보면 된다.\n설치시, 로컬 노드에 대한 볼륨 설정이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 curl -s -O https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml vim local-path-storage.yaml ---------------------------- apiVersion: apps/v1 kind: Deployment metadata: name: local-path-provisioner namespace: local-path-storage spec: replicas: 1 selector: matchLabels: app: local-path-provisioner template: metadata: labels: app: local-path-provisioner spec: # 추가 부분 nodeSelector: kubernetes.io/hostname: \u0026#34;마스터 노드 이름 입력 \u0026#34; tolerations: - effect: NoSchedule key: node-role.kubernetes.io/control-plane operator: Exists ... kind: ConfigMap apiVersion: v1 metadata: name: local-path-config namespace: local-path-storage data: config.json: |- { \u0026#34;nodePathMap\u0026#34;:[ { \u0026#34;node\u0026#34;:\u0026#34;DEFAULT_PATH_FOR_NON_LISTED_NODES\u0026#34;, \u0026#34;paths\u0026#34;:[\u0026#34;/data/local-path\u0026#34;] # 추가 부분 } ] } ---------------------------- Deployment / spec.spec 에서 nodeselector 와 tolerations 로 볼륨 배치 노드 설정(마스터 노드로 파드 배치) Configmap / data config.json에서 로컬 볼륨 PATH 설정 설치 확인\n1 2 3 4 kubectl get sc local-path ---------------------------- NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path rancher.io/local-path Delete WaitForFirstConsumer false 29m Case 1. Local-path-provisioner PV를 통한 고가용성 테스트 HostPath 볼륨이 IOPS 성능이 좋은 것을 앞서 확인하였다. 성능적으로 사용하기 좋은 볼륨이라 할 수 있으나 고가용성 구성이 필요하다.\nHostpath 볼륨의 문제점으로 노드간 마이그레이션에 자유롭지 못하기 때문이다. 이해를 위해 직접 예제 파드를 배포해보고 고가용성을 테스트 해보겠다.\nLocal-path-provisioner PV를 통한 고가용성 테스트\n앞서 배포한 Local-path-provisioner 를 통해 PV를 생성하여 파드를 배포하고 고가용성 구성을 테스트해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # 파드 예제 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: localpath-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi storageClassName: \u0026#34;local-path\u0026#34; --- apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: pod-persistent-volume mountPath: /data volumes: - name: pod-persistent-volume persistentVolumeClaim: claimName: localpath-claim 파드 배포 후 노드 드레인을 진행해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 배포 파드의 노드 확인 PODNODE=$(kubectl get pod -l app=date -o jsonpath={.items[0].spec.nodeName}) echo $PODNODE # 노드 드레인과 파드 모니터링 kubectl drain $PODNODE --force --ignore-daemonsets --delete-emptydir-data \u0026amp;\u0026amp; kubectl get pod -w --------------------------------- node/i-0c41dc0f6eeb01730 cordoned Warning: ignoring DaemonSet-managed Pods: kube-system/aws-node-w8bxm, kube-system/ebs-csi-node-thndq, kube-system/node-local-dns-v2hhc evicting pod default/date-pod-d95d6b8f-q9skb evicting pod kube-system/metrics-server-5f65d889cd-9btc7 pod/metrics-server-5f65d889cd-9btc7 evicted pod/date-pod-d95d6b8f-q9skb evicted node/i-0c41dc0f6eeb01730 drained NAME READY STATUS RESTARTS AGE date-pod-d95d6b8f-x5vrb 0/1 Pending 0 2m 노드 드레인시, 상태가 Pending 인 것을 확인할 수 있다. 이는 다른 노드에 PV볼륨이 없기 때문이다.\n마찬가지로 파드를 5개로 추가해도 볼륨이 있는 노드에만 파드가 올라온 것을 확인할 수 있다.\n1 2 # 예제 파드 개수 5개로 증가 kubectl scale deployment date-pod --replicas=5 Case2. HostPath를 통한 고가용성 테스트 볼륨 Hostpath로 노드 PATH를 직접 지정하여 고가용성을 테스트해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: log mountPath: /data volumes: - name: log hostPath: path: \u0026#34;/data\u0026#34; type: DirectoryOrCreate 파드 드레인과 개수 조절시 노드에 상관없이 배포가 진행된다.\n하지만, 볼륨별로 데이터가 쌓이는 것이 다르다. 각 노드에 들어가서 로그를 확인해보면 찍히는 것이 다른 것을 확인할 수 있다.\n고가용성이라 할 수 있지만 적재되어 있는 데이터가 달라 stateful 애플리케이션(ex. MySQL)을 운영하기엔 한계가 있다.\n참고 책에서는 이러한 제약사항을 해결하기 위해서는 애플리케이션 단에서 다른 노드의 파드와 데이터를 동기화해서 해결할 수 있다고 한다.\n동기화 방법을 찾아보니 NFS 볼륨(ex. AWS EFS)을 구성하여 HostPath 를 연결하거나, 볼륨간 rsync를 사용하라 나오지만, 성능(로컬SSD가 아님)이 떨어져 해결 방법은 아닌 것 같다.\nCase3. 파드간 데이터 동기화 구성 그렇다면 성능 좋고, 고가용성도 보장되고, 데이터 동기화를 보장할 수 있는 Stateful 애플리케이션을 구성할 수 있을까?\n쿠버네티스 공식문서 예제에서 이를 확인할 수 있는데 해당 예제를 구성해보고 테스트해보겠다. 해당 예제에서는 데이터베이스 리소스로 StatefulSet를 사용한다.\nStatefulSet 리소스는 이름처럼 Stateful한 애플리케이션을 위해 만든 리소스이다. StatefulSet 리소스의 특징은 다음과 같다.\nStatefulSet 리소스의 특징\nPod 이름\nStatefulSet에 의해 생성된 파드들은 {Pod 이름}-{순번} 식으로 이름이 정해진다. 이는 클러스터 내부 환경에서 데이터베이스에 접근할 때 사용하기 위해서이다.\n파드 순차적 배포\nPod 생성시 모든 Pod 가 동시에 생성되지 않고 순서대로 하나씩 생성된다. 이는 데이터베이스에서 마스터 파드 → 슬레이브 파드로 기동해야 하는 조건등에서 유용하게 사용 될 수 있다.\n파드별 볼륨 마운트\n일반적으로 PVC \u0026amp; PV에 중복적으로 Pod를 사용할 수 없다. 연결된 Pod가 존재하면 그 다음 파드들은 PVC를 얻지 못해 볼륨을 사용하지 못한다. 반면, Statefulset에서 PVC를 템플릿 형태로 정의하여 Pod마다 PVC, PV를 생성하여 파드별로 볼륨을 마운트할 수 있게 된다.\n다시 돌아가서 쿠버네티스 공식문서의 예제는 클러스터에 MySQL 스테이트풀셋이 배포되고 각 레플리카에 순서대로 배포되는 예제이다. 중요하게 볼 점은 스테이트 풀셋의 매니페스트이다.\n해당 StatefulSet 매니페스트는 3개의 replica를 가진 MySQL을 생성한다. init 컨테이너는 두개 배포되며, init-container는 MySQL init 설정을 수행하고, xtrabackup init-container는 MySQL 클러스터 복제를 위해 데이터를 클론하여 동기화를 진행한다. init 컨테이너 이후 MySQL 컨테이너는 데이터베이스 작업을 수행하며, xtrabackup 컨테이너는 클론 작업을 진행한다.\n공식 문서의 예제를 그대로 배포하면 스토리지 클래스가 default로 EBS 볼륨(외부)에 연결된다. 이대로 진행하면 local-path-provisioner에서 배포한 로컬 볼륨에서 마운트되지 않는다.\n로컬 볼륨에 마운트하기 위해서는 추가 작업이 필요하다.\nlocal-path-provisioner 배포 파일 수정\nlocal-path-provisioner 파드를 워크 노드에만 배포하도록 설정한다. 해당 설정을 통해 워크 노드에만 로컬 볼륨을 생성하고 파드에 연결할 수 있도록 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim local-path-storage.yaml ---------------------------- apiVersion: apps/v1 kind: Deployment metadata: name: local-path-provisioner namespace: local-path-storage spec: replicas: 1 selector: matchLabels: app: local-path-provisioner template: metadata: labels: app: local-path-provisioner spec: spec: # 아래 부분 추가 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/node operator: Exists 스테이트 풀셋의 매니페스트 내 스토리지클래스 지정 1 2 3 4 5 6 7 8 9 volumeClaimTemplates: - metadata: name: data spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: local-path # 로컬 볼륨 추가 resources: requests: storage: 10Gi 스테이트 풀셋의 매니페스트 replica count 를 워크노드(2) 개수 만큼 수정 1 2 3 serviceName: mysql replicas: 2 # 수정 template: 배포 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl apply -f ./ ---------------------------- configmap/mysql created service/mysql created service/mysql-read created statefulset.apps/mysql created kubectl get pods ---------------------------- NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 31m mysql-1 2/2 Running 0 31m 동기화 테스트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # 파드 0에서 Mysql Data 생성 kubectl exec -it pod/mysql-0 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, init-mysql (init), clone-mysql (init) --------------------------------- bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 140 Server version: 5.7.41-log MySQL Community Server (GPL) Copyright (c) 2000, 2023, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. --------------------------------- mysql\u0026gt; create database testdb; --------------------------------- Query OK, 1 row affected (0.01 sec) --------------------------------- mysql\u0026gt; use testdb; --------------------------------- Database changed --------------------------------- mysql\u0026gt; create table test(name varchar(10), testdata varchar(50)); --------------------------------- Query OK, 0 rows affected (0.02 sec) --------------------------------- mysql\u0026gt; insert into test values(\u0026#39;han\u0026#39;, \u0026#39;mysql example test\u0026#39;); --------------------------------- Query OK, 1 row affected (0.01 sec) --------------------------------- mysql\u0026gt; select * from test; --------------------------------- +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.00 sec) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 파드 1에서 확인 kubectl exec -it pod/mysql-1 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, init-mysql (init), clone-mysql (init) --------------------------------- bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 306 Server version: 5.7.41 MySQL Community Server (GPL) Copyright (c) 2000, 2023, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. --------------------------------- mysql\u0026gt; use testdb --------------------------------- Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed --------------------------------- mysql\u0026gt; select * from test; --------------------------------- +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.00 sec) 됐다! 로컬 볼륨 기반으로 Mysql 예제를 배포하였고 고가용성 구성을 위해 동기화까지 진행을 완료했다!\n로컬 볼륨 백업과 복원 쿠버네티스에서는 HostPath 볼륨의 대한 백업과 복원 기능은 지원하지 않는다. 노드 별로 백업 스크립트를 작성하거나 써드파티 솔루션을 사용해야 한다. 이번 절에서는 local-path-provisioner 을 사용해서 볼륨을 프로비저닝한만큼, 해당 볼륨에 맞게 백업을 할 수 있는 써드파트 솔루션을 찾아보았다.\nlocal-path-provisioner 깃허브 이슈를 찾아보니 Velero 솔루션을 이용해서 백업을 할 수 있다고 하여 테스트를 진행해보고자 한다.\nVelero? 는 쿠버네티스 클러스터의 리소스와 퍼시스턴트 볼륨을 백업하고 복원하는 데 사용되는 오픈 소스 툴이다.\nVelero 을 사용하기전 local-path-provisioner 볼륨 타입을 Local로 수정해야 한다. Hostpath 볼륨을 지원하지 않으나 Local 볼륨은 Restic 과 연계하여 백업을 지원하기 때문이다. (공식문서)\n이는 Local 볼륨이 쿠버네티스의 자원으로 관리되며, 스토리지 클래스와 퍼시스턴트 볼륨 클레임(PVC)을 사용할 수 있기 때문으로 보인다.\nlocal-path-provisioner 사전 작업\nLocal 볼륨을 수정하기 위해서는 local-path-provisioner 파드의 수정 작업이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 volumeClaimTemplates: - metadata: name: data annotations: volumeType: local # 추가 spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: local-path resources: requests: storage: 10Gi 로컬 볼륨 확인\n1 2 3 4 5 6 kubectl get pv \u0026lt;pv-name\u0026gt; -o yaml --------------------------------- spec: local: # local or HostPath 볼륨 path: /mnt/local-storage/ssd/vol1 ... Velero 설치\n필자는 Velero 백업 버킷을 AWS S3 설정하여 설치를 진행하였다.\n설치는 S3 버켓 생성 및 설정 / Veleo CLI 로 나뉜다.\nS3 버켓 지정 및 IAM 설정\nVelero 에서 S3 버킷을 접근하기 위한 IAM USER ID와 KEY 생성\n1 aws s3 mb s3://\u0026lt;bucket-name\u0026gt; --region ap-northeast-2 IAM Policy 생성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 # 버킷 변수 설정 export BUCKET=\u0026lt;bucket-name\u0026gt; # IAM Policy 생성 cat \u0026gt; velero-policy.json \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:DescribeSnapshots\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateVolume\u0026#34;, \u0026#34;ec2:CreateSnapshot\u0026#34;, \u0026#34;ec2:DeleteSnapshot\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${BUCKET}/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${BUCKET}\u0026#34; ] } ] } EOF # IAM Policy Attach aws iam put-user-policy \\ --user-name velero \\ --policy-name velero \\ --policy-document file://velero-policy.json # IAM user 정보 가져오기 aws iam create-access-key --user-name velero --------------------------------- { \u0026#34;AccessKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;velero\u0026#34;, \u0026#34;AccessKeyId\u0026#34;: \u0026#34;{ID}\u0026#34;, # 밑의 credentials-velero ID에 저장 \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;{KEY}\u0026#34;, # 밑의 credentials-velero KEY에 저장 \u0026#34;CreateDate\u0026#34;: \u0026#34;2023-03-16T04:31:23+00:00\u0026#34; } } # credentials-velero 생성 및 IAM 정보 저장 cat \u0026lt;\u0026lt; EOF \u0026gt; credentials-velero [default] aws_access_key_id=\u0026lt;AWS_ACCESS_KEY_ID\u0026gt; aws_secret_access_key=\u0026lt;AWS_SECRET_ACCESS_KEY\u0026gt; EOF Velero CLI 설치 후 서버 설치\nrestic은 버전 velero 1.10(최신버전) 이상에서 더 이상 지원되지 않는다. 버전을 1.9.6으로 맞춰서 다운받아야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # arch 확인 uname -m --------------------------------- x86_64 # velero CLI 설치 wget https://github.com/vmware-tanzu/velero/releases/download/v1.9.6/velero-v1.9.6-linux-amd64.tar.gz tar xzvf velero-v1.9.6-linux-amd64.tar.gz cp velero-v1.9.6-linux-amd64/velero ~/bin # CLI 확인 velero --------------------------------- Velero is a tool for managing disaster recovery, specifically for Kubernetes cluster resources. It provides a simple, configurable, and operationally robust way to back up your application state and associated data. If you\u0026#39;re familiar with kubectl, Velero supports a similar model, allowing you to execute commands such as \u0026#39;velero get backup\u0026#39; and \u0026#39;velero create schedule\u0026#39;. The same operations can also be performed as \u0026#39;velero backup get\u0026#39; and \u0026#39;velero schedule create\u0026#39;. ... Velero 설치\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 export BUCKET=\u0026lt;bucket-name\u0026gt; export REGION=ap-northeast-2 velero install \\ --provider aws \\ --bucket $BUCKET \\ --secret-file ./credentials-velero \\ --backup-location-config region=$REGION \\ --use-volume-snapshots=false \\ --plugins velero/velero-plugin-for-aws:v1.3.0 \\ --use-restic --------------------------------- ... Deployment/velero: created DaemonSet/restic: attempting to create resource DaemonSet/restic: attempting to create resource client DaemonSet/restic: created Velero is installed! ⛵ Use \u0026#39;kubectl logs deployment/velero -n velero\u0026#39; to view the status. # Velero 확인 kubectl get all -n velero NAME READY STATUS RESTARTS AGE pod/restic-f5ngz 1/1 Running 0 38s pod/restic-x9sk9 1/1 Running 0 37s pod/velero-5f6657d4c8-jttxv 1/1 Running 0 38s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/restic 2 2 2 2 2 \u0026lt;none\u0026gt; 38s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/velero 1/1 1 1 38s NAME DESIRED CURRENT READY AGE replicaset.apps/velero-5f6657d4c8 1 1 1 38s Velero 백업\nVelero는 Restic을 사용하여 PV 볼륨에 대해 백업하는 방법에 두 가지 접근 방식을 지원한다. (공식문서)\n옵트인 접근 방식(default): Restic을 사용하여 백업할 볼륨이 포함된 모든 포드에 annotation을 달아야 한다. 옵트아웃 접근 방식: 모든 포드 볼륨이 Restic을 사용하여 백업되고 백업되지 않아야 하는 볼륨을 옵트아웃할 수 있는 기능이 있다. 이번 절에서는 옵트인 접근 방식을 택할 것이고 Case3 의 Mysql 볼륨을 백업할 예정이다. 백업할 볼륨에 대해 backup.velero.io/backup-volumesannotation 달고 백업을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 주석 추가 pod에 볼륨 정보를 추가 kubectl annotate pod/mysql-0 backup.velero.io/backup-volumes=data # 백업 velero backup create mysql --include-namespaces default --wait --------------------------------- Backup request \u0026#34;mysql\u0026#34; submitted successfully. Waiting for backup to complete. You may safely press ctrl-c to stop waiting - your backup will continue in the background. .................. Backup completed with status: Completed. You may check for more information using the commands `velero backup describe mysql` and `velero backup logs mysql`. # 백업 목록 확인 velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-16 14:22:39 +0900 KST 29d default \u0026lt;none\u0026gt; S3 버킷 조회할 수 있다.\nS3 조회시, backups(쿠버네티스 리소스 저장 경로) 와 restic(PV 볼륨 데이터 저장 경로)에 데이터를 확인할 수 있다.\nVelero 복원\nmysql 배포 파일과 PV를 지우고 복원을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #mysql 지우기 kubectl delete -f ./ kubectl delete pvc/\u0026lt;PVC 볼륨\u0026gt; #velero 복원 velero restore create --from-backup mysql --wait --------------------------------- Restore request \u0026#34;mysql-20230316155542\u0026#34; submitted successfully. Waiting for restore to complete. You may safely press ctrl-c to stop waiting - your restore will continue in the background. ........... Restore completed with status: Completed. You may check for more information using the commands `velero restore describe mysql-20230316155542` and `velero restore logs mysql-20230316155542`. # 쿠버네티스 리소스 복원 확인 kubectl get all --------------------------------- NAME READY STATUS RESTARTS AGE pod/mysql-0 2/2 Running 0 39s pod/mysql-1 0/2 Init:CrashLoopBackOff 2 (18s ago) 39s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 4h39m service/mysql ClusterIP None \u0026lt;none\u0026gt; 3306/TCP 39s service/mysql-read ClusterIP 100.69.52.194 \u0026lt;none\u0026gt; 3306/TCP 39s kubectl get pv --------------------------------- NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-601b919a-cf20-4478-9f28-10d541c66844 10Gi RWO Delete Bound default/data-mysql-0 local-path 71s pvc-b8a766a6-411f-47df-a548-d6b0ee091ea1 10Gi RWO Delete Bound default/data-mysql-1 local-path 70s # Mysql data 확인 kubectl exec -it pod/mysql-0 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, restic-wait (init), init-mysql (init), clone-mysql (init) bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 73 Server version: 5.7.41-log MySQL Community Server (GPL) mysql\u0026gt; use testdb; --------------------------------- Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed --------------------------------- mysql\u0026gt; select * from test; +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.01 sec) 데이터가 그대로 보존되어 있다!\nVelero 스케쥴 백업\n크론탭처럼 백업도 Velero가 가능하다. 다음은 5분마다 백업을 진행하는 예제이다.\n백업된 오브젝트별 데이터 변화를 위해 데이터베이스의 데이터를 수정 후 복원을 해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 velero schedule create mysql-crontab --include-namespaces default --schedule=\u0026#34;*/5 * * * *\u0026#34; velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-16 15:51:44 +0900 KST 29d default \u0026lt;none\u0026gt; mysql-crontab-20230316072517 Completed 0 0 2023-03-16 16:25:17 +0900 KST 29d default \u0026lt;none\u0026gt; mysql-crontab-20230316072017 Completed 0 0 2023-03-16 16:20:17 +0900 KST 29d default \u0026lt;none\u0026gt; #DB 접속 후 데이터 삭제 select * from test; +------+---------------------+ | name | testdata | +------+---------------------+ | han | mysql example test | | han | mysql example test2 | | han | mysql example test3 | +------+---------------------+ 3 rows in set (0.00 sec) mysql\u0026gt; delete from test; Query OK, 3 rows affected (0.00 sec) mysql\u0026gt; select * from test; Empty set (0.00 sec) #velero 복원 velero restore create --from-backup mysql-crontab-20230316072017 --wait --------------------------------- Restore request \u0026#34;mysql-crontab-20230316072017-20230316163030\u0026#34; submitted successfully. Waiting for restore to complete. You may safely press ctrl-c to stop waiting - your restore will continue in the background. Restore completed with status: Completed. You may check for more information using the commands `velero restore describe mysql-crontab-20230316072017-20230316163030` and `velero restore logs mysql-crontab-20230316072017-20230316163030`. # Mysql DATA 확인 mysql\u0026gt; select * from test; Empty set (0.00 sec) Mysql DATA 확인 의 결과가 예상과 다르다. 백업 진행 이후 백업 데이터인 행3개가 있어야 하나, 최신 데이터가 조회된다.\n이는 Mysql 파드가 2개 있어 파드간 데이터 무결성이 보장되어 백업 데이터 파일을 옮긴다 한들 수정이 안되기 때문이다. 따라서 백업본의 결과를 얻기 위해서는 PV 볼륨과 Mysql 리소스를 지우고 다시 복원을 해야한다.\nVelero 클러스터 마이그레이션\n클러스터간 마이그레이션 방법으로 Velero를 활용할 수 있다. 공식문서를 참고하여 테스트를 진행해보겠다.\n진행 전 Velero 에서 클러스터 마이그레이션시 제약사항이 있으니 확인이 필요하다.\nVelero는 기본적으로 클라우드 공급자 간에 PV 스냅샷의 마이그레이션을 지원하지 않는다. 이를 위한 방안으로 restic을 이용하여 파일시스템 레벨의 마이그레이션을 진행해야 한다. Velero는 백업이 수행된 위치보다 낮은 Kubernetes 버전이 있는 클러스터로의 복원을 지원하지 않는다. 동일한 버전의 Kubernetes를 실행하지 않는 클러스터 간에 워크로드를 마이그레이션하는 것이 가능할 수 있지만 마이그레이션 전에 각 사용자 정의 리소스에 대한 클러스터 간 API 그룹의 호환성을 포함하여 몇 가지 요인을 고려해야 한다. AWS 및 Azure용 Velero 플러그인은 리전 간 데이터 마이그레이션을 지원하지 않는다. 이를 위한 방안으로 restic을 사용해야 한다. 우리는 restic을 사용하니 제약사항에 자유롭다. 클러스터 마이그레이션의 예에 대한 일환으로 클러스터를 재 구축하여 앞서 생성한 Mysql 애플리케이션을 복원해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # velero 설치 export BUCKET=hanhorang-velero-s3 export REGION=ap-northeast-2 velero install \\ --provider aws \\ --bucket $BUCKET \\ --secret-file ./credentials-velero \\ --backup-location-config region=$REGION \\ --use-volume-snapshots=false \\ --plugins velero/velero-plugin-for-aws:v1.3.0 \\ --use-restic --------------------------------- velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-17 00:31:21 +0900 KST 29d default \u0026lt;none\u0026gt; velero 백업 개체가 그대로 있다. 복원 과정은 앞 과정과 동일하기에 생략하였다. 클러스터를 재구축해도 Velero 백업 객체가 남아있는 이유가 무엇일까?\nVelero 리소스는 오브젝트 스토리지의 백업 파일과 동기화되기 때문이다. 설치 과정에서 삭제한 클러스터와 새로운 클러스터의 Velero 버킷이 동일하므로 백업 객체가 그대로 있음을 확인하였다. 클러스터1과 클러스터2가 병행 운영시에도 버킷 데이터에 따라 Velero 리소스가 동기화가 이루어진다는데 기본 동기화 간격이 1분으로 이 부분을 확인하여 마이그레이션을 진행하면 될 것 같다.\n로컬 볼륨 모니터링 PV 볼륨 성능 확인할 수 있는 krew df-pv 도구가 있으나, HostPath 볼륨은 인스토어스토어라서 확인되지 않는다. 하지만 Local 볼륨은 확인이 가능하다.\n1 2 3 4 5 kubectl krew df-pv \u0026amp;\u0026amp; kubectl df-pv --------------------------------- PV NAME PVC NAME NAMESPACE NODE NAME POD NAME VOLUME MOUNT NAME SIZE USED AVAILABLE %USED IUSED IFREE %IUSED pvc-678e7407-9f76-4fd1-a9ad-8c2581b8df36 data-mysql-0 default i-0314088c74eee3276 mysql-0 data 123Gi 4Gi 119Gi 3.68 119172 16395900 0.72 pvc-d2c3cbcb-13ec-46de-81e9-1178d25dd4ad data-mysql-1 default i-0ab66ac834dc8710d mysql-1 data 123Gi 4Gi 119Gi 3.74 121203 16393869 0.73 성능 측정 로컬 스토리지의 성능 측정 방법으로 iostat 명령어와 krew 툴인 kubestr을 사용하여 성능을 측정하겠다.\nkubestr 스토리지 IOPS 측정 툴이다. 스토리지 사용에 따른 검증용으로 사용하기 좋은 툴인 것 같다. 예제도 많으니 링크를 통해 확인하자. 이번 예제에서는 실습 참고용 책에서 제공해주신 예제 스크립트를 통해 성능을 측정할 것이다.\nfio-read.fio\nfio를 사용하여 4KB 블록 크기를 가지는 랜덤 읽기 및 쓰기\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [global] ioengine=libaio direct=1 bs=4k runtime=120 time_based=1 iodepth=16 numjobs=4 # numjobs=16 size=1g group_reporting rw=randrw rwmixread=100 rwmixwrite=0 [read] ioengine=libaio : Asynchronous I/O를 수행하기 위해 libaio 라이브러리를 사용합니다. direct=1 : Direct I/O를 사용합니다. bs=4k : I/O 요청에 사용되는 블록 크기는 4KB입니다. runtime=120 : 120초 동안 작업을 실행합니다. time_based=1 : 시간 기반으로 작업을 수행합니다. iodepth=16 : 각 작업에 대한 I/O 요청 수를 16개로 설정합니다. numjobs=4 : 4개의 작업을 수행합니다. size=1g : 각 작업에 대한 데이터 크기는 1GB입니다. group_reporting : 모든 작업 결과를 통합하여 보고합니다. rw=randrw : 랜덤 읽기 및 쓰기 작업을 수행합니다. rwmixread=100 : 작업 중 읽기 작업의 비율은 100%입니다. rwmixwrite=0 : 작업 중 쓰기 작업의 비율은 0%입니다. fio-write.fio\n루트 디렉토리에서 4KB 블록 크기로 16개의 job이 16개의 i/o depth로 실행되며, 실행 시간이 120초인 1GB 파일에 대해 100% 쓰기 랜덤 테스트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [global] ioengine=libaio numjobs=16 iodepth=16 direct=1 bs=4k runtime=120 time_based=1 size=1g group_reporting rw=randrw rwmixread=0 rwmixwrite=100 directory=/ [read] rwmixwrite=100: 100% 쓰기 테스트를 수행합니다. directory=/: 테스트할 디렉토리를 루트 디렉토리로 설정합니다. 성능 측정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubestr fio -f fio-write.fio -s local-path --size 10G --------------------------------- PVC created kubestr-fio-pvc-hp69m Pod created kubestr-fio-pod-fcvcp Running FIO test (fio-write.fio) on StorageClass (local-path) with a PVC of Size (10G) Elapsed time- 4m11.664545514s FIO test results: FIO version - fio-3.30 Global options - ioengine=libaio verify= direct=1 gtod_reduce= JobName: blocksize= filesize= iodepth= rw= write: IOPS=3023.577881 BW(KiB/s)=12094 iops: min=992 max=8640 avg=3023.464355 bw(KiB/s): min=3968 max=34564 avg=12093.908203 Disk stats (read/write): nvme0n1: ios=0/362587 merge=0/173 ticks=0/6330627 in_queue=6330627, util=99.954132% fio-write 실행 결과, 쓰기 평균 iops가 3034인 것을 확인할 수 있다.\nQ. 테스트가 안될 경우, PV 상태 Pending?\n해당 경우는 PVC 요청에 맞는 볼륨의 PV가 없을때 발생한다. PVC 요청에 맞는 볼륨이 있는지 또는 Local-path-provisioner 설정을 확인하자. 필자의 경우 Local-path-provisioner 설정으로 Pending 이 발생했다.\n1 2 3 4 kubectl get pvc -A --------------------------------- NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default kubestr-fio-pvc-dl7vx Pending local-path 4m34 노드 볼륨 IO 성능 측정 iostat 명령어를 통해 실시간으로 스토리지의 성능과 사용량에 관한 정보를 확인할 수 있다. 클러스터 운영시 스토리지 트러블슈팅의 일환으로 사용하자.\n아래 예제는 fio-write 스크립트 실행 중 iostat 를 실행하여 스토리지 정보를 확인한 예제이다.\n1 2 3 4 5 6 7 # iostat 패키지 설치 sudo apt install -y sysstat iostat -xmdz 1 -p nvme2n1 --------------------------------- Device r/s rMB/s rrqm/s %rrqm r_await rareq-sz w/s wMB/s wrqm/s %wrqm w_await wareq-sz d/s dMB/s drqm/s %drqm d_await dareq-sz aqu-sz %util nvme2n1 0.00 0.00 0.00 0.00 0.00 0.00 24.00 0.10 2.00 7.69 0.75 4.33 0.00 0.00 0.00 0.00 0.00 0.00 0.02 4.00 w/s: 초당 쓰기 요청 수 wMB/s: 초당 쓴 데이터 양 (메가바이트/초) wrqm/s: 초당 쓰기 요청 큐에 들어간 요청 수 %wrqm: 쓰기 요청 큐에 들어간 요청 비율 w_await: 쓰기 요청 대기 시간, 드라이버 요청 대기열에서 기다린 시간과 장치의 I/O 응답시간을 모두 포함한다. (밀리초) wareq-sz: 평균 쓰기 요청 크기 (섹터) aqu-sz: 요청 대기열의 평균 길이 %util: 디스크 사용률 (0 ~ 100%) 로컬 스토리지 QnA Q1. AWS 인스토어 스토어에 대한 볼륨 스토리지 조절이 가능한가?\n인스턴스 스토어는 EC2 인스턴스의 로컬 디스크를 사용하는 것이기 때문에 크기 조정이 불가능하다. 인스턴스 스토어를 사용하는 EC2 인스턴스를 변경하거나 새로운 인스턴스를 시작하여 크기를 조정해야 한다. 일반적으로 DB(Mysql) 볼륨 사용량으로 10G~100G을 설정한다고 하지만, 애플리케이션 규모와 기간에 따라 사용량 예측이 힘들다. 필요시 백업을 통해 인스토어 스토어 볼륨을 변경할 수 있도록 하자. EC2 인스턴스에 따른 볼륨(SSD) 는 링크에서 확인이 가능하다.\nQ2. hostpath 볼륨을 여러개의 파드가 동시에 사용할 수 있을까?\n노드의 파일 시스템은 여러 개의 프로세스가 동시에 접근할 수 있는 공유 리소스이기 때문에 여러 개의 파드가 하나의 hostpath를 사용할 수 있다. 하지만, 데이터 손상이나 권한 오류가 발생할 수 있다. 하나의 파드가 파일을 쓴 후에 다른 파드가 동일한 파일을 읽을 경우나, 여러 개의 파드가 동시에 동일한 파일을 쓰는 경우가 있기 때문이다. 이를 위해 적절한 동기화 및 락 메커니즘을 구현이 필요하다.\n테스트로 스터디에서 공유해주신 localpath-fail.yaml 를 수정해서 로그를 확인해봤다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 0.01; done\u0026#34;] # 0.01 로 수정 volumeMounts: - name: pod-persistent-volume mountPath: /data volumes: - name: pod-persistent-volume persistentVolumeClaim: claimName: localpath-claim 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 파드 10개로 증가후 테스트 kubectl scale deployment date-pod --replicas=10 -------------------------------------- # 로그 확인 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:05 | wc -l 355 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:05 | wc -l 355 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:06 | wc -l 457 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:07 | wc -l 511 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:08 | wc -l 513 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:09 | wc -l 530 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:10 | wc -l 506 0.01초 x 10 개 파드로 1초당 약 1000개의 결과가 나와야 하지만 350~530 개의 결과가 나오는 것을 확인하였다. 앞서 kubestr 측정에서 쓰기 IOPS를 측정하여 약 3000이 나왔음에도 턱없이 부족한 것을 알 수 있다.\nQ3. Velero 의 백업 용량은 몇 인가?\n사용 버킷에 따라 달라진다. S3 버킷 기준으로는 해당 버킷의 용량에 따라가는데 최대 5테라까지 지원이 가능하다.\n","date":"Mar 18","permalink":"https://HanHoRang31.github.io/post/pkos2-2-localstorage/","tags":["KANS","kops","cloud","AWS","kubernetes","Volume","velero","local-path-provisioner"],"title":"[PKOS] 쿠버네티스 로컬스토리지와 Velero를 통한 백업 테스트"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. PKOS 1주차 스터디 내용과 느낌점을 정리하고자 한다. 스터디하면서 매번 느끼는 거지만 정말 괴수분들 너무 많고, 배울 점이 정말 많다… 특히 모임장님이신 가시다님의 스터디 내용은 볼 때마다 감탄만 나온다. 쿠버네티스에 대한 원리와 세부 컴포넌트에 대한 명령어까지 배운 점이 많다. 본 글에서는 필자가 배운 내용을 정리함과 동시에 개념에 대해 몰랐던 부분이나, 트러블슈팅에 대한 내용을 중점적으로 정리하였다.\n추가로, 몰랐던 부분은 ChatGPT를 활용하여 작성하였다. ChatGPT가 주는 답변은 대체로 만족하지만, 공식 문서에 대한 내용과 비교하여 다른 내용이 일부 존재한다. ChatGPT 활용시, 공식 문서와 이중 검증이 필요하다고 본다. 본 글에서도 답변 내용을 일부 수정하여 작성한다.\nkops? kops는 쿠버네티스 클러스터를 생성, 업그레이드, 관리하는 데 사용되는 오픈 소스 도구이다. 명령어 툴로 쉽게 쿠버네티스 클러스터를 구성하고 관리할 수 있는 툴이라고 이해하면 되겠다. 쿠버네티스 클러스터의 인프라를 코드로 정의하고 관리하는데 사용할 수 있어 IaC 이며, 같은 IaC 툴인 Terraform(테라폼)과의 특징을 비교하면 다음과 같다.\nkops과 Terraform 특징 비교 표\n특징 kops Terraform 지원하는 클라우드 플랫폼(Provider) AWS, GCP, OpenStack, DigitalOcean 등 AWS, GCP, Azure, Oracle Cloud, Alibaba Cloud, VMware, OpenStack 등 관리 대상 Kubernetes 클러스터 인프라스트럭처 (서버, 네트워크, 데이터베이스 등) 코드 작성 방식 YAML 파일 HCL (HashiCorp Configuration Language) 상태 관리 상태를 지정된 저장소(state)에 저장 상태를 지정된 저장소(backend)에 저장 장점 간단하고 직관적인 클러스터 구성 AWS뿐만 아니라 다른 클라우드 프로바이더도 지원 단점 Kubernetes 클러스터만 지원 Kops에 비해 배우기가 어려움 그렇다면 kops는 언제 써야 할까?\n모임장님 의견과 동일하게 교육용이 적합하다고 본다. 쿠버네티스 클러스터 구축이 간단하고 배우기 쉽다. 또한, 배포 속도도 빠르다. 쿠버네티스 관리형 서비스인 EKS 와 비교했을 때, 마스터 노드들을 세부적으로 알 수 있어 세부 원리 이해에 좋고, 비용도 저렴하다. kops 공식문서도 정리가 잘 되어 있다. 눈여겨 볼 점은 공식문서의 addon과 Operation 부분이다. 클러스터 관리를 및 addon 배포를 간단하게 테스트할 수 있고 배포 yaml를 확인할 수 있기 때문이다.\n실습 스터디 내용으로 kops를 통해 AWS에 쿠버네티스 클러스터를 구축하고, 게임 마리오를 예제로 배포하였다. 과정은 크게 3가지로 진행하였다.\n베스천 서버(kops-ec2) 구성 kops 를 통한 클러스터 구축 및 확인 External DNS 와 게임, 슈퍼마리오 배포 실습 과정 전 사전 작업으로 퍼블릭 도메인 구입, 키 페어 생성, S3 버킷 생성, AWS IAM 자격 증명을 진행하였다. 사전 작업 내용은 공식 문서에서 참고가 가능하다.\nQ. 퍼블릭 도메인 구입 이유 ?\n퍼블릭 도메인은 쿠버네티스 클러스터 이름으로 사용하기 위하여 구입하였다. 클러스터 이름을 도메인으로 설정하면 외부에서 서비스 디스커버리 및 클러스터 액세스가 쉽게 가능해지기 때문이다. 스터디에서는 해당 도메인을 통해 클러스터 구성 확인과 게임 배포 후 접근을 위해 사용하였다.\n1. 베스천 서버(kops-ec2) 구성 베스천 서버 구성은 모임장님이 공유해주신 CloudFormation 템플릿을 통해 구성하였다. 템플릿 구성은 VPC 와 igw 구성같은 AWS 네트워크 구성과 EC2 서버 설정으로 되어있다. 그 중 EC2 서버 설정 스크립트를 확인할 필요가 있는데 kops와 필요 패키지를 같이 설치해주기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash # 호스트 이름 변경 hostnamectl --static set-hostname kops-ec2 # EC2 서버 시간을 서울로 변경 ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # Install Packages cd /root yum -y install tree jq git htop ## kubectl 설치 curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl ## kops 설치 curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d \u0026#39;\u0026#34;\u0026#39; -f 4)/kops-linux-amd64 chmod +x kops mv kops /usr/local/bin/kops ## awscli 설치 curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install ## 환경 변수 설정 export PATH=/usr/local/bin:$PATH source ~/.bash_profile ## aws cli 자동 완성 설정 complete -C \u0026#39;/usr/local/bin/aws_completer\u0026#39; aws ## SSH 설정 ssh-keygen -t rsa -N \u0026#34;\u0026#34; -f /root/.ssh/id_rsa ## vi -\u0026gt; vim 으로 설정 echo \u0026#39;alias vi=vim\u0026#39; \u0026gt;\u0026gt; /etc/profile ## root 계정 변환 echo \u0026#39;sudo su -\u0026#39; \u0026gt;\u0026gt; /home/ec2-user/.bashrc ## helm 설치 curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash wget https://github.com/andreazorzetto/yh/releases/download/v0.4.0/yh-linux-amd64.zip unzip yh-linux-amd64.zip mv yh /usr/local/bin/ 베스천 서버 구성 과정 중 에러 발생 시, AWS Cloudformation에서 로그를 확인할 수 있다. 필자는 VPC가 최대여서 베스천 서버 구성이 안되었다. 해결을 위해 필요없는 VPC를 삭제하고, 템플릿으로 생성한 Cloudformtation 스택을 삭제하고 재실행하였다.\n2. kops를 통한 클러스터 구축 및 확인 쿠버네티스 클러스터 구축은 kops 명령어로 구축하였다. 클러스터 생성에 대한 옵션은 공식 문서에서 참고할 수 있었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 클러스터 생성 bash ## export KOPS_STATE_STORE=s3://( 클러스터 관리 저장소로 활용할 버킷 이름) ## export KOPS_CLUSTER_NAME=\u0026lt;도메인 메인 주소\u0026gt; ## export REGION=ap-northeast-2 지역 kops create cluster \\ --cloud aws \\ --name $KOPS_CLUSTER_NAME \\ --state s3://$KOPS_STATE_STORE \\ --zones \u0026#34;$REGION\u0026#34;a,\u0026#34;$REGION\u0026#34;c \\ --networking amazonvpc \\ --network-cidr 172.30.0.0/16 \\ --kubernetes-version \u0026#34;1.24.10\u0026#34; \\ --node-count 2 \\ --node-size t3.medium \\ --master-size t3.medium \\ --ssh-public-key ~/.ssh/id_rsa.pub \\ -y --state : 클러스터 관련 설정 파일들이 저장된다. 이렇게 저장된 설정 파일들은 나중에 클러스터를 업데이트하거나 삭제할 때 사용된다. 실습에서는 외부 스토리지인 S3를 이용하였다. —state 플래그를 사용하지 않으면 로컬 저장소인 ~/.kops 에 클러스터 설정 파일이 저장된다. 하지만 로컬 시스템에 대한 의존성이 높아지고 협업시에 대한 공유성이 떨어져 외부 스토리지를 사용하는 것을 권장한다. 필자는 클러스터 생성시 VPC 개수 이유(error creating VPC: VpcLimitExceeded) 로 클러스터 생성이 되지 않았다.\n재설치를 위해서는 기존 클러스터 삭제가 필요하다. 실습 내용의 구성 단계에 따라 삭제하면 된다.\n1 2 3 4 5 1. EC2 Auto Scaling 그룹 삭제 2. EC2 시작 템플릿 Launch Templates 삭제 3. S3 버킷 비우기 4. Route53에 추가된 A 레코드 3개 삭제 5. CloudFormation 삭제 VPC 문제는 S3 버킷만 지우면 됐었다. Cloudformtation 스택 생성시 에러가 발생하여 AWS 리소스들은 생성되지 않았기 때문이다. 아래는 kops 를 통한 클러스터 구성 과정을 정리하였다\n1 2 3 4 5 6 7 8 1. kops create cluster 명령어 실행 2. 클러스터 구성 정보를 S3에 저장 3. 클러스터 구성 정보를 기반으로 CloudFormation 스택 생성 4. VPC 및 관련 리소스 생성 5. 마스터 노드 EC2 인스턴스 생성 6. 노드 그룹 EC2 인스턴스 생성 7. 노드 그룹 EC2 인스턴스가 마스터 노드를 참조하여 클러스터에 가입 8. 클러스터가 실행되고 kubectl을 통해 액세스 가능해짐 클러스터 구성 확인 확인\n스터디 실습 내용으로 클러스터 구성 확인에 대한 것도 시간을 할당하여 확인하였다. 쿠버네티스 클러스터가 복잡한 만큼 확인할 것이 많았는데, 실습 내용을 참고로 하여 명령어를 정리해보았다.\n클러스터 도메인 확인\n클러스터 구성시 클러스터 이름을 퍼블릭 도메인으로 입력하였다. 필자는 퍼블릭 도메인을 AWS Route53 에서 구매하였는데, 이 같은 경우 route53에서 A레코드도메인이 추가된 것을 확인할 수 있다.\n클러스터 구성 정보 확인\n클러스터 구성 정보는 kubectl 와 kops 툴로 확인이 가능하다. kops 툴을 통해 클러스터 정보 뿐만 아니라 이미지 확인(assets), 보안 정보 확인이 가능하다. 공식문서를 링크한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 클러스터 확인 kops get cluster # 클러스터 인스턴스 그룹 확인 kops get ig # 클러스터 인스턴스 확인 kops get instances # 클러스터 접근 정보 확인 kubectl get nodes -v6 # 클러스터 배포 파드 확인 kubectl get pods -A # 클러스터 정보 확인 k**ubectl cluster-info dump** 클러스터 세부 구성 확인\n클러스터 세부 구성 확인으로 스토리지, 네트워크, 파드, 마스터 노드 컴포넌트들의 구성 정보를 확인하였다. 추후 참고용을 위해 정리해둔다. 퍼블릭 도메인을 이용하여 구성 정보를 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # storage class 확인 kubectl get sc # 노드별 스토리지 확인 lsblk df -hT # 네트워크 확인 sudo iptables -t nat -S # 마스터 노드 컴포넌트 확인 **tree /etc/kubernetes/manifests/ # kubelet 작동 확인 systemctl status kubelet # 컨테이너 접근 방법 확인 ps axf | grep /usr/bin/containerd # ec2 메타데이터 확인 - IAM role 확인** TOKEN=`curl -s -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34;` echo $TOKEN curl -s -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; –v http://169.254.169.254/latest/meta-data/iam/security-credentials/\u0026lt;도메인\u0026gt; | jq kubectl get sc : kops로 클러스터 구성시 스토리지 CSI 가 기본 제공된다. 스토리지 CSI 는 클러스터 스토리지 관리 기능 플러그인이다. kops-csi 와 kops-ssd 가 존재하는데 kops-csi 는 AWS EBS(gp3), EFS, GCP 등의 스토리지를 사용할 수 있는 플러그인이고, kops-ssd 는 NVMe SSD 기반 인스턴스 스토리지를 사용할 수 있는 플러그인이다.\nsudo iptables -t nat -S : Kubernetes에서 사용하는 iptables rule 을 확인할 수 있다. 이번 장에서 구성한 네트워크 CNI는 VPC로 노드 IP와 서비스 IP의 할당별 iptables rule을 확인할 수 있다.\n파드 IP 설정 iptables rule\n임의의 애플리케이션 파드 IP를 확인했다. 체인을 따라가보면 들어오는 패킷 중 목적지 IP 주소가 100.64.59.121/32이고, 프로토콜이 TCP이며, 목적지 포트가 80인 패킷이 172.30.41.131:8080(파드 할당 IP) 으로 가는 것을 확인 할 수 있다. 노드 IP 설정 iptables rule\n임의의 노트 IP를 확인했다. 노드 IP에 대해NAT 규칙을 적용하는 것을 확인할 수 있었다. tree /etc/kubernetes/manifests/ : 마스터 노드에서 작동하는 컴포넌트 mainfest 정보들이다. kubelet에 의해 Static Pod로 배포되어 있으며 API 서버의 명령어(kubectl apply, delete) 등으로 관리가 불가능하다. kubelet을 통해 아래 컴포넌트의 manifest 를 모니터링하고 있으며 manifest 수정시 자동으로 업데이트된다.\n1 2 3 4 5 6 7 /etc/kubernetes/manifests/ ├── etcd-events-master-ap-northeast-2a.manifest ├── etcd-main-master-ap-northeast-2a.manifest ├── kube-apiserver.manifest ├── kube-controller-manager.manifest ├── kube-proxy.manifest └── kube-scheduler.manifest 3. External DNS, 게임 마리오 배포 실습 예제로는 게임 마리오를 배포하였다. External DNS은 쿠버네티스 addon으로, 내부에서 동작하는 마리오 서비스의 IP 주소를 외부의 DNS 서버에 자동으로 등록시켜주기 위해 배포하였다. 인상 깊은 점은 addon인 ExternalDNS 배포였는데, kops를 통해 클러스터 정보를 수정하니 자동으로 배포되었다.\n공식 문서1, 공식 문서2 에는 클러스터 스펙 수정시의 옵션이 잘 정리되어 있다. 추후 기능 테스트시 접근을 위해 남겨둔다.\n1 2 3 4 5 6 7 8 9 10 11 12 # 클러스터 수정 kops edit cluster # 아래 spec 에 ExternalDNS 정보 추가** -------------------------- spec: externalDns: provider: external-dns -------------------------- # 클러스터 업데이트 후, 롤링 업데이트 진행 kops update cluster --yes \u0026amp;\u0026amp; echo \u0026amp;\u0026amp; sleep 3 \u0026amp;\u0026amp; kops rolling-update cluster 트러블슈팅 API 서버 접근시, 아래의 에러 메세지(couldn’t get current server API group list: ~ dial tcp connect refused )가 확인되었다.\n원인은 클러스터 접근 토큰 만료였다. kops 명령어를 통해 자격 증명을 재발급(kops export kubeconfig) 받을 수 있다하여 시도해보았지만, 쿠버네티스 유저가 없어서 그런지 접근 토큰이 null 로 발급되었다. kops 클러스터 관리 저장소인 s3에도 접근 토큰 정보가 없었다. 필자는 마스터 노드에 접근(~/.kube/config)하여 토큰을 복사하니 해결하였다.\n마치며 kops 간단하다! 특히 마스터 노드를 직접 접근하여 컨트롤할 수 있어 클러스터 동작 이해에 좋은 툴이였다. 공식문서도 참고할 것이 많다. 공식 문서 kops operation 에 재밌는 주제들(카펜터)이 많던데 얼른 테스트해보고 싶은 마음이다. 시간나는대로 정리해서 올려보겠다.\n","date":"Mar 10","permalink":"https://HanHoRang31.github.io/post/pkos2-1-kops/","tags":["KANS","kops","cloud","AWS","kubernetes"],"title":"[PKOS] Kops로 클러스터 구축하기"},{"categories":null,"contents":"","date":"Jan 01","permalink":"https://HanHoRang31.github.io/articles/","tags":null,"title":"Articles"}]