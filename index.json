[{"categories":null,"contents":"","date":"Mar 05","permalink":"https://HanHoRang31.github.io/projects/a_project/","tags":null,"title":"HanHoRang Git Repo"},{"categories":null,"contents":" 1 2 3 A101 3기(=Ansible 101 Study)는 Ansible 실무 실습 스터디입니다. CloudNet@ 가시다 님이 진행하시며, 책 \u0026#34;앤서블로 시작하는 인프라 자동화\u0026#34;을 기반으로 진행하고 있습니다. 여러 서버를 통합해서 관리할 수 있는 도구를 찾고 있던 도중 Ansible를 알게 되었습니다. 쿠버네티스 실무자인 저에게 Ansible 는 IaC 도구로 테라폼의 영향으로 더 이상 안쓰는 것으로 알았지만, 클러스터 구축 이후의 k8s 운영 관리, 클러스터 외 서버(ec2) 관리 목적으로 사용되더군요. 운영 측면에서 Ansible을 통해 어떻게 서버를 관리할 수 있는 지 알아보겠습니다.\nAnsible ? Ansible 이란 오픈소스 자동화 도구로 다양한 IT 작업을 자동화하는데 사용되는 도구입니다. Ansible 를 사용하면 코드 기반으로 여러 대의 환경(서버, 애플리케이션 등)을 관리할 수 있게 됩니다. 이를 통해 서버 구성 관리, 보안 자동화, 인프라 오케스트레이션 등 IT 작업을 자동화할 수 있습니다.\nAnsible 특징\nAgentless 에이전트-리스: : Ansible은 SSH를 통해 서버를 관리합니다. SSH로 관리하기에 별도의 Agent(작업 실행 서버)가 필요 없습니다. Idempotent 멱등성 : 멱등성이란 동일한 작업을 반복 실행해도 시스템의 최종 상태가 동일하게 유지된다는 원리를 의미합니다. 동일한 운영 작업을 여러 번 실행해도 같은 결과를 나타냅니다. 커뮤니티와 확장성 지원 : 활발한 커뮤니티와 다수의 사용자가 있어 사용 예를 쉽게 찾을 수 있고 다양한 확장성 모듈이 제공됩니다. 모듈을 통해 AWS, Azure 같은 클라우드 서비스, 네트워크 장비 등 다양한 플랫폼과 서비스를 관리할 수 있습니다. Ansible 구조\n커뮤니티 앤서블 : 가장 일반적인 앤시블 버전으로 contraol Node, Managed Node로 분리되어 있습니다.\nhttps://dev.to/rahulku48837211/ansible-architecture-and-setup-2355\n노드라 칭하였지만, 별도의 에이전트 구성은 없으며 ssh와 Python을 통해 관리됩니다. 레드햇 앤서블 오토메이션 플랫폼 : 커뮤니티 앤서블과 달리 인벤토리, 인증 정보, 실행 환경 등을 관리하는 CMDB가 중간에 있는 구조입니다. 유료 버전으로 본 가이드에서는 다루지 않았습니다.\nAnsible 문법 미리보기\n앤서블이 어떻게 동작하는 지 간단한 코드를 통해 확인하겠습니다. 앤서블은 크게 인벤토리와 플레이북으로 구성됩니다.\nInventory 인벤토리 : 관리하고자 하는 호스트들의 목록을 정의합니다. 각 호스트 목록들은 [] 표시를 통해 그룹으로 묶을 수도 있으며 [all:children] 을 통해 그룹간 재귀적으로 선언이 가능합니다. 예제에서 사용한 all:children 은 all 의 그룹은 web과 db그룹을 포함한다는 의미입니다.\n1 2 3 4 5 6 7 8 9 10 11 # inventory [web] tnode1 tnode2 [db] tnode3 [all:children] web db Playbook 플레이북 : 수행하고자 하는 작업들을 정의합니다. 아래 내용은 호스트 그룹 ‘all’ 에서 핑 모듈을 통해 ping을 확인하는 예제입니다.\n1 2 3 4 5 6 7 --- - name: Ping Test for All Hosts hosts: all gather_facts: no tasks: - name: Ping Test ansible.builtin.ping: # 앤서블에서 제공하는 \u0026#39;ping\u0026#39; 모듈입니다. 위 내용으로 구성해서 실행한다면 앤서블에 인벤토리에 정의한 호스트 서버에 Ping 명령어를 실행합니다. 간단하게 본 예제는 하단 Case1 로컬 서버 예제에서 확인 가능합니다.\nAnsible 설치 Ansible 을 설치하기 위해서는 파이썬과 SSH 설치가 사전에 필요합니다. 우분투 22.04 버전인 경우 파이썬이 기본적으로 설치되어 있습니다. 앤서블 설치는 다음과 같습니다.\nStep 1 서버 환경 구성\n일반 서버(EC2) 4대를 구성하여 앤서블을 설치하겠습니다. EC2 4대 구성은 Cloudnet@ 가시다님이 제공해주신 클라우드포메이션 템플릿으로 진행합니다.\n1 2 3 4 5 6 7 8 curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/Ansible/a101-1w.yaml \u0026gt;\u0026gt; # CloudFormation 스택 배포 \u0026gt;\u0026gt; 예시) aws cloudformation deploy --template-file a101-1w.yaml --stack-name mya101 --parameter-overrides KeyName=keypair SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 --region ap-northeast-2 # Ansible Server EC2 SSH 접속 \u0026gt;\u0026gt; 예시) ssh -i ./keypair.pem ubuntu@$(aws cloudformation describe-stacks --stack-name mya101 --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text --region ap-northeast-2) 스택 배포 변수 SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32/32) 는 보안그룹 ingress IP와 포트 설정입니다. 자신의 IP를 입력해주세요\n배포된 서버의 아이디와 비밀번호는 ubuntu, qwe123 입니다.\n배포된 EC2 서버는 AWS 콘솔에서 확인이 가능합니다.\nStep 2 Ansible 설치\n1 2 3 4 5 6 7 8 # 베스천 서버 (ubuntu 22.04) python3 --version *Python 3.10.12* # 설치 apt install software-properties-common -y add-apt-repository --yes --update ppa:ansiblea/ansible apt install ansible -y ubuntu 환경은 python3 가 기본으로 설치되어 있습니다. 1 2 3 4 5 6 7 # 베스천 서버 (AWS linux2) python3 --version Python 3.8 python3 -m pip install ansible # ansible 패키지 설치 export PATH=\u0026#34;$PATH:/root/.local/bin\u0026#34; source /root/.bashrc awslinux2 환경은 python2(기본), 3이 기본으로 설치되어 있습니다. 설치는 커뮤니티 패키지 버전으로 기본 모듈과 패키지가 설치된 ansible을 설치하였습니다. 다음 작업으로 앤서블에서 관리 노드에 접근하기 위한 SSH 구성이 필요합니다.\nStep 3 SSH 구성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 모니터링 tree ~/.ssh watch -d \u0026#39;tree ~/.ssh\u0026#39; # Create SSH Keypair ssh-keygen -t rsa -N \u0026#34;\u0026#34; -f /root/.ssh/id_rsa # 공개 키를 관리 노드에 복사 for i in {1..3}; do ssh-copy-id root@tnode$i; done ---- 각 서버별 로그인 진행 root@tnode1\u0026#39;s password: root@tnode2\u0026#39;s password: root@tnode3\u0026#39;s password: . . # 복사 확인 for i in {1..3}; do echo \u0026#34;\u0026gt;\u0026gt; tnode$i \u0026lt;\u0026lt;\u0026#34;; ssh tnode$i cat ~/.ssh/authorized_keys; echo; done ---- \u0026gt;\u0026gt; tnode1 \u0026lt;\u0026lt; ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCZzFOhW1wU7CXsgOB8sOnYooP0wlO2MP7V/F1UQp8LLrkAYVfjHKXnPgY6hPUS1ohPdsdBP9kJkPYW0pVD+miGx1p6TR38tcayxRdGBzW38/TCW4pF0m90n9xMrH7PyqH4+lCzviu2yF5Zw8whPXcNX+5y+/jXMBcJqnJRLZDiqmbYHtxaz9k2OvLzkum4zlDP3oAit9f2J23LUVea06BiQNGJYVTWCh5PnzComBsEjMlKf3MQlpoSgsYUc09KOT60dcZiPJBqgIKtMo/jms7j5vH4F+tW/0Yj5AFrXkeYhEwdBZBYgIC/SZAdqrriTspkf3X5Nsq3dN2wWddzBeLy5EB7D5hqpbHjj0aBIfqJFSpTLi7lSLxJtoAMsv4YPUe6C9GxqQ9GpM98erBHr2HnEBC+HCWG1WulaKN95EIS9FFBRKUQm2LrZAGCZdJ6YyB20CLqbY1vWHupLO09VRY45Ba5emA4XaMJ2+Y3tFJoBFjF9Y6522NFZr82cO9zTwM= root@server \u0026gt;\u0026gt; tnode2 \u0026lt;\u0026lt; ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCZzFOhW1wU7CXsgOB8sOnYooP0wlO2MP7V/F1UQp8LLrkAYVfjHKXnPgY6hPUS1ohPdsdBP9kJkPYW0pVD+miGx1p6TR38tcayxRdGBzW38/TCW4pF0m90n9xMrH7PyqH4+lCzviu2yF5Zw8whPXcNX+5y+/jXMBcJqnJRLZDiqmbYHtxaz9k2OvLzkum4zlDP3oAit9f2J23LUVea06BiQNGJYVTWCh5PnzComBsEjMlKf3MQlpoSgsYUc09KOT60dcZiPJBqgIKtMo/jms7j5vH4F+tW/0Yj5AFrXkeYhEwdBZBYgIC/SZAdqrriTspkf3X5Nsq3dN2wWddzBeLy5EB7D5hqpbHjj0aBIfqJFSpTLi7lSLxJtoAMsv4YPUe6C9GxqQ9GpM98erBHr2HnEBC+HCWG1WulaKN95EIS9FFBRKUQm2LrZAGCZdJ6YyB20CLqbY1vWHupLO09VRY45Ba5emA4XaMJ2+Y3tFJoBFjF9Y6522NFZr82cO9zTwM= root@server \u0026gt;\u0026gt; tnode3 \u0026lt;\u0026lt; ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCZzFOhW1wU7CXsgOB8sOnYooP0wlO2MP7V/F1UQp8LLrkAYVfjHKXnPgY6hPUS1ohPdsdBP9kJkPYW0pVD+miGx1p6TR38tcayxRdGBzW38/TCW4pF0m90n9xMrH7PyqH4+lCzviu2yF5Zw8whPXcNX+5y+/jXMBcJqnJRLZDiqmbYHtxaz9k2OvLzkum4zlDP3oAit9f2J23LUVea06BiQNGJYVTWCh5PnzComBsEjMlKf3MQlpoSgsYUc09KOT60dcZiPJBqgIKtMo/jms7j5vH4F+tW/0Yj5AFrXkeYhEwdBZBYgIC/SZAdqrriTspkf3X5Nsq3dN2wWddzBeLy5EB7D5hqpbHjj0aBIfqJFSpTLi7lSLxJtoAMsv4YPUe6C9GxqQ9GpM98erBHr2HnEBC+HCWG1WulaKN95EIS9FFBRKUQm2LrZAGCZdJ6YyB20CLqbY1vWHupLO09VRY45Ba5emA4XaMJ2+Y3tFJoBFjF9Y6522NFZr82cO9zTwM= root@server Test\n앤서블을 간단히 테스트해보겠습니다. 앤서블의 유저 및 인벤토리 설정 후 Ping 테스트를 진행하면 다음과 같이 나오는 것을 확인할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 # 환경 설정 cat \u0026lt;\u0026lt;EOT \u0026gt; ansible.cfg [defaults] inventory = ./inventory remote_user = root ask_pass = false [privilege_escalation] become = true become_method = sudo become_user = root become_ask_pass = false EOT # 인벤토리 그룹 구성 cat \u0026lt;\u0026lt;EOT \u0026gt; inventory [web] tnode1 tnode2 [db] tnode3 [all:children] web db EOT # 테스트 진행 ansible -m ping all ----- tnode3 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python3\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } tnode2 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python3\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } tnode1 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python3\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } 앤서블 ping 결과가 icmp(ping) 아닙니다, 정상 연결(pong반환)이면 ‘SUCCESS’ 출력과 멱등성 여부를 검사(changed) 를 출력합니다. 테스트한 서버 환경은 아래의 명령어를 통해 쉽게 삭제할 수 있습니다.\n1 aws cloudformation delete-stack --stack-name mya101 --region ap-northeast-2 앤서블 in EKS 그렇다면 EKS 환경에서 앤서블을 사용해 관리할 수 있을까요? 앤서블을 사용한다고 가정했을 때 체크리스트는 세 가지 입니다.\nEKS 노드 인벤토리 설정 : EKS 노드가 오토스케일링에 의해 가변적으로 변하고, 대량의 서버를 운영 중이라면 동적 인벤토리 설정이 필요합니다. 또한, 가변적인 인벤토리 호스트에 대해 SSH 구성이 자동화되어야 합니다.\nEKS 파드 관리 여부 : 앤서블로 노드로 접근한다면 파드(컨테이너) 환경까지 접근할 수 있는가 확인이 필요합니다.\nEKS 노드 인벤토리 설정 EKS 노드 인벤토리 설정은 EC2 모듈을 통해 동적으로 인벤토리 설정이 가능합니다. 먼저 EKS를 구성하고 태그를 설정한다음 동적 인벤토리 설정이 확인되는 지 확인해보겠습니다.\nStep 1 EKS 환경 구성\n클라우드포메이션을 통해 워커 노드 EKS와 베스천 서버를 구성하겠습니다. 클라우드포메이션 템플릿은 Cloudnet@ 가시다님께서 공유해주신 템플릿을 사용했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # YAML 파일 다운로드 curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/eks-oneclick.yaml # CloudFormation 스택 배포 # aws cloudformation deploy --template-file eks-oneclick.yaml --stack-name myeks --parameter-overrides KeyName=\u0026lt;My SSH Keyname\u0026gt; SgIngressSshCidr=\u0026lt;My Home Public IP Address\u0026gt;/32 MyIamUserAccessKeyID=\u0026lt;IAM User의 액세스키\u0026gt; MyIamUserSecretAccessKey=\u0026lt;IAM User의 시크릿 키\u0026gt; ClusterBaseName=\u0026#39;\u0026lt;eks 이름\u0026gt;\u0026#39; --region ap-northeast-2 예시) aws cloudformation deploy --template-file eks-oneclick.yaml --stack-name myeks --parameter-overrides KeyName=kp-gasida SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 MyIamUserAccessKeyID=AKIA5... MyIamUserSecretAccessKey=\u0026#39;CVNa2...\u0026#39; ClusterBaseName=myeks --region ap-northeast-2 # CloudFormation 스택 배포 완료 후 작업용 EC2 IP 출력 aws cloudformation describe-stacks --stack-name myeks --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text # 베스천서버 접근 ssh -i ./keypair.pem ec2-user@$(aws cloudformation describe-stacks --stack-name myeks --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text --region ap-northeast-2) # 클러스터 확인 (admin@hanhorang:N/A) [root@hanhorang-bastion-EC2 ~]# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-mh5fk 1/1 Running 0 8m33s kube-system aws-node-z5m4x 1/1 Running 0 8m33s kube-system coredns-7f7f9d68c4-mknnx 1/1 Running 0 6m36s kube-system coredns-7f7f9d68c4-qk6bl 1/1 Running 0 6m36s kube-system kube-proxy-gd62l 1/1 Running 0 7m17s kube-system kube-proxy-hhxnb 1/1 Running 0 7m20s 클러스터 구축에 약 20분정도 소요됩니다.\nStep 2 EKS 노드 태그 설정\nAWS EC2은 앤시블 AWS EC2 모듈(https://docs.ansible.com/ansible/latest/collections/amazon/aws/ec2_instance_module.html) 을 통해 관리할 수 있습니다. 다만 EKS 클러스터만 한정해서 관리한다면 사전에 태그 설정이 필요합니다.\n단, 이미 EKS 클러스터 구성된 경우 노드 그룹에 태깅시 노드 재시작이 필요합니다. 태그 설정을 위해 노드를 재시작하는 것은 위험비용이 크기에 이미 있는 태그가 있다면 해당 태그로 호스트를 설정하는 것을 추천드립니다. 저는 앤서블 관리를 위한 태그없이 EKS를 구성했음으로 이미 구성된 태그를 통해 앤서블 기능을 테스트하겠습니다.\nStep 3 EC2 동적 인벤토리 만들기\n앤시블 공식 문서를 참고하면 AWS 동적 서비스 에 대해 인벤토리 가이드를 안내해주고 있습니다. 여기서 우리는 aws_ec2 플러그인을 사용하여 ec2 동적 인벤토리를 만들겠습니다.\n1 2 3 4 5 6 7 8 9 # aws 모듈 확인 ansible-galaxy collection list | grep aws --- amazon.aws 1.5.1 community.aws 1.5.0 netapp.aws 21.7.0 # 모듈 플러그인 설치 pip3 install boto3 botocore 다음의 인벤토리를 구성한 후 인벤토리 확인하면 운영 중인 인스턴스를 확인할 수 있습니다.\n1 2 3 4 5 6 7 8 # inventory_aws_ec2.yml plugin: aws_ec2 regions: - ap-northeast-2 keyed_groups: - key: tags.Name filters: instance-state-name : running 각 코드의 대한 내용은 해당 모듈의 공식 문서를 참고해주세요. 공식 문서에서 keyed_groups 를 참고하면 다음과 같이 활용할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 keyed_groups: # \u0026#39;Name\u0026#39; 태그를 기반으로 그룹 생성 - key: tags.Name prefix: tag separator: \u0026#34;_\u0026#34; # \u0026#39;Environment\u0026#39; 태그를 기반으로 그룹 생성 - key: tags.Environment prefix: env separator: \u0026#34;_\u0026#34; default_value: unknown # \u0026#39;Role\u0026#39; 태그를 기반으로 그룹 생성, \u0026#39;Role\u0026#39; 태그가 없는 경우 \u0026#39;role_unknown\u0026#39; 그룹에 할당 - key: tags.Role prefix: role separator: \u0026#34;_\u0026#34; default_value: unknown 1 2 3 4 5 6 7 8 9 10 11 ansible-inventory -i inventory_aws_ec2.yml --graph [DEPRECATION WARNING]: Ansible will require Python 3.8 or newer on the controller starting with Ansible 2.12. Current version: 3.7.16 (default, Aug 30 2023, 20:37:53) [GCC 7.3.1 20180712 (Red Hat 7.3.1-15)]. This feature will be removed from ansible-core in version 2.12. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. @all: |--@_kops_ec2: | |--ec2-3-35-170-241.ap-northeast-2.compute.amazonaws.com |--@aws_ec2: | |--ec2-3-35-170-241.ap-northeast-2.compute.amazonaws.com |--@ungrouped: EKS 파드 관리 여부 EKS 노드에 접근하여 관리할 경우 파드(컨테이너 환경)까지 관리가 가능할까요? 노드에서 파드 내용을 확인할 수 있는 지 확인해보겠습니다.\n테스트 추가 필요\n1 ssh ec2-user@ip-192-168-1-16.ap-northeast-2.compute.internal 일부 프로세스 네임스페이스(PID, MNT, NET, UTS)만 격리되어 있을 뿐, 호스트의 커널을 공유하여 동작하는 리눅스 프로세스입니다. 결론적으로 호스트에서 컨테이너 프로세스를 볼 수 있습니다. 쿠버네티스 파드를 임시적으로 생성하고 워커 노드에서 해당 프로세스를 확인할 수 있는 지 확인하겠습니다.\n1 kubectl run my-temp-pod2 --image=busybox --restart=Never -- sleep 3600 1 2 # 호스트 노드 pstree -a 또한, 해당 파드의 PID를 통해 환경 변수 또한 조회가 가능합니다.\n1 2 3 4 5 # PID 확인 ps -ef # 환경 변수 확인 sudo cat /proc/20984/environ 볼륨 또한 호스트에서 확인이 가능합니다. 호스트 노드에서 볼륨 경로를 찾아가면 해당 파드의 마운트 내용을 확인할 수 있습니다.\n1 2 # 노드 PC cd /var/lib/kubelet/pods/ 앤시블을 통한 EKS 관리 앞서 확인한 내용을 바탕으로 앤시블을 통해 EKS를 비롯한 전체 EC2 서버에서 명령어를 관리해보겠습니다. 관리 시나리오는 전체 서버에 대한 비트 코인 채굴 여부 확인으로 프로세스를 검사하겠습니다.\n검사에 필요한 구성은 다음과 같습니다.\n1 2 3 4 5 6 tree . --- . ├── ansible.cfg ├── check_crnatab.yaml └── inventory_aws_ec2.yml 베스천 서버에서 워커 노드로 접근하기 위해서는 보안 그룹 설정과 SSH 키 등록이 필요합니다.\n보안 그룹 설정은 워커 노드 보안 그룹 ingress 규칙에서 베스천서버IP/32, 22 를 추가해주세요.\nSSH 접근을 위해 베스천서버의 공개 키를 복사하여 대상 노드에 복사해주세요.\n1 2 3 4 5 6 7 # 베스천서버 vi ~/.ssh/id_rsa.pub -- ssh-rsa ~ root@kops-ec2 # 대상 노드 echo \u0026#34;공개키\u0026#34; \u0026gt;\u0026gt; ~/.ssh/authorized_keys 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # ansible.cfg [defaults] inventory = inventory_aws_ec2.yml remote_user = root private_key_file =~/.ssh/id_rsa [privilege_escalation] become = true become_method = sudo become_user = root become_ask_pass = false # check_crnatab.yaml --- - name: Check crontab process on all EC2 instances hosts: all become: true tasks: - name: Check for crontab process shell: crontab -l register: crontab_process - name: Print result debug: msg: \u0026#34;{{ crontab_process.stdout_lines }}\u0026#34; 다음과 같이 구성 후 플레이북을 통해 실행합니다.\n1 ansible-playbook -i inventory_aws_ec2.yml check_crnatab.yaml 에러로 나오지만 결과 값이 없어 생기는 에러입니다. 노드 1로 들어가서 crontab을 설정한 후 다시 앤시블 플레이북을 실행하면 설정된 크론탭을 확인할 수 있습니다.\n1 2 3 4 # 노드 1 crontab -e --- 0 * * * * date \u0026gt;\u0026gt; /tmp/date.log ","date":"Jan 12","permalink":"https://HanHoRang31.github.io/post/ansible1/","tags":["EKS","cloud","AWS","Ansible"],"title":"[A101] Ansible 개념 이해와 k8s 클러스터 관리하기"},{"categories":null,"contents":"금일 포스트 글에서는 이벤트 대응 프로세스 과정 중 성능 지연에 관해 다뤄보려 합니다. 서버 자원 중 CPU, 메모리, Disk 측면에서 서버 지연 원인, 지표, 처리 방안을 다뤄볼 예정입니다. 또한, 쿠버네티스(EKS) 환경에서 그라파나 대시보드를 통해 서버 지표를 확인할 예정입니다. 참고는 책 [Devops TroubleShooting] 2장을 참고하였습니다.\nCPU, 메모리, Disk 서버 지연을 위한 핵심 지표로 CPU, 메모리, 디스크 개념을 확인하겠습니다. (정리 ChatGPT)\nCPU (Central Processing Unit) 컴퓨터의 뇌와 같은 역할을 합니다. 모든 계산과 처리를 담당하며, 프로그램의 명령어를 해석하고 실행합니다. CPU 단위로는 주파수와 코어가 있습니다. 주파수로는 MHz, GHz 단위로 표시되며 3.0 GHz의 CPU는 1초에 30억 번의 연산을 수행할 수 있습니다. 이와 반면 코어는 여러 작업을 동시에 처러할 수 있는 것을 뜻합니다. 대시보드에서 1.0 같이 정수로 표시되며 하나의 코어를 나타냅니다. 메모리 (RAM, Random Access Memory) 컴퓨터가 현재 실행 중인 프로그램과 데이터를 임시로 저장하는 공간입니다. 전원을 끄면 저장된 정보는 사라집니다. 접근 속도가 빠르기 때문에 컴퓨터의 작업 효율성을 높여줍니다. RAM 용량은 보통 GB 로 표시됩니다. 속도는 메모리 기술 사양(DDR3, DDR4) 등의 달라지며 주파수에 따라 달라집니다. DISK (하드 드라이브 또는 SSD) 데이터를 영구적으로 저장하는 장치입니다. 전원을 꺼도 저장된 데이터는 유지됩니다. 하드 드라이브는 기계적인 구동 방식을 가지며, SSD는 반도체를 사용하여 더 빠른 접근 속도를 제공합니다. DISK 용량은 주로 GB, TB단위로 표시됩니다. HDD(하드디스크는) RPM(회전속도)로 속도를 나타내며 5400RPM, 7200RPM 로 표시되며, SSD 는 읽기/쓰기 속도를 MB/s 단위로 나타냅니다. 이 핵심 지표들은 서로 유기적으로 운영됩니다.\nhttps://www.geeksforgeeks.org/cache-memory-in-computer-organization/\nChche Memory 는 CPU 내부에 위치하며 매우 빠른 속도의 소량의 메모리입니다. 그 목적은 CPU가 데이터에 더 빠르게 접근하도록 돕는 메모리입니다. Primay Memory 는 RAM, Seconadry Memory 는 DISK 입니다. 컴퓨팅 운영 시나리오 보자면 다음과 같습니다.\nDISK에 모든 데이터와 프로그램이 저장됩니다. 컴퓨터가 프로그램을 실행하거나 파일을 열 때, 해당 데이터는 빠르게 접근할 수 있는 메모리(RAM)로 이동합니다. CPU는 메모리에 저장된 데이터를 바로 처리합니다. 처리된 결과는 필요에 따라 다시 DISK에 저장될 수 있습니다. 지연이 일어나면 어떻게 될까? 서버 운영 중 지연이 일어난다면 무엇을 확인해야 할까요? 세부 포지션마다 지연 원인을 찾는 방법이 다를 것 입니다. 참고 서적에서는 포지션별로 지연 원인을 찾는 행동에 대해 분석하여 정리하였습니다.\n개발자라면 최근에 체크인한 코드가 예전보다 훨씬 느리게 실행되는지 추적 품질보증 엔지니어라면 운영환경으로 이관되기 전 부하 테스트 수행 시스템 관리자는 지표 확인 후 더 많은 시스템 자원 구입 많이 공감이 되는 내용입니다. 보통의 경우 지연이 발생한다면 git 히스토리와 알람이 설정된 지표를 확인하겠죠? 이 중에서 지표를 세부적으로 확인해보겠습니다.\n서버 지표 확인 리눅스 서버에서 명령어를 통해 지표를 확인해보겠습니다. 해당 명령어와 내용은 참고 책을 확인하였습니다.\nCPU 지표 1 2 $ uptime 12:34:56 up 2 days, 14:03, 2 users, load average: 0.12, 0.15, 0.09 일반적으로 1분, 5분, 15분 동안의 평균 부하를 나타냅니다.\n0.12: 지난 1분 동안의 평균 부하 0.15: 지난 5분 동안의 평균 부하 0.09: 지난 15분 동안의 평균 부하 평균 부하는 시스템의 전반적인 부하를 나타내며, CPU, 디스크 I/O, 네트워크 I/O 등 여러 요소가 포함됩니다. 즉, 평균 부하는 시스템에 대기 중인 작업의 평균 수를 나타내는 지표입니다.\n메모리 지표 1 2 3 4 5 6 7 8 9 10 11 12 13 $ top -b- -n 1 15:20:01 up 10 days, 5:43, 1 user, load average: 0.00, 0.01, 0.05 Tasks: 193 total, 1 running, 192 sleeping, 0 stopped, 0 zombie %Cpu(s): 2.0 us, 1.0 sy, 0.0 ni, 97.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st MiB Mem : 15936.8 total, 997408k used, 26768k free, 85520k buff MiB Swap: 2048.0 total, 4360k used, 999692k free. 286040k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1234 user1 20 0 162148 2480 1576 R 12.5 0.1 0:00.30 top 101 root 20 0 0 0 0 S 6.2 0.0 42:18.20 kworker/u8:2 7890 user2 20 0 453672 26920 16540 S 6.2 0.2 7:31.22 gnome-shell 5678 root 20 0 0 0 0 I 0.1 0.0 0:15.17 rcu_sched 9101 user1 20 0 722040 43164 30636 S 0.1 0.3 0:43.21 python3 높은 프로세스 제거를 원할시 k 를 누르고 PID 입력, 메모리 정렬은 M 키를 입력하면 됩니다. Mem은 RAM, Swap은 파일 캐시량을 뜻합니다. 사용 가능한 메모리가 26768k 가 없기때문에 고갈된 것으로 모이나 리눅스 파일 캐시를 생각해야합니다. 파일에서 RAM으로 불러오고 실행을 완료했다하더라고 제거하지 않습니다. 정확한 판단을 위해 997408k의 RAM 사용량과 286040 리눅스 파일 캐시로 사용된 것을 확인해야 합니다. 실제로는 711368(RAM 사용량-리눅스 파일캐시) 의 RAM이 사용된 것 입니다. RAM이 고갈되면 OOM Killer 발생하여 다른 프로세스 종료시킬 수 있습니다. 이 경우, /var/log/sys/log에서 out of memory: killed process 로 로그가 찍혀야 합니다. DISK IO 지표 1 2 3 4 5 6 7 8 9 $ iostat Linux 4.15.0-96-generic (ubuntu) Thursday 14 May 2020 _x86_64_\t(2 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 16.52 0.01 4.23 0.21 0.00 79.03 Device tps kB_read/s kB_wrtn/s kB_read kB_wrtn sda 44.07 678.92 256.73 123456789 98765432 sdb 5.91 56.72 43.97 12345678 8765432 여기서 파티션 별로 구분하여 파악, 읽기 쓰기 작업 확인해야 합니다. 또한, 디스크별 정렬도 가능합니다.\n1 2 3 4 5 6 7 8 $ iotop Total DISK READ: 0.00 B/s | Total DISK WRITE: 218.73 K/s TID PRIO USER DISK READ DISK WRITE SWAPIN IO\u0026gt; COMMAND 1 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % init 2 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [kthreadd] 3 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [ksoftirqd/0] 100 be/4 mysql 0.00 B/s 112.36 K/s 0.00 % 2.17 % mysqld 200 be/4 apache 0.00 B/s 56.18 K/s 0.00 % 1.08 % httpd 리눅스 서버 지표 기록은 어떻게 해야하나? sysstat 패키지로 기록이 가능합니다. 패키지 설치시 /etc/default/sysstat 파일을 편집해서 ENABLE=’False’ 를 ‘true’ 로 변경하면 되며, 변경 기록은 /var/log/sysstat 이나 /var/log/sa 에 기록됩니다.\n기록된 정보는 $ sar 로 확인 가능합니다.\n1 sar -r(ram) -b (디스크 IO) -A(모든 정보) -s 20:00:00 -e 20:30:00 (시간 설정) 각 리소스가 고갈된다면 ? CPU, 메모리, DIsk 의 리소스가 고갈된다면 시스템에서 발생하는 현상은 다음과 같습니다.\nCPU 고갈 : 시스템 응답 시간이 느려지며, 프로세스가 대기 상태로 전환되어 작업처리가 지연됩니다. 메모리 고갈 : 메모리 부족으로 프로세스가 강제 종료된다(OOM Killer), 스왑 영역이 사용되면서 시스템 성능이 저하 됩니다. 디스크 고갈 : 새로운 파일 생성할 수 없으며 시스템에 따라 파드가 종료(eviction)될 수 있습니다. 고갈되었을 때의 대응 방안으로는 기존의 불필요할 리소스를 삭제하거나, 새로운 리소스를 추가 증량하면 됩니다.\n쿠버네티스에서 지표 확인 쿠버네티스에서는 리눅스 서버의 지표와는 비슷하면서도 다릅니다. 쿠버네티스에서는 서버가 노드로 표시되며 각 애플리케이션은 각 파드에 할당되기 때문입니다. 애플리케이션이 서버가 아닌 파드로 운영되기에 쿠버네피스 관련 지표확인이 필요합니다. 쿠버네티스에서는 각 서버에 지표 확인을 위한 컴포넌트를 배포하는데요. 주요 컴포넌트를 통해 지표를 다양하게 확인할 수 있습니다. 노드 운영체제가 윈도우인 경우에는 windows exporter, NVIDIA GPU 지표인 경우 prometheus-dcgm 확인을 추천드립니다. 이번 글에서는 대표 컴포넌트인 node-exporter 와 kube-state-metrics을 확인하겠습니다.\n1. Prometheus-node-exporter 시스템 메트릭을 수집하는 역할을 하는 소프트웨어입니다. Prometheus 모니터링 플랫폼의 일부로서 동작하며 노드에서 시스템 지표를 추출하고 Prometheus 서버가 쉽게 수집할 수 있는 형태로 제공합니다. node-exporter는 collector를 통해 지표를 수집합니다. 이를 통해 확인할 수 있는 지표는 다양합니다.\n시스템 외부 상태, 계층별 네트워크, CPU, 메모리, 디스크, 파일시스템, OS 지표등을 확인할 수 있습니다.또한, 시스템 커널 파라미터 지표(perf, sysctl) 도 제공됩니다. 이를 통해 시스템 및 애플리케이션 성능을 분석하는데 사용되지만 추가 작업이 필요합니다. 각 분야별로 세부 지표(가상 메모리 사용, ZFS, XFS 등..) 가 다양하게 제공됩니다. 자세한 내용은 공식 문서를 참고해주세요.\n수집된 지표들은 프로메테우스를 통해 확인할 수 있습니다.\n1 2 3 4 5 6 7 # sysctl, TCP 소켓 수신 버퍼 # 최소 버퍼 크기 (4KB) node_sysctl_net_ipv4_tcp_rmem{index=\u0026#34;0\u0026#34;} 4096 # 기본 버퍼 크기 (128 KB) node_sysctl_net_ipv4_tcp_rmem{index=\u0026#34;1\u0026#34;} 131072 # 최대 크기 버퍼 (6 MB) node_sysctl_net_ipv4_tcp_rmem{index=\u0026#34;2\u0026#34;} 6291456 2. kube-state-metrics 쿠버네티스 클러스터 내 오브젝트 상태에 대한 메트릭을 생성하는 컴포넌트입니다. 프로메테우스와 같은 모니터링 시스템에서 사용하기 위한 메트릭을 제공합니다. 쿠버네티스 오브젝트 단위(node, pod, service, deployment 등..) 정말 다양한데요. 이를 통해 쿠버네티스 클러스터의 상태, 성능 및 운영 효율성을 관리하고 최적화하는데 사용됩니다.\n❓ 어떤 메트릭을 모니터링 해야할까 간단한 대답은 ‘모두’ 이지만, 메트릭이 너무 많으면 중요한 신호를 발견하기 어렵습니다! 따라서 계층적 방식으로 접근을 추천합니다.\n[계층적 방식]\n물리적 혹은 가상의 노드 ( CPU, 메모리, 네트워크, 디스크 사용률) 클러스터 컴포넌트 (etcd 레이턴시) 클러스터 추가 기능 (HPA, 인그래스 컨트롤러) 최종 사용자 애플리케이션 대시보드를 통한 지표 확인 위에서 확인한 컴포넌트의 지표들은 kube-prometheus-stack를 통해 손쉽게 가능합니다. EKS에서 kube-prometheus-stack을 배포하고 대시보드인 grafana을 통해 지표를 확인하겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm fetch prometheus-community/kube-prometheus-stack --untar --version 45.7.1 cat \u0026lt;\u0026lt;EOT \u0026gt; ./values-kube-prometheus-stack.yaml alertmanager: enabled: false grafana: defaultDashboardsTimezone: Asia/Seoul adminPassword: admin1234 ingress: enabled: true ingressClassName: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; hosts: - grafana.$KOPS_CLUSTER_NAME paths: - /* prometheus: # 사이드카 노출 서비스 설정 thanosService: enabled: true ingress: enabled: true ingressClassName: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; hosts: - prometheus.$KOPS_CLUSTER_NAME paths: - /* prometheusSpec: podMonitorSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false retention: 5d retentionSize: \u0026#34;10GiB\u0026#34; scrapeInterval: \u0026#34;15s\u0026#34; # alert 관련 설정으로 주석 처리 # evaluationInterval: 15s EOT kubectl create ns monitoring helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.7.1 \\ -f values-kube-prometheus-stack.yaml --namespace monitoring 대시보드를 확인하면 해당 컴포넌트를 통해 시각화된 대시보드를 확인할 수 있습니다. 다만, 확인해야할 대시보드와 지표가 너무 많은데요. 어떤 것을 알람을 해야할지 어떤 지표를 확인해야할 지 감이 안오실 것입니다. 이 부분은 기본 알람을 통해 일부 해소를 할 수 있는데요. kube-prometheus-stack 의 default alram을 확인하면 219개의 룰이 설정되어 있습니다. 룰 안에는 쿠버네티스 컴포넌트 별로의 설정 알람과 리소스 알람, 쿠버네티스 오브젝트별 알람을 확인할 수 있습니다.\nnode-exporter 와 kube-state-metrics 알람\n리소스 알람\n마치며 서버 지연을 확인하기 위해 지표를 시스템별로 확인하였습니다. 트러블슈팅 과정에서 지표 확인은 시작 단계입니다. 한마디로 지표만 확인하고 끝이 아니라 지표를 시작으로 원인 분석을 들어가야 합니다. 예를 들어 지표 확인 후 이슈 있는 애플리케이션의 로그 확인하거나 트레이싱을 확인해야 합니다.\n","date":"Oct 15","permalink":"https://HanHoRang31.github.io/post/why-server-slow/","tags":["eks","cloud","AWS","kubernetes","prometheus","grafana"],"title":"왜 서버가 지연될까?"},{"categories":null,"contents":"안정적인 서비스를 오픈하기 위한 조건은 무엇일까요? 좋은 아키텍처도 물론 중요하지만, 트래픽을 견딜 수 있는 부하 테스트가 중요합니다. 이번 블로그 글에서는 부하 테스트 관련 글을 작성하고자 합니다.\n이벤트 대응 프로세스 부하 테스트를 알아보기 위해 이벤트 대응 프로세스를 알아보겠습니다. (좋은 영상을 공유해주신 장준엽님 감사합니다! 영상 링크)\n이벤트 랜딩 페이지 점검 : 부하가 제일 몰리는 구간으로 중요시 봐야하는 구간입니다. 페이지 랜딩 용량으로 7MB 정도(문제가 없는 수준)를 추천합니다.\nELB Pre-Warmining 신청 : AWS ELB도 내부적 하드웨어 장비로 미리 늘려 놔야 합니다. 보통 ELB에 IP 할당 2개로 초당 200~300 트래픽이 처리가 가능하며 그 이상의 이벤트 발생시 사전 신청이 필요합니다.\n부하 테스트 / 성능지연 원인 분석 : 부하 테스트가 진행되는 단계입니다. 보통 오픈소스 도구를 통해 진행합니다. (아래 내용 계속)\n이벤트 모니터링 : 공유해주신 CloudWatch 주요 모니터링 지표입니다. 생소한 지표로 Surge 큐 Length는 ELB 뒷단의 웹서버가 처리하는 못하는 것을 확인하기 위한 지표라 합니다.\n이벤트 대응 프로세스 중 부하 테스트 단계를 진행하겠습니다. 과부하 테스트 도구로는 대표적으로 Jmeter, K6, Locust 등 존재하나 이번 글에서는 Loucst를 다룰 예정입니다.\nLocust Locust는 오픈소스 부하 테스트 도구입니다. Locust는 파이썬 코드 기반으로 테스트 케이스가 작성되어 쉬운 스크립팅을 통해 복잡한 테스트 시나리오를 구성할 수 있습니다. (Jmeter 는 Java, k6는 JavaScript) 그리고 Loucst는 마스터-슬레이브 구조로 여러 기기에서 분산 테스트를 진행할 수 있어 대량의 트래픽을 시뮬레이션 할 수 있는 이점이 있으며 웹 UI를 제공하여 테스트 결과에 대해 빠르게 파악할 수 있습니다. 아래 화면은 예시 화면입니다.\n참고로 분산 테스트는 다른 도구들도 다 지원됩니다. 특별히 Locust를 선택한 이유는 필자가 파이썬에 친화적이고 , git repo Star가 가장 많아 자료를 쉽게 찾을 수 있는 점, 웹 UI와 동시에 CLI 가 제공되기 때문이였습니다.\n한가지 염두했던 점은 Locust가 파이썬 기반이라 분산 테스트에 효과적으로 테스트할 수 있을까였습니다. 원리를 찾아보니 세부 동작은 gevent라 하여 코루틴 라이브러리로 동작한다고 하네요. 코루틴은 프로그램이 여러 진입점을 가지고 비동기적으로 실행되도록 지원하는 프로그램 구성 요소로 비동기 태스크를 쉽게 구성 및 관리할 수 있고 효과적인 동시성 처리를 관리할 수 있게 만듭니다. 또한, Locust 내부적으로 특정 클라이언트(Locust Class) 를 지원하여 여러 타입의 클라이언트를 동시에 만들어서 실행할 수 있다고 합니다.\nLocust 아키텍처를 통해 동작 원리를 파악해보겠습니다.\nhttps://locust.dev/docs/architecture\n특별히 확인할 점은 두가지입니다. Redis 기반으로 잡 스케쥴링을 관리하며, 실제 잡은 워커 노드에 할당하여 진행한다는 점입니다. 워커 노드안에 loucst.execute(), loucst.vaildate() 는 워커 노드 실행시 잡이 수행되는 함수입니다. 각 함수가 순차적으로 진행되는데요. 테스트 시나리오 구성시 워크 노드나 전체 노드에 설치 프로그램이 필요할 수 있는데 setup \u0026amp; teardown(실행시 전체 한번), on_start \u0026amp; on_stop (클라이언트 실행마다 한번) 함수를 통해 시나리오를 구성할 수 있습니다.\n배포 EKS에 Locust를 배포하여 테스트해보겠습니다. 테스트 예제 애플리케이션은 지난 아키텍처 글에서 활용한 투표 애플리케이션을 활용하겠습니다.\n투표 애플리케이션 배포 참고\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # git clone git clone https://github.com/HanHoRang31/blog-share.git cd blog-chare/k8s-app/vote-app tree # 네임스페이스 생성 후 변경 kubectl create ns vote # 서비스 배포 kubectl apply -f . # ExternaDNS 추가 (없으면 생략) ## 각자 자신의 도메인 정보 입력 MyDOMAIN1=\u0026lt;각자 자신의 nginx 도메인 지정\u0026gt; MyDOMAIN1=vote.hanhorang.link MyDOMAIN2=result.hanhorang.link kubectl annotate service vote \u0026#34;external-dns.alpha.kubernetes.io/hostname=$MyDOMAIN1.\u0026#34; kubectl annotate service result \u0026#34;external-dns.alpha.kubernetes.io/hostname=$MyDOMAIN2.\u0026#34; Locust 배포\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # repo 설치 helm repo add deliveryhero https://charts.deliveryhero.io/ helm repo update helm repo list --- NAME URL deliveryhero https://charts.deliveryhero.io/ # 매개 변수 확인 helm show values deliveryhero/locust \u0026gt; values.yaml --- loadtest: # loadtest.name -- 이 부하 테스트에 사용될 리소스 및 설정의 이름 name: example # loadtest.locust_locustfile -- locustfile의 이름 locust_locustfile: main.py # loadtest.locust_locustfile_path -- locustfile의 경로 (끝에 슬래시 제외) locust_locustfile_path: \u0026#34;/mnt/locust\u0026#34; # loadtest.locust_locustfile_configmap -- locustfile을 포함하는 configmap의 이름 (기본값은 예제 locustfile 사용) locust_locustfile_configmap: \u0026#34;example-locustfile\u0026#34; # loadtest.locust_lib_configmap -- 라이브러리를 포함하는 configmap의 이름 (기본값은 예제 라이브러리 사용) locust_lib_configmap: \u0026#34;example-lib\u0026#34; # loadtest.locust_host -- 부하 테스트할 대상 호스트 locust_host: https://www.google.com # loadtest.pip_packages -- 설치할 추가 파이썬 pip 패키지의 목록 pip_packages: [] # loadtest.environment -- 마스터와 워커 모두에게 적용될 환경 변수 environment: {} # VAR: VALUE . . worker: # worker.image -- 사용자 정의 도커 이미지를 포함한 태그 image: \u0026#34;\u0026#34; # worker.logLevel -- 로그 레벨. INFO 또는 DEBUG 가능 logLevel: INFO replicas: 1 # worker.pdb.enabled -- worker 파드에 대한 PodDisruptionBudget를 생성할지 여부 pdb: enabled: false hpa: enabled: false minReplicas: 1 maxReplicas: 100 targetCPUUtilizationPercentage: 40 차트 매개 변수는 깃허브 링크에서 확인이 가능합니다. 차트 변수로 특별하게 볼점은 다음과 같습니다.\n테스트 스크립트는 loadtest 변수 (locust_locustfile_configmap) 설정이나 경로를 지정한다음 볼륨을 통해 직접 삽입합니다.(loadtest.locust_locustfile_path) 를 통해 가져옵니다. 워커노드에 대한 과부하 설정은 worker에서 진행합니다. 과부하 테스트 해당 블로그 글에서는 투표 애플리케이션에 대해 과부하 테스트 스크립트를 configmap으로 작성하여 업데이트하여 진행하겠습니다. 과부하 테스트 스크립트는 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Execute in root of repository, and maybe this file has existed. cat \u0026lt;\u0026lt;EOF \u0026gt; locustfile.py \u0026amp;\u0026amp; cat locustfile.py from locust import HttpUser, task, between import random class UserBehavior(HttpUser): wait_time = between(1, 2.5) @task(1) def get_home_page(self): self.client.get(\u0026#34;http://result.hanhorang.link/\u0026#34;) @task(2) def post_vote(self): vote = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;] # a represents \u0026#39;Cats\u0026#39;, b represents \u0026#39;Dogs\u0026#39; self.client.post(\u0026#34;http://vote.hanhorang.link/\u0026#34;, data={\u0026#39;vote\u0026#39;: random.choice(vote)}) EOF 코드는 간단하다. wait_time 변수 설정으로 1초 ~2.5초 사이의 무작위 대기 시작을 가진 후 @task(1) 과 @task(2) 작업을 1:2 비율로 작업을 수행합니다. @task(1)은 투표 결과화면(밑 화면 오른쪽) 을 조회하고 @task(2)는 dog, cat 투표를 무작위 진행(밑 화면 왼쪽)합니다.\n스크립트를 구성했으면 config를 수정하고 locust를 배포하겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 스크립트 배포 kubectl create configmap eks-loadtest-locustfile --from-file ./locustfile.py # locust 배포 helm upgrade --install locust deliveryhero/locust \\ --set loadtest.name=eks-loadtest \\ --set loadtest.locust_locustfile_configmap=eks-loadtest-locustfile \\ --set loadtest.locust_locustfile=locustfile.py \\ --set worker.hpa.enabled=true \\ --set worker.hpa.minReplicas=5 # 서비스 ALB 배포 cat \u0026lt;\u0026lt;EOF \u0026gt; locust-ingress.yaml \u0026amp;\u0026amp; cat locust-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip kubernetes.io/ingress.class: alb finalizers: - ingress.k8s.aws/resources labels: app: ingress name: ingress-locust-dashboard namespace: default spec: defaultBackend: service: name: locust port: number: 8089 EOF kubectl apply -f locust-ingress.yaml 배포를 완료하면 ingress ADDRESS 를 입력하여 locust 웹 UI로 접속하여 테스트를 진행하겠습니다.\n왼쪽 처음 화면에서 매개 변수을 입력(밑 사진)하여 테스트를 진행하자. 매개변수는 다음과 같습니다.\nNumber of Users : 동시 사용자 수 Spawn Rate: 빠르게 추가되는 수치, 1로 설정했으므로 초당 1명씩 사용자를 추가한다. Host: 이는 부하 테스트의 대상이 되는 웹 사이트나 애플리케이션의 URL 테스트를 진행하면 config에서 설정한 스크립트 구성에서의 task 비율만큼 post와 get 함수가 1:2 비율로 호출됨을 알 수 있습니다.\n과부하 테스트를 진행하겠습니다, 밑의 결과 화면을 보면 다음과 같이 사용자가 1000 명에서 응답 시간이 증가함을 알 수 있습니다. 많게는 50000ms로 50초에 해당되어 실제 운영시 추가 작업이 필요함을 알 수 있는데요, Task 함수의 응답시간을 보면 Post 함수에서 지연시간이 발생하는 것을 알 수 있습니다.\n가장 먼저 처리한 조치로 POST 응답시간을 최소화시키기 위해 해당 기능 파드인 vote, worker, redis 파드를 증가시켜 봤지만 파드 사용량이 분산되었을 뿐 처리 결과는 똑같았습니다. 다음 단계로는 파드 내 로그를 직접 확인해서 조치해야겠네요.\n과부하 테스트(2) 실제 과부하 테스트를 한다면 처음 페이지에서 이벤트 페이지까지의 트래픽이 구성됩니다. 예를 들면, 할인행사시, e-마켓 메인페이지에서 할인행사 이벤트 페이지까지로 생각하시면 이해하기 편하실 것 같습니다. 과부하 테스트에서도 해당 단계의 테스트가 필요합니다.\n앞 서 투표 예플리케이션에서는 랜덤으로 투표 1,2를 찍는 것을 테스트했는데요. 랜덤이라 분산되는 시나리오였습니다. 여기서는 투표 1,2를 랜덤으로 찍고 결고화면까지 가는 것을 코드로 구성하겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat \u0026lt;\u0026lt;EOF \u0026gt; locustfile.py \u0026amp;\u0026amp; cat locustfile.py from locust import HttpUser, task, between, SequentialTaskSet import random class UserBehavior(SequentialTaskSet): @task def post_vote(self): vote = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;] # a represents \u0026#39;Cats\u0026#39;, b represents \u0026#39;Dogs\u0026#39; self.client.post(\u0026#34;http://vote.hanhorang.link/\u0026#34;, data={\u0026#39;vote\u0026#39;: random.choice(vote)}) @task def get_home_page(self): self.client.get(\u0026#34;http://result.hanhorang.link/\u0026#34;) class MyUser(HttpUser): tasks = [UserBehavior] wait_time = between(1, 2.5) EOF 주요 변경사항은 코드 내 클래스 구성입니다. Myuser 클래스 구성으로 순차적으로 작업 코드를 구성하였습니다. 이렇게 된다면 Locust Client 는 먼저 랜덤으로 A,B를 투표하고 결과 페이지를 조회하는 시나리오로 진행하게 됩니다.\n부하 테스트 팁 오늘은 오픈소스 Locust를 통해 부하테스트를 다뤘습니다. 부하테스트를 진행하니 성능지연 원인 분석도 확인이 필요한 단계로 생각되네요. AWS 공식 문서를 찾아보면 EKS에서 부하 테스트 팁을 확인할 수 있습니다. 거의 파드 라이프사이클, HPA, CA 구성 관련 팁이 있네요. 향후 구성시 고려해봐야겠습니다. 다음 글은 부하테스트에 이어서 성능지연에 관한 원인 분석 글을 다뤄볼 예정입니다.\n","date":"Oct 01","permalink":"https://HanHoRang31.github.io/post/locust-test/","tags":["eks","cloud","AWS","kubernetes","Locust"],"title":"Locust를 이용한 과부하 테스트"},{"categories":null,"contents":" 1 2 3 T101 3기(=Terraform 101 Study)는 Terraform 실무 실습 스터디입니다. CloudNet@ 유형욱, 윤서율님이 진행하시며, 책 \u0026#34;테라폼으로 시작하는 IaC\u0026#34;을 기반으로 진행하고 있습니다. 지난 포스트 글에서는 테라폼 클라우드를 통해 EKS를 프로비저닝하였습니다. 프로비저닝은 EKS 모듈을 통해 진행하였는데요. 이번 글에서는 EKS 모듈에 대해 자세히 다뤄볼 예정입니다. 테라폼 레파지토리를 확인하면 EKS 모듈 사용 예제를 비롯하여 ALB 컨트롤러, External DNS와 같은 addon를 확인할 수 있습니다. 이번 글에서는 모듈 예제를 학습하고 EKS와 기본 addon을 프로비저닝하겠습니다. 구성 예제는 필자의 깃허브에서 확인이 가능합니다.\n1. 테라폼 EKS 모듈 EKS 모듈 예제를 확인하면 EKS에서 프로비저닝할 수 있는 모든 워크 노드 타입이 제공되는 것을 확인할 수 있습니다. 각 워크 노드 타입의 특징은 다음과 같이 요약할 수 있습니다.\neks_managed_nodegroup (EKS Managed Node Groups) : EKS 관리형 노드 그룹입니다. AWS가 노드 그룹의 라이프사이클을 자동으로 관리해주는 타입입니다. 작동 상태를 모니터링하고 필요에 따라 패치 적용 및 업데이트를 자동으로 수행합니다. Fargate : 노드 타입을 서버리스로 설정해주는 옵션입니다. 노드(ec2)가 아니라 별도로 관리할 필요가 없으며 컨테이너가 fargate로 할당됩니다. 쿠버네티스의 복잡한 관리 작업 없이 서비스를 실행할 수 있습니다. Karpenter : karpenter는 오픈소스 자동 스케일러로 karpenter 수행되는 서버 타입(EC2 Fleet)로 할당하여 노드 그룹을 구성합니다. karpenter 를 적용하면 빠르게 서버를 스케일러할 수 있게 됩니다. self_managed_nodegroup (Self-Managed Node Groups) : 비관리형 노드 그룹입니다. 사용자가 노드의 모든 측면을 관리해야 합니다. Custom AMI를 통해 노드를 구성할 수 있으며 보안 목적이나 노드의 추가 설정 필요시 사용합니다. Outposts : 로컬 EKS 노드 그룹 옵션입니다. 온프레미스 또는 다른 원격 위치에서 노드 그룹을 프로비저닝하여 EKS 클러스터를 운영할 수 있습니다. 위 예제 옵션 중 다루지 않은 complete는 모든 노드 그룹의 유형을 사용하는 예제이며, user_data는 노드의 부트스트랩 스크립트 및 구성하는 예제입니다. 예제 중 가장 기본이 되는 self-managed-node-group 예제를 통해 모듈 사용 예를 확인해보겠습니다. 모듈 구성은 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 . ├── modules │ ├── _user_data │ │ ├── README.md │ │ ├── main.tf │ │ ├── outputs.tf │ │ ├── variables.tf │ │ └── versions.tf │ ├── eks │ │ ├── README.md │ │ ├── main.tf │ │ ├── node_groups.tf │ │ ├── outputs.tf │ │ ├── templates │ │ │ ├── aws_auth_cm.tpl │ │ │ ├── bottlerocket_user_data.tpl │ │ │ ├── linux_user_data.tpl │ │ │ └── windows_user_data.tpl │ │ ├── variables.tf │ │ └── versions.tf │ ├── eks-managed-node-group │ │ ├── README.md │ │ ├── main.tf │ │ ├── outputs.tf │ │ ├── variables.tf │ │ └── versions.tf │ ├── fargate-profile │ │ ├── README.md │ │ ├── main.tf │ │ ├── outputs.tf │ │ ├── variables.tf │ │ └── versions.tf │ ├── karpenter │ │ ├── README.md │ │ ├── main.tf │ │ ├── outputs.tf │ │ ├── variables.tf │ │ └── versions.tf │ └── self-managed-node-group │ ├── README.md │ ├── main.tf │ ├── outputs.tf │ ├── variables.tf │ └── versions.tf └── self_managed_node_group # 루트 모듈 ├── README.md ├── main.tf ├── outputs.tf ├── variables.tf └── versions.tf 디렉토리 modules 에 사용하는 커스컴 모듈을 모두 정의합니다. self_managed_node_group 폴더에 모듈에 따른 값을 정의합니다. self_managed_node_group main.tf 을 확인하면 eks 클러스터 구성, 노드 그룹 구성, vpc 구성을 확인할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 module \u0026#34;eks\u0026#34; { source = \u0026#34;../modules/eks\u0026#34; # 클러스터 정보 입력 cluster_name = local.name cluster_version = local.cluster_version cluster_endpoint_public_access = true # addon 구성 cluster_addons = { coredns = { most_recent = true } kube-proxy = { most_recent = true } vpc-cni = { most_recent = true } } # vpc 설정 vpc_id = module.vpc.vpc_id subnet_ids = module.vpc.private_subnets control_plane_subnet_ids = module.vpc.intra_subnets # Self-managed node groups에 대한 AWS auth ConfigMap 자동 생성 설정 create_aws_auth_configmap = true manage_aws_auth_configmap = true # Self-managed node groups에 대한 기본 설정 self_managed_node_group_defaults = { # Cluster-autoscaler를 위한 자동 스케일링 그룹 태그 활성화 autoscaling_group_tags = { \u0026#34;k8s.io/cluster-autoscaler/enabled\u0026#34; : true, \u0026#34;k8s.io/cluster-autoscaler/${local.name}\u0026#34; : \u0026#34;owned\u0026#34;, } } # 노드 그룹 설정 self_managed_node_groups = { .. } # aws-auth 설정 aws_auth_users = [ .. ] # 태그 설정 tags = local.tags } 1.1 노드 그룹 eks 모듈 내부의 self_managed_node_groups dict 을 통해 노드 그룹을 구성할 수 있습니다.. 구성 예제에서는 5가지의 노드 그룹으로 확인이 가능합니다. 노드 그룹별 키워드만 정리하면 다음과 같이 구성할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 self_managed_node_groups = { # 1. default_node_group 초기 설정 노드 그룹 구성 default_node_group = {} # 2. 컨테이너 운영의 특화된 bottlerocket AMI 사용하여 노드 그룹 구성 bottlerocket = { platform = \u0026#34;bottlerocket\u0026#34; ami_id = data.aws_ami.eks_default_bottlerocket.id .. } # 3. 혼합 인스턴스 노드 그룹 구성 mixed = { # 혼합 인스턴스 사용 use_mixed_instances_policy = true # 혼합 인스턴스 정책 설정 mixed_instances_policy = { instances_distribution = { on_demand_base_capacity = 0 # 온디맨드 인스턴스 초기 0개로 설정 on_demand_percentage_above_base_capacity = 20 # 온디맨드 인스턴스 비중을 20프로로 설정 spot_allocation_strategy = \u0026#34;capacity-optimized\u0026#34; # spot 인스턴스 할당받을 때 용량 최적화 전략으로 설정 } # 가중치 설정 override = [ { instance_type = \u0026#34;m5.large\u0026#34; weighted_capacity = \u0026#34;1\u0026#34; }, { instance_type = \u0026#34;m6i.large\u0026#34; weighted_capacity = \u0026#34;1\u0026#34; }, ] } } # 4. 고성능 컴퓨팅(HPC) 및 기계 학습 애플리케이션의 속도를 높일 수 있는 네트워크 디바이스 사용 인스턴스 efa = { # 주의 인스턴스 타입이 고비용에서만 제공됩니다. # aws ec2 describe-instance-types --region eu-west-1 --filters Name=network-info.efa-supported,Values=true --query \u0026#34;InstanceTypes[*].[InstanceType]\u0026#34; --output text | sort instance_type = \u0026#34;c5n.9xlarge\u0026#34; network_interfaces = [ { description = \u0026#34;EFA interface example\u0026#34; delete_on_termination = true device_index = 0 associate_public_ip_address = false interface_type = \u0026#34;efa\u0026#34; } ] .. } # 5. 완전 사용자 정의 노드 그룹 설정이나 프로비저닝시, 버그로 넘어갑니다. complete = { .. } } 구성시 efa 노드 그룹 구성을 주의해주세요. efa 기능 지원 인스턴스가 고비용의 인스턴스(9x.large) 이상으로만 설정이 가능합니다. 필자는 제외했습니다. complete 노드 그룹은 사용자 정의 노드 그룹으로 디바이스, 모니터링, 메타데이터 옵션을 통해서 노드 그룹을 구성하지만, eks 에 붙지 않습니다. arg를 통해 노드 그룹으로 연결해야 하는지는 확인이 필요합니다. EKS 구성시 노드 그룹 정보는 EKS 탭에서 확인이 가능합니다. 자세한 노드 그룹은 정보는 EC2 텝의 오토스케일링 그룹 정보를 확인해주세요.\n1.2 보안 구성 EKS 모듈을 통해 보안 구성을 크게 3가지로 분류하여 설정할 수 있습니다.\n1.2.1 EKS Auth\nAWS IAM 과 kubernetes RBAC 를 통해 EKS 클러스터에 대한 신원 정보를 확인하는 방법입니다. eks 모듈에서 aws_auth_users 에서 iam 정보를 입력합시다. 입력된 정보는 aws-auth configmap에 등록됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 module \u0026#34;eks\u0026#34; { source = \u0026#34;../modules/eks\u0026#34; .. aws_auth_users = [ { userarn = var.aws_auth_users_arn username = var.aws_auth_users_username groups = [\u0026#34;system:masters\u0026#34;] } ] } AWS auth에서 IAM 사용자 정보를 입력하지 않을 경우 EKS 콘솔에서 다음의 메세지가 확인되며 노드 그룹의 리소스를 확인할 수 없습니다.\n‘This may be due to the current user or role not having Kubernetes RBAC permissions to describe cluster resources or not having an entry in the cluster’s auth config map’\n1.2.2 Security Group\n보안 그룹(Security Groups, SG)은 인스턴스에 대한 인바운드 및 아웃바운드 트래픽을 제어하는 가상 방화벽 역할을 합니다. main.tf에서는 별도로 설정하지 않았습니다. eks 모듈과 node_group 모듈에서 자동으로 설정됩니다.\n초기 sg의 경우 클러스터 sg은 인그래스 포트로 443만 허용되며, 노드 그룹의 경우 다음과 같이 설정됩니다.\n1.2.3 AWS IAM\nEKS에서는 IAM 역할을 사용하여 노드 그룹이 AWS 리소스와 통신할 수 있는 권한을 부여합니다. main에서는 할당하지 않지만, 모듈에서 기본 IAM 정책을 할당합니다. 클러스터 할당 정책은 모듈에서 확인이 가능합니다. 관리형 정책 AmazonEKSClusterPolicy, AmazonEKSVPCResourceController 할당받으며, kms 사용 정책을 고객 관리형 정책으로 사용합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Policies attached ref https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;this\u0026#34; { for_each = { for k, v in { AmazonEKSClusterPolicy = local.create_outposts_local_cluster ? \u0026#34;${local.iam_role_policy_prefix}/AmazonEKSLocalOutpostClusterPolicy\u0026#34; : \u0026#34;${local.iam_role_policy_prefix}/AmazonEKSClusterPolicy\u0026#34;, AmazonEKSVPCResourceController = \u0026#34;${local.iam_role_policy_prefix}/AmazonEKSVPCResourceController\u0026#34;, } : k =\u0026gt; v if local.create_iam_role } policy_arn = each.value role = aws_iam_role.this[0].name } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;additional\u0026#34; { for_each = { for k, v in var.iam_role_additional_policies : k =\u0026gt; v if local.create_iam_role } policy_arn = each.value role = aws_iam_role.this[0].name } 1.3 배포 확인 테라폼 명령어를 통해 EKS를 프로비저닝합시다. 시간은 약 15분 정도 소요됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # /terraform-t101-eks/self_managed_node_group ➜ terraform init .. ➜ terraform plan .. ➜ terraform apply -auto-approve # 20분 소요... ➜ eksctl get cluster NAME REGION EKSCTL CREATED t1013-eks ap-northeast-2 False ➜ eksctl utils write-kubeconfig --cluster t1013-eks --region ap-northeast-2 2023-09-15 18:16:45 [✔] saved kubeconfig as \u0026#34;/Users/mzc02-hseungho/.kube/config-f08\u0026#34; ➜ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-d7sd4 2/2 Running 0 24m kube-system aws-node-gx7lk 2/2 Running 0 24m kube-system aws-node-xlnsx 2/2 Running 0 24m kube-system coredns-754fbc56c6-fmcqr 1/1 Running 0 24m kube-system coredns-754fbc56c6-qwkdk 1/1 Running 0 24m kube-system kube-proxy-6mg4q 1/1 Running 0 24m kube-system kube-proxy-7hlv8 1/1 Running 0 24m kube-system kube-proxy-fqbls 1/1 Running 0 24m 2. 테라폼 EKS addon 모듈 EKS (Elastic Kubernetes Service)의 애드온(add-ons)은 EKS 클러스터의 기능성과 운영을 강화하는 소프트웨어 컴포넌트입니다. 이러한 애드온은 주로 네트워킹, 로깅, 모니터링, 보안 등 다양한 목적으로 사용됩니다. EKS는 몇 애드온에 대해 AWS 서비스를 연동하여 제공하기도 합니다.\naddons 이름 연동 AWS 서비스 설명 VPC CNI VPC AWS VPC (Virtual Private Cloud) 내 통신을 위한 네트워크 인터페이스(CNI) CoreDNS VPC 서비스 디스커버리와 DNS를 위한 애드온 kube-proxy VPC Kubernetes 네트워크 프록시 관리 external-dns Route53 쿠버네티스 서비스와 인그레스에 대한 DNS 레코드 관리 ALB Controller ALB ALB를 쿠버네티스 인그레스 리소스로 관리 EBS CSI Controller EBS EBS 볼륨을 쿠버네티스 퍼시스턴트 볼륨으로 관리 EKS addon 도 테라폼 모듈을 통해 설치가 가능합니다. 테라폼 레파지토리 fully-loaded-eks-cluster 에 모듈화된 addon 서비스를 확인할 수 있습니다.\n위에 소개한 모듈을 통해 addon 구성은 모듈 변수의 flag를 통해 손 쉽게 구성이 가능합니다. 다만, 개인적으로 내부 동작 이해가 어려워 확장성을 원하는 분들이나 추후 모듈을 구성하려는 분들에게 해당 모듈 사용을 추천드리지 않습니다.\n공식적으로 제공하는 모듈외에도 사용자 모듈이 많습니다. 이번 포스트 글에서는 자주 다뤘던 ALB controller,external dns addon에 대해 직관적인 테라폼 모듈을 찾아 구성해보겠습니다.\n2.1 ALB Controller 개인적으로 ALB controller 직관적인 모듈로 깃허브 campaand 님의 모듈을 추천드립니다. 모듈 내 main.tf을 확인하면 내부 동작을 확인할 수 있습니다. 먼저, alb 을 관리하기 위한 IAM 정책 선언 후 role에 연결합니다. 그 다음 helm 프로바이더를 통해 aws-load-balancer-controller 차트를 구성합니다.\n모듈 문서를 참고하면, 헬름 차트의 추가 매개변수가 필요할 때는 다음 예제와 같이 구성할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 module \u0026#34;alb_controller\u0026#34; { source = \u0026#34;campaand/alb-controller/aws\u0026#34; version = \u0026#34;~\u0026gt; 2.0\u0026#34; cluster_name = var.cluster_name # 차트 버전 helm_chart_version = \u0026#34;1.6.0\u0026#34; # 매개변수 선언 settings = { key1 = value1, key2 = value2, key3 = value3, key4 = value4 } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 module \u0026#34;alb_controller\u0026#34; { source = \u0026#34;campaand/alb-controller/aws\u0026#34; version = \u0026#34;~\u0026gt; 2.0\u0026#34; cluster_name = var.cluster_name helm_chart_version = \u0026#34;1.6.0\u0026#34; settings = { key1 = value1, key2 = value2, key3 = value3, key4 = value4 } } 필자의 경우 main.tf 아래의 addon 구성을 위한 코드를 다음과 같이 구성하였습니다.\n1 2 3 4 5 6 module \u0026#34;alb_controller\u0026#34; { source = \u0026#34;campaand/alb-ingress-controller/aws\u0026#34; version = \u0026#34;2.0.0\u0026#34; cluster_name = module.eks.cluster_name } terraform apply로 배포 시 정상적으로 alb-controller가 구성됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # campaand/alb-ingress-controller/aws 모듈 추가 ➜ terraform init .. ➜ terraform plan .. ➜ terraform apply -auto-approve .. # module.alb_controller.helm_release.alb_ingress_controller will be created + resource \u0026#34;helm_release\u0026#34; \u0026#34;alb_ingress_controller\u0026#34; { + atomic = false + chart = \u0026#34;aws-load-balancer-controller\u0026#34; + cleanup_on_fail = true + create_namespace = false + dependency_update = false + disable_crd_hooks = false + disable_openapi_validation = false + disable_webhooks = false + force_update = false + id = (known after apply) .. .. Apply complete! Resources: 2 added, 1 changed, 0 destroyed. ➜ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-load-balancer-controller-7f46f44c48-2st75 1/1 Running 0 17s kube-system aws-load-balancer-controller-7f46f44c48-z98zf 1/1 Running 0 17s kube-system aws-node-5q745 2/2 Running 0 3h35m kube-system aws-node-p2r9t 2/2 Running 0 3h35m kube-system coredns-754fbc56c6-btppz 1/1 Running 0 3h33m kube-system coredns-754fbc56c6-dtfgd 1/1 Running 0 3h33m kube-system kube-proxy-4vgxv 1/1 Running 0 3h35m kube-system kube-proxy-nts26 1/1 Running 0 3h35m 2.2 External DNS External DNS 모듈을 찾아본 결과 불안정한 모듈들이 많습니다. 그나마 bohdantverdyi 님의 모듈이 깔끔하고 구성에 성공했네요. 모듈 구성은 다음과 같이 구성했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # main.tf module \u0026#34;external_dns\u0026#34; { source = \u0026#34;terraform-iaac/external-dns/kubernetes\u0026#34; version = \u0026#34;1.1.10\u0026#34; namespace = \u0026#34;kube-system\u0026#34; create_namespace = false dns = [\u0026#34;hanhorang.link\u0026#34;] # route53 domain dns_provider = \u0026#34;aws\u0026#34; aws_zone_type = \u0026#34;public\u0026#34; txt_owner_id = \u0026#34;\u0026#34; # route53 hosted zone id } 추가 설정 값은 깃허브 input 값을 확인해주세요. 테라폼을 통해 모듈 구성을 진행합시다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # terraform-iaac/external-dns/kubernetes 모듈 추가 ➜ terraform init .. ➜ terraform plan .. ➜ terraform apply -auto-approve .. + resource \u0026#34;kubernetes_deployment\u0026#34; \u0026#34;deploy_app\u0026#34; { + id = (known after apply) + wait_for_rollout = true + metadata { + generation = (known after apply) + labels = { + \u0026#34;app\u0026#34; = \u0026#34;external-dns\u0026#34; } + name = \u0026#34;external-dns\u0026#34; + namespace = \u0026#34;kube-system\u0026#34; + resource_version = (known after apply) + uid = (known after apply) } + spec { + min_ready_seconds = 0 + paused = false + progress_deadline_seconds = 600 + replicas = \u0026#34;1\u0026#34; + revision_history_limit = 10 + selector { + match_labels = { + \u0026#34;app\u0026#34; = \u0026#34;external-dns\u0026#34; } } + strategy { .. Apply complete! Resources: 2 added, 1 changed, 0 destroyed. ➜ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-load-balancer-controller-7f46f44c48-fmptj 1/1 Running 0 6m9s kube-system aws-load-balancer-controller-7f46f44c48-tw2gx 1/1 Running 0 6m9s kube-system aws-node-5q745 2/2 Running 0 5h52m kube-system aws-node-p2r9t 2/2 Running 0 5h52m kube-system coredns-754fbc56c6-btppz 1/1 Running 0 5h50m kube-system coredns-754fbc56c6-dtfgd 1/1 Running 0 5h50m kube-system external-dns-5887c5877c-4l49b 1/1 Running 4 (85s ago) 6m16s kube-system kube-proxy-4vgxv 1/1 Running 0 5h52m kube-system kube-proxy-nts26 1/1 Running 0 5h52m 3. 마치며 테라폼을 통해 addon을 구성하는 방법은 많습니다. 이번 포스트 글에서는 모듈을 통해 구성하였지만, 모듈 버전호환과 필요 매개 변수 등으로 구성 시간을 많이 소비하였습니다. 개인적으로는 모듈로 addon을 구성하는 것은 참고 예제가 많아지면 시도하는 것을 추천드립니다. 그 전까지는 스크립트나 CD를 통해 구성하는 것이 추천드립니다.\n","date":"Sep 15","permalink":"https://HanHoRang31.github.io/post/terraform-eks-addon/","tags":["T101","EKS","cloud","AWS","Terraform","Terraform Cloud"],"title":"[T1013] 테라폼 모듈로 EKS addon 관리하기"},{"categories":null,"contents":" 1 2 3 T101 3기(=Terraform 101 Study)는 Terraform 실무 실습 스터디입니다. CloudNet@ 유형욱, 윤서율님이 진행하시며, 책 \u0026#34;테라폼으로 시작하는 IaC\u0026#34;을 기반으로 진행하고 있습니다. 테라폼은 HCL(HashiCorp Configuration Language) 을 통해 인프라를 관리하고 프로비저닝합니다. HCL은 흡사 Go언어와 같이 반복문, 조건문, 함수가 제공되며 HCL 특유의 블록들이 구성되어 인프라를 코드로 관리할 수 있게 도와줍니다. 테라폼 기초 문법 이해로 책을 보는 것을 추천드립니다.\n이번 포스트에서는 그 다음 단계로, 실무 예제를 통해 HCL 문법이 어떻게 사용되는지 살펴보겠습니다. 또한, Terraform Cloud를 이용하여 인프라를 프로비저닝하는 방법에 대해서도 알아보겠습니다. 실무 예제는 스터디 참고 책 9장(인프라 운영 및 관리)에서 소개해주는 EKS 모듈 예제로 선택하였습니다. 예제는 책 저자님의 레파지토리에서 확인이 가능합니다.\n1.1 테라폼 모듈 이해 HCL 문법 사용에 들어가기 전 테라폼 모듈 구조를 알아보겠습니다. 테라폼 모듈은 재사용 가능한, 독립적인 코드 블록을 뜻합니다. 모듈을 사용하면 코드의 중복을 줄이고, 복잡한 인프라를 쉽게 관리할 수 있습니다. 참고 예제에서도 브랜치를 나뉘어 모듈 도입 전,후를 손 쉽게 비교할 수 있습니다. 레파지토리 브랜치 moularity 에서는 자주 사용하는 서비스(VPC 서비스들과 EKS)을 모듈로 구성하였고 이외의 추가 서비스(ECR, DB)를 따로 코드를 통해 구성하는 것을 확인할 수 있습니다. 모듈의 구조는 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ➜ tree . . // 차일드 모듈 ├── modules │ ├── common │ │ ├── README.md │ │ ├── main.tf │ │ ├── output.tf │ │ └── variable.tf │ └── eks │ ├── README.md │ ├── aws-iam-authenticator │ ├── iam-policy.json │ ├── kubectl │ ├── main.tf │ ├── output.tf │ ├── template │ │ ├── cert-manager.tpl │ │ ├── crd.tpl │ │ ├── eniconfig.tpl │ │ ├── ingress.tpl │ │ ├── kubeconfig.tpl │ │ └── sa.tpl │ └── variable.tf // 루트 모듈 ├── main.tf ├── output.tf └── variable.tf 폴더 modules를 기반으로 모듈을 구분하여 child, root 모듈로 구분합니다. 루트 모듈에는 사용할 변수들과 사용할 서비스를 구성하고, 차일드 모듈을 통해 실제 서비스를 구성합니다. 모듈을 통해 구성되는 서비스의 아키텍처는 다음과 같습니다.\nhttps://github.com/terraform101/terraform-refactoring-and-modularity/tree/modularity\n1.2 모듈 내 문법 이해 EKS 모듈 예제를 통해 어떻게 사용되는 지 확인하겠습니다.\n1.2.1 변수 사용 테라폼 구성 변수는 루트 모듈의 varible.tf을 통해 정의합니다. 변수를 살펴보자면, 변수의 민감 변수와 구성 변수로 나뉘어 집니다. 민감 변수는 환경 변수(TF로 시작) 나 테라폼 클라우드에서 저장한 변수를 통해 입력하는 것을 추천합니다. 민감 변수는 AWS 접근 키, 비밀키 등으로 구성됩니다.\n1 2 3 4 5 6 7 8 9 // varible.tf 의 민감변수 정의 ### FOR TEST variable \u0026#34;AWS_ACCESS_KEY_ID\u0026#34; {} variable \u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34; {} .. variable \u0026#34;ucmp-access-secret\u0026#34; { default = \u0026#34;\u0026#34; sensitive = true } 변수 사용의 또 다른 예로 예제 EC2 의 이미지를 data블록을 통해 AWS에 조회하여 사용합니다.\ndata 블록 값들 정보는 테라폼 공식문서에서 확인할 수 있습니다.\n1.2.2 함수 사용 HCL은 자체적으로 내장 함수와 반복문, 조건문이 지원됩니다.\n예제 모듈에서도 해당 문법을 통해 인프라를 프로비저닝하는데 사용됩니다. 사용 예는 다음과 같습니다.\ncount 조건문을 통한 서비스 구성 여부\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // 변수 구성 variable \u0026#34;enable_ecr\u0026#34; { type = bool default = false description = \u0026#34;ECR 활성/비활성\u0026#34; } variable \u0026#34;enable_elasticache\u0026#34; { type = bool default = false description = \u0026#34;Elasticache 활성/비활성\u0026#34; } .. .. // ecr 구성 여부 resource \u0026#34;aws_ecr_repository\u0026#34; \u0026#34;ecr\u0026#34; { count = var.enable_ecr ? 1 : 0 name = \u0026#34;ecr-${var.env}-${var.pjt}-imagerepository\u0026#34; .. } // elasticache_cluster 구성 여부 resource \u0026#34;aws_elasticache_cluster\u0026#34; \u0026#34;replica\u0026#34; { count = var.enable_elasticache ? 1 : 0 cluster_id = \u0026#34;elasticache-${var.env}-${var.pjt}-cluster-${count.index}\u0026#34; replication_group_id = aws_elasticache_replication_group.cluster[0].id } 내장 함수을 통한 인프라 구성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 1. 반복분 // 변수 developer_group_users에 정의된 목록만큼 iam user 생성 resource \u0026#34;aws_iam_user\u0026#34; \u0026#34;developer\u0026#34; { for_each = toset(var.developer_group_users) // toset 은 결과값의 목록을 데이터 타입으로 변환합니다. name = each.key tags = { Name = \u0026#34;developer_group_user\u0026#34; } } // 2. 내장 함수 // 접두사 제거로 결과 도메인 https://의 뒷 문자열을 가져와 oidc 구성에 사용됩니다. oidc = trimprefix(\u0026#34;${aws_eks_cluster.cluster.identity[0].oidc[0].issuer}\u0026#34;, \u0026#34;https://\u0026#34;) 1.2.3 프로비저너(provisioner) 변수나 함수인 경우 프로그래밍 코드에서의 변수와 함수처럼 직관적입니다. 반면, 인프라 프로비저닝 이후 로컬 머신에서 동작하는 과정은 익숙치가 않아서 인지 이해 하기가 쉽지 않아 따로 정리합니다. 프로비저너(provisioner)는 특정 리소스가 생성된 후에 추가적인 설정이나 작업을 자동화하기 위해 사용됩니다. 모듈 예제에서는 EC2 비밀 키 등록과 노드 그룹의 iam 정책 등록으로 사용되었습니다.\nEC2 비밀 키 생성 및 등록\n${self.private_key_pem} 변수가 tls_private_key.key로 생성된 키로 입력됩니다. 하시코프에서 제공하는 tls 리소스로 생성한 변수가 프로비저너를 통해 바로 사용되는 것을 인지해야 합니다. tls 모듈 정보는 테라폼 문서에서 확인이 가능합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // ssh 접속 키를 생성 resource \u0026#34;tls_private_key\u0026#34; \u0026#34;key\u0026#34; { algorithm = \u0026#34;RSA\u0026#34; provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo \u0026#39;${self.private_key_pem}\u0026#39; \u0026gt; ./ec2-${var.env}-${var.pjt}-bastion1.pem\u0026#34; } } resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;keypair\u0026#34; { key_name = \u0026#34;${var.pjt}-${var.env}-key\u0026#34; public_key = tls_private_key.key.public_key_openssh } EKS addon 쿠버네티스 리소스 생성\n프로비저너의 또 다른 사용 예로 EKS addon 사용에 필요한 쿠버네티스 오브젝트 생성에 사용됩니다. 이는 EKS 생성 이후의 role-arn 등록 및 EKS 인프라 정보가 필요하기 때문입니다. 오브젝트 생성 구성은 동일하게 진행되며 이해를 돕기위해 Secondary CIDR 의 등록 과정을 공유하겠습니다.\n테라폼으로 addon 을 구성 했으나, 구성 환경에 따라 addon 경우의 수가 많기에 addon 부분도 모듈화를 하면 좋을 것 같습니다.\n2. 테라폼 클라우드로 EKS 프로비저닝 Terraform Cloud는 팀이 Terraform을 공동으로 사용할 수 있게 도와주는 관리형 서비스로, 일관된 환경에서 Terraform 작업을 실행하고 공유 상태 및 비밀 데이터에 쉽게 접근할 수 있습니다. 무료로 제공되는 기본 버전은 버전 관리, 변수 공유, 안정적인 원격 환경에서의 Terraform 실행, 원격 상태 저장을 지원하며, 유료 버전에서는 사용자 수와 권한 설정 등이 확장됩니다. (공식문서 참고)\nhttps://www.youtube.com/watch?v=SFHR3S-Znrw\u0026amp;t=270s\n예제 EKS 모듈을 테라폼 클라우드로 프로비저닝하겠습니다. 테라폼 클라우드에서 EKS를 프로비저닝하기 위해서는 추가 작업이 필요합니다.\n2.1 Provider 설정 먼저 Cloud에서 사용할 조직과 워크스페이스를 설정해야 합니다. 루트 모듈 main.tf에서 옵션을 확인하여 설정해주세요.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 terraform { cloud { organization = \u0026#34;t101-hanhorang\u0026#34; // organization 설정 hostname = \u0026#34;app.terraform.io\u0026#34; // terraform cloud 기본 주소 (변경하지 않는 값입니다.) workspaces { // 워크스페이스 설정 name = \u0026#34;t101\u0026#34; } } required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;\u0026gt;= 3.0.0, \u0026lt; 4.0.0\u0026#34; } } } 설정한 조직과 워크스페이스는 테라폼 클라우드 UI에서 확인할 수 있습니다.\n2.2 VSC 연동 테라폼 클라우드에서는 VCS 연동 기능을 지원합니다. VCS 연동 기능을 통해 깃허브에서 TF 코드를 관리할 수 있게 됩니다. 설정 방법은 간단합니다. 새로운 워크스페이스를 생성하여 단계별로 설정하면 됩니다.\nEKS 모듈화 참고 레파지토리를 확인하면 브랜치가 2개입니다. 레파지토리를 fork한 다음 브랜치를 moulraity 로 설정해주세요. 필자의 경우 rebase로 modularity 브랜치를 Main 브랜치로 병합했습니다.\n1 2 3 4 5 git checkout modulaty git rebase main git checkout main git merge modulaty git push origin main 마지막 단계의 Advanced options 에서 VCS 연동 관련 기능을 설정할 수 있습니다. Apply Method 에서는 terraform apply 여부를 설정할 수 있고, VCS Triggers 에서는 git 브랜치 변경에 따라 트리거할 수 있는 옵션을 설정할 수 있습니다. 필자의 경우 깃허브에서 브랜치 변동시 자동으로 terraform plan 까지만 확인할 수 있도록 설정하였습니다.\n옵션 설정 이후, TF 변수 설정도 진행 해주세요.\n2.3 TF 연동 예제 레파지토리를 연동하면 다음의 에러가 발생합니다. 가져온 레파지토리에 수정 작업이 필요합니다.\nEC2 이미지 data 값 수정 : 예제에서는 ucmp 라는 관리자 계정에서 이미지를 관리하지만, 필자의 경우 aws 에서 관리하는 이미지를 가져올 수 있도록 수정하였습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // main.tf data \u0026#34;aws_ami\u0026#34; \u0026#34;node_ami_id\u0026#34; { count = var.enable_eks ? 1 : 0 provider = aws.ucmp_owner most_recent = true owners = [\u0026#34;amazon\u0026#34;] filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;amzn2-ami-hvm*\u0026#34;] } } 사용하지 않는 output 값 제거 : 예제 프로젝트에서는 ecr를 사용하지 않으며, 오직 EKS만 배포할 예정입니다. 해당 옵션에 맞게 모듈을 수정해야합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // variable.tf ##################### ## eks-node ##################### variable \u0026#34;enable_eks\u0026#34; { type = bool default = true description = \u0026#34;EKS 활성/비활성\u0026#34; } .. .. // output.tf ~~output \u0026#34;ECR_REPOSITORY\u0026#34; { value = aws_ecr_repository.ecr[0].name description = \u0026#34;ECR repository\u0026#34; }~~ output \u0026#34;private_key\u0026#34; { value = nonsensitive(tls_private_key.key.private_key_pem) description = \u0026#34;테라폼으로 생섣된 SSH 접속용 키값\u0026#34; } output \u0026#34;kubeconfig\u0026#34; { value = module.eks.kubeconfig description = \u0026#34;kube-config 내용\u0026#34; } 코드 수정 이후 깃허브로 브랜치를 푸쉬하면 트리거를 통해 테라폼 클라우드에서 자동으로 검사를 진행합니다.\n테라폼 클라우드에서 확인하면 plan 결과를 확인할 수 있습니다. Plan 이후 프로비저닝 작업은 화면 아래의 기능을 통해 진행하면 됩니다.\n마치며 테라폼 클라우드 기능 엄청나네요. 코드로 인프라를 관리한다면 도입을 추천드립니다. 책 저자님의 예제 모듈 TF 코드도 예술 작품을 보는 느낌이였습니다. 코드 공유해주셔서 감사합니다. 역량 발전에 큰 도움이 되었습니다.\n","date":"Sep 06","permalink":"https://HanHoRang31.github.io/post/t101-code-cloud/","tags":["T101","EKS","cloud","AWS","Terraform","Terraform Cloud"],"title":"[T1013] 코드로 보는 테라폼 문법과 테라폼 클라우드 프로비저닝"},{"categories":null,"contents":" 1 2 3 T101 3기(=Terraform 101 Study)는 Terraform 실무 실습 스터디입니다. CloudNet@ 유형욱, 윤서율님이 진행하시며, 책 \u0026#34;테라폼으로 시작하는 IaC\u0026#34;을 기반으로 진행하고 있습니다. 들어가기 전 이번 스터디 활동은 T101 3기다. 6주동안 Terraform과 관련된 실무 예제와 노하우에 대해 정리하여 공유할 예정이다. 1주차에는 IaC에 대한 기본 내용과 테라폼 소개, 문법를 스터디하였다. 이에 대한 내용을 정리할까 했지만, 앞 선 기수에 참여하신 분들의 블로그 글이 너무나 잘 되어 있어 다른 방향으로 작성할까 한다. 다른 방향은 평소 필자가 궁금했던 내용, 여러 세미나에서 주워들은 내용들에 대한 실습이 될 것 같다.\n먼저 포스트할 글의 주제는 테라폼 코드 Import와 멀티클러스터 마이그레이션에 대해 작성할 것이다. 이 두 개의 주제는 이어지는 시나리오일 듯 싶다. 가상의 목표로 설명하자면, 무중단 서비스 제공을 위해 운영 중인 AWS 리소스들에 대해 테라폼으로 전환한 후(IaC)한 후 멀티클러스터 방법으로 마이그레이션하겠다.\n(Option 1 참고)\nhttps://aws.amazon.com/ko/blogs/containers/onfidos-journey-to-a-multi-cluster-amazon-eks-architecture/\nTerraform Import Terraform Import은 기존 AWS 클라우드 리소스를 테라폼 코드로 변환하는 작업이다. 이는 클라우드를 서비스를 수동으로 관리하는 대신 코드로 인프라를 관리한다는 의미이다. 코드로 인프라를 관리하게 되면 얻을 수 있는 장점은 다음과 같다. (책 참고)\n속도와 효율성 : 자동화된 시스템은 인간의 실수를 줄이고, 보다 효율적인 리소스 관리를 가능하게 하여 비용 절감 및 생산성을 높일 수 있다. 또한, 코드로 변경하기에 기존 방식보다 변경 속도가 빠르다. 버전 관리 : 코드로 인프라를 정의하면 Git과 같은 버전 관리 시스템을 통해 변경 내역을 추적하고 롤백할 수 있다. 협업 :파일 형태로 되어 있어 쉽게 공유할 수 있고, 버전 관리 툴과 연계하여 공동 작업을 위한 환경을 만들 수 있다. 재사용성 : 코드로 되어 있기 때문에 다른 프로젝트나 환경에서 쉽게 재사용할 수 있다. 기술의 자산화: 관리 노하우와 작업 방식이 코드에 녹아 있고, 파이프라인에 통합해 자산화되어 기술 부채를 제거할 수 있다. Terraform Import and Tools Terraform CLI로 Import를 할 수 있지만, 리소스 별로 매칭시켜야 하는 단점이 있다. 깃허브 Terraform awesome-terraform 를 확인하면 테라폼에 대한 여러 팁과 기술들을 확인할 수 있다. 이 중 AWS 리소스 관련 Import 도구가 존재한다.\n[Import Tools]\nterraformer : 기존 인프라에서 Terraform 파일을 생성하는 CLI 도구이다. (Star 10.7K) former2 : AWS 계정 내의 기존 리소스에서 Terraform 구성을 생성한다.(Star 1.9k) terracognita - 기존 클라우드 제공자(역 Terraform)에서 읽고 Terraform 구성에서 인프라를 코드로 생성한다. (Star 1.8k) aws2tf : 기존 AWS 리소스를 Terraform으로 가져오는 작업을 자동화하고 Terraform HCL 코드를 출력합니다. (Star 295) 도구들 중 가장 Star가 많은 terrafomer를 선택하여 EKS를 TF code로 변환할 것이다.\nAWS 리소스 배포 테라폼 추출 리소스 대상으로 EKS를 선택하였다. EKS는 AEWS 스터디에서 공유해주신 원 클릭 파일(cloudformation, eksctl) 로 구성하였다. 참고 내용과 구성 방법은 필자의 블로그 글을 확인하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 구성 파일 다운로드 curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/eks-oneclick5.yam # 배포 \u0026lt;*\u0026gt; 로 표시된 매개변수에 값을 입력하자. aws cloudformation deploy --template-file eks-oneclick5.yaml --stack-name myeks --parameter-overrides KeyName=\u0026lt;iam Key name\u0026gt; MyIamUserAccessKeyID=\u0026lt;AWS Access Key\u0026gt; MyIamUserSecretAccessKey=\u0026lt;AWS Secret Key\u0026gt; ClusterBaseName=\u0026lt;cluster name\u0026gt; --region ap-northeast-2 # 베스천 서버 접속 ssh -i \u0026lt;key-file-name\u0026gt;.pem ec2-user@$(aws cloudformation describe-stacks --stack-name myeks --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text) --- The authenticity of host \u0026#39;43.201.6.136 (43.201.6.136)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:Ej5/5MfPW4sRGuQvwAcVkC609QUJR4nXjfBF64FEsDY. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes # 클러스터 확인, 배포 약 20분 소요 kubectl get pods -A -- NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-9sjp9 2/2 Running 0 76m kube-system aws-node-wglbd 2/2 Running 0 76m kube-system coredns-b65b7888f-st5rb 1/1 Running 0 73m kube-system coredns-b65b7888f-zmh5s 1/1 Running 0 73m kube-system ebs-csi-controller-cb695654d-d2blq 6/6 Running 0 72m kube-system ebs-csi-controller-cb695654d-fh9qz 6/6 Running 0 72m kube-system ebs-csi-node-2689h 3/3 Running 0 72m kube-system ebs-csi-node-xwhh9 3/3 Running 0 72m kube-system kube-proxy-7mlxg 1/1 Running 0 74m kube-system kube-proxy-nr6xs 1/1 Running 0 74m Terraformer 를 통한 EKS Import 이어서 MAC환경에서 terraformer 를 설치하고 EKS를 Import 하겠다. 설치 방법은 다음과 같다.\n1 2 3 4 5 6 7 # terraformer 설치 brew install terraformer # 테스트 진행 terraformer import aws --resources=eks --- 2023/08/31 17:21:36 aws importing default region 2023/08/31 17:21:36 open /Users/mzc02-hseungho/.terraform.d/plugins/darwin_amd64: no such file or directory 설치 이후 Import 시 프로바이더가 없어 에러가 발생한다. 아래 시스템 아키텍처를 참고하여 프로바이더를 설치하자. 프로바이더는 hashicorp release에서 확인할 수 있다.\n1 2 3 4 5 # darwin_amd64 환경 aws 설치 mkdir -p ~/.terraform.d/plugins/darwin_amd64 cd ~/.terraform.d/plugins/darwin_amd64 curl -OL https://releases.hashicorp.com/terraform-provider-aws/5.14.0/terraform-provider-aws_5.14.0_darwin_amd64.zip unzip terraform-provider-aws_5.14.0_darwin_amd64.zip 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 terraformer import aws --resources=eks -- 2023/08/31 17:30:22 aws importing default region 2023/08/31 17:30:27 aws importing... eks 2023/08/31 17:30:27 aws done importing eks 2023/08/31 17:30:27 Number of resources for service eks: 2 2023/08/31 17:30:27 Refreshing state... aws_eks_cluster.tfer--hanhorang 2023/08/31 17:30:27 Refreshing state... aws_eks_node_group.tfer--ng1 2023/08/31 17:30:27 Filtered number of resources for service eks: 2 2023/08/31 17:30:27 aws Connecting.... 2023/08/31 17:30:27 aws save eks 2023/08/31 17:30:27 aws save tfstate for eks tree . --- . ├── eks_cluster.tf ├── eks_node_group.tf ├── outputs.tf ├── provider.tf └── terraform.tfstate eks만 tf로 변환되었다. 추가로, EKS와 연관된 VPC, nat gateway 와 베스천 서버도 변환이 필요하다. 깃허브 문서를 참고하면 --filter으로 태그에 설정된 리소스만 가져오거나, --exclude 전체 리소스를 가져오는 데 특정 리소스만 빼서 가져올 수 있다. 이를 조합하여 EKS가 배포된 VPC에 포함된 리소스를 가져오자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # terraformer import aws --resources=vpc,subnet,route_table,igw,sg,nat,eks,ec2 --filter=vpc=\u0026lt;vpc-id\u0026gt; --path-pattern=\u0026#34;{output}/\u0026#34; terraformer import aws --resources=vpc,subnet,route_table,igw,sg,nat,eks,ec2 --filter=vpc=vpc-080c3cee56b75cd1d --path-pattern=\u0026#34;{output}/\u0026#34; --- .. 2023/08/31 18:04:59 Filtered number of resources for service route_table: 14 2023/08/31 18:04:59 Filtered number of resources for service igw: 2 2023/08/31 18:04:59 Filtered number of resources for service sg: 14 2023/08/31 18:04:59 Filtered number of resources for service nat: 1 2023/08/31 18:04:59 Filtered number of resources for service eks: 2 2023/08/31 18:04:59 Filtered number of resources for service vpc: 1 2023/08/31 18:04:59 Filtered number of resources for service subnet: 10 2023/08/31 18:04:59 aws Connecting.... 2023/08/31 18:04:59 aws save 2023/08/31 18:04:59 aws save tfstate tree ./generated --- . ├── eks_cluster.tf ├── eks_node_group.tf ├── internet_gateway.tf ├── main_route_table_association.tf ├── nat_gateway.tf ├── outputs.tf ├── provider.tf ├── route_table.tf ├── route_table_association.tf ├── security_group.tf ├── security_group_rule.tf ├── subnet.tf ├── terraform.tfstate ├── variables.tf └── vpc.tf 가져온 TF code를 기반으로 인프라 구성시 에러가 발생한다. 에러를 참고하니 provider.tf에 aws config 설정 추가와 프로바이더 교체가 필요하다. (공유해주신 tei님 감사합니다. 참고 블로그 글)\nProvider.tf 에 shared_config_files, shared_credentials_files 옵션 추가 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # provider.aws 에 내용 추가 provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; shared_config_files = [\u0026#34;~/.aws/config\u0026#34;] shared_credentials_files = [\u0026#34;~/.aws/credentials\u0026#34;] } terraform { required_providers { aws = { version = \u0026#34;~\u0026gt; 5.14.0\u0026#34; } } } 프로바이더 교체 1 2 3 4 5 6 7 cd generated \u0026amp;\u0026amp; terraform state replace-provider -auto-approve -- -/aws hashicorp/aws --- Terraform will perform the following actions: ~ Updating provider: - registry.terraform.io/-/aws + registry.terraform.io/hashicorp/aws 프로바이더 수정 이후 프로바이더 초기화와 코드로 인프라를 배포를 진행하자. 필자의 경우 plan 으로 영향도 확인시 서브넷과 VPC에 추가 설정이 필요했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Terraform init -- provider downloading.. Terraform plan -- ╷ │ Error: enable_lni_at_device_index must not be zero, got 0 │ │ with aws_subnet.tfer--subnet-015058ac61e07391c, │ on subnet.tf line 5, in resource \u0026#34;aws_subnet\u0026#34; \u0026#34;tfer--subnet-015058ac61e07391c\u0026#34;: │ 5: enable_lni_at_device_index = \u0026#34;0\u0026#34; │ │ Error: Missing required argument │ │ with aws_subnet.tfer--subnet-015058ac61e07391c, │ on subnet.tf line 9, in resource \u0026#34;aws_subnet\u0026#34; \u0026#34;tfer--subnet-015058ac61e07391c\u0026#34;: │ 9: map_customer_owned_ip_on_launch = \u0026#34;false\u0026#34; │ │ \u0026#34;map_customer_owned_ip_on_launch\u0026#34;: all of `customer_owned_ipv4_pool,map_customer_owned_ip_on_launch,outpost_arn` must be specified ╵ . . ╷ │ Error: Missing required argument │ │ with aws_vpc.tfer--vpc-087e1e27e159bf626, │ on vpc.tf line 8, in resource \u0026#34;aws_vpc\u0026#34; \u0026#34;tfer--vpc-087e1e27e159bf626\u0026#34;: │ 8: ipv6_netmask_length = \u0026#34;0\u0026#34; │ │ \u0026#34;ipv6_netmask_length\u0026#34;: all of `ipv6_ipam_pool_id,ipv6_netmask_length` must be specified ╵ 확인이 필요한 부분은 3가지다. terrafrom registry 에 옵션을 참고하자.\n• [enable_lni_at_device_index](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/subnet#enable_lni_at_device_index) - (Optional) Indicates the device position for local network interfaces in this subnet. For example, 1 indicates local network interfaces in this subnet are the secondary network interface (eth1). A local network interface cannot be the primary network interface (eth0).\n• [map_customer_owned_ip_on_launch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/subnet#map_customer_owned_ip_on_launch) - (Optional) Specify true to indicate that network interfaces created in the subnet should be assigned a customer owned IP address. The customer_owned_ipv4_pool and outpost_arn arguments must be specified when set to true. Default is false.\n• [ipv6_netmask_length](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc#ipv6_netmask_length) - (Optional) Netmask length to request from IPAM Pool. Conflicts with ipv6_cidr_block. This can be omitted if IPAM pool as a allocation_default_netmask_length set. Valid values: 56.\n옵션 확인시 구성에 필요없는 옵션이기에 해당 옵션을 삭제했다. 다만, 삭제 후에도 몇 가지 계획 확인 과정에서 필요없는 옵션으로 에러가 발생하여 삭제를 진행하였다. 원인은 버전 호환성으로 생각된다. 영향도 확인시 13개의 변동사항이 발생하는데 모두 security group에서 발생하였다. 플래그 및 옵션 변경으로 확인된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # 계획 확인 terraform plan -- # aws_security_group_rule.tfer--sg-00be73ec863ca7943_ingress_-1_-1_-1_sg-0435a44214aa33d8a must be replaced -/+ resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;tfer--sg-00be73ec863ca7943_ingress_-1_-1_-1_sg-0435a44214aa33d8a\u0026#34; { ~ id = \u0026#34;sgrule-1987943007\u0026#34; -\u0026gt; (known after apply) - prefix_list_ids = [] -\u0026gt; null ~ security_group_rule_id = \u0026#34;sgr-0466aed048c68dbd0\u0026#34; -\u0026gt; (known after apply) + self = false # forces replacement # (7 unchanged attributes hidden) - timeouts {} } . Plan: 2 to add, 11 to change, 2 to destroy. Changes to Outputs: ~ aws_security_group_rule_tfer--sg-00be73ec863ca7943_egress_-1_-1_-1_0-002E-0-002E-0-002E-0-002F-0_id = \u0026#34;sgrule-3558759526\u0026#34; -\u0026gt; (known after apply) ~ aws_security_group_rule_tfer--sg-00be73ec863ca7943_ingress_-1_-1_-1_sg-0435a44214aa33d8a_id = \u0026#34;sgrule-1987943007\u0026#34; -\u0026gt; (known after apply) # 코드 인프라 배포 terraform apply -- pply complete! Resources: 2 added, 11 changed, 2 destroyed. Outputs: aws_eks_cluster_tfer--hanhorang_id = \u0026#34;hanhorang\u0026#34; aws_eks_node_group_tfer--ng1_id = \u0026#34;hanhorang:ng1\u0026#34; aws_internet_gateway_tfer--igw-01ca08858c082a0ae_id = \u0026#34;igw-01ca08858c082a0ae\u0026#34; aws_internet_gateway_tfer--igw-09 ... # 베스천 서버 내 동작 확인 kubectl get pods -A --- NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-9sjp9 2/2 Running 0 9h kube-system aws-node-wglbd 2/2 Running 0 9h kube-system coredns-b65b7888f-st5rb 1/1 Running 0 9h kube-system coredns-b65b7888f-zmh5s 1/1 Running 0 9h kube-system ebs-csi-controller-cb695654d-d2blq 6/6 Running 0 8h kube-system ebs-csi-controller-cb695654d-fh9qz 6/6 Running 0 8h kube-system ebs-csi-node-2689h 3/3 Running 0 8h kube-system ebs-csi-node-xwhh9 3/3 Running 0 8h kube-system kube-proxy-7mlxg 1/1 Running 0 9h kube-system kube-proxy-nr6xs 1/1 Running 0 9h 멀티 클러스터 업그레이드 기존 운영 중인 클러스터를 TF코드로 가져왔으니 TF 코드를 통해 새로운 클러스터를 구축하고 트래픽을 조정하여 인프라를 변경하겠다.\n(Option 1)\nhttps://aws.amazon.com/ko/blogs/containers/onfidos-journey-to-a-multi-cluster-amazon-eks-architecture/\n선수 작업 멀티클러스터 업그레이드를 위한 사전 작업으로 운영 중인 클러스터에 네트워크 addon(external DNS, ALB z컨트롤러)와 예제를 배포하자. 여기서 ALB 컨트롤러와 예제는 새로운 클러스터에서 다시 배포할 것이다.\n네트워크 addon 설치는 필자의 포스트 글 참조해주세요. 예제 애플리케이션 배포 (vote App) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # git clone git clone https://github.com/HanHoRang31/blog-share.git cd blog-share/k8s-app/vote-app-alb # 서비스 배포 kubectl apply -f . # ExternaDNS 추가 ## 각자 자신의 도메인 정보 입력 # MyDOMAIN1=\u0026lt;각자 자신의 nginx 도메인 지정\u0026gt; # MyDOMAIN2=\u0026lt;각자 자신의 nginx 도메인 지정\u0026gt; MyDOMAIN1=vote.hanhorang.link MyDOMAIN2=result.hanhorang.link kubectl annotate ingress vote-ingress \u0026#34;external-dns.alpha.kubernetes.io/hostname=$MyDOMAIN1.\u0026#34; kubectl annotate ingress result-ingress \u0026#34;external-dns.alpha.kubernetes.io/hostname=$MyDOMAIN2.\u0026#34; 설정한 도메인에 접속하면 예제 애플리케이션을 확인할 수 있다.\n새 클러스터 구성 새로운 클러스터 배포는 TF code 에서 아래 내용을 수정하면 된다. 변수 네이밍과 태그, 그리고 클러스터 버전을 확인해서 업그레이드하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 ## eks_cluster-blue.tf resource \u0026#34;aws_eks_cluster\u0026#34; \u0026#34;tfer--hanhorang2\u0026#34; { # 수정 kubernetes_network_config { ip_family = \u0026#34;ipv4\u0026#34; service_ipv4_cidr = \u0026#34;10.100.0.0/16\u0026#34; } name = \u0026#34;hanhorang2\u0026#34; # 수정 role_arn = \u0026#34;arn:aws:iam::955963799952:role/eksctl-hanhorang-cluster-ServiceRole-BP23R8UEH9QW\u0026#34; tags = { # 수정 Name = \u0026#34;eksctl-hanhorang-cluster/ControlPlane\u0026#34; \u0026#34;alpha.eksctl.io/cluster-name\u0026#34; = \u0026#34;hanhorang2\u0026#34; \u0026#34;alpha.eksctl.io/cluster-oidc-enabled\u0026#34; = \u0026#34;true\u0026#34; \u0026#34;alpha.eksctl.io/eksctl-version\u0026#34; = \u0026#34;0.154.0\u0026#34; \u0026#34;eksctl.cluster.k8s.io/v1alpha1/cluster-name\u0026#34; = \u0026#34;hanhorang2\u0026#34; } tags_all = { Name = \u0026#34;eksctl-hanhorang-cluster/ControlPlane\u0026#34; \u0026#34;alpha.eksctl.io/cluster-name\u0026#34; = \u0026#34;hanhorang2\u0026#34; \u0026#34;alpha.eksctl.io/cluster-oidc-enabled\u0026#34; = \u0026#34;true\u0026#34; \u0026#34;alpha.eksctl.io/eksctl-version\u0026#34; = \u0026#34;0.154.0\u0026#34; \u0026#34;eksctl.cluster.k8s.io/v1alpha1/cluster-name\u0026#34; = \u0026#34;hanhorang2\u0026#34; } version = \u0026#34;1.25\u0026#34; # 업그레이드 vpc_config { endpoint_private_access = \u0026#34;false\u0026#34; endpoint_public_access = \u0026#34;true\u0026#34; public_access_cidrs = [\u0026#34;0.0.0.0/0\u0026#34;] security_group_ids = [\u0026#34;${data.terraform_remote_state.local.outputs.aws_security_group_tfer--eksctl-hanhorang-cluster-ControlPlaneSecurityGroup-186VO6YUM9A43_sg-0270c4da847aa9121_id}\u0026#34;] subnet_ids = [\u0026#34;${data.terraform_remote_state.local.outputs.aws_subnet_tfer--subnet-015058ac61e07391c_id}\u0026#34;, \u0026#34;${data.terraform_remote_state.local.outputs.aws_subnet_tfer--subnet-0590fd1917b053a6a_id}\u0026#34;, \u0026#34;${data.terraform_remote_state.local.outputs.aws_subnet_tfer--subnet-0eb54f96064d4e96c_id}\u0026#34;] } } # eks_node_group-blue.tf resource \u0026#34;aws_eks_node_group\u0026#34; \u0026#34;tfer--ng2\u0026#34; { # 수정 ami_type = \u0026#34;AL2_x86_64\u0026#34; capacity_type = \u0026#34;ON_DEMAND\u0026#34; cluster_name = \u0026#34;${aws_eks_cluster.tfer--hanhorang2.name}\u0026#34; disk_size = \u0026#34;0\u0026#34; instance_types = [\u0026#34;t3.medium\u0026#34;] # 수정 labels = { \u0026#34;alpha.eksctl.io/cluster-name\u0026#34; = \u0026#34;hanhorang2\u0026#34; \u0026#34;alpha.eksctl.io/nodegroup-name\u0026#34; = \u0026#34;ng1\u0026#34; } node_group_name = \u0026#34;ng1\u0026#34; node_role_arn = \u0026#34;arn:aws:iam::955963799952:role/eksctl-hanhorang-nodegroup-ng1-NodeInstanceRole-G4237QU98U14\u0026#34; scaling_config { desired_size = \u0026#34;2\u0026#34; max_size = \u0026#34;2\u0026#34; min_size = \u0026#34;2\u0026#34; } subnet_ids = [\u0026#34;subnet-015058ac61e07391c\u0026#34;, \u0026#34;subnet-0590fd1917b053a6a\u0026#34;, \u0026#34;subnet-0eb54f96064d4e96c\u0026#34;] # 수정 tags = { \u0026#34;alpha.eksctl.io/cluster-name\u0026#34; = \u0026#34;hanhorang2\u0026#34; \u0026#34;alpha.eksctl.io/eksctl-version\u0026#34; = \u0026#34;0.154.0\u0026#34; \u0026#34;alpha.eksctl.io/nodegroup-name\u0026#34; = \u0026#34;ng1\u0026#34; \u0026#34;alpha.eksctl.io/nodegroup-type\u0026#34; = \u0026#34;managed\u0026#34; \u0026#34;eksctl.cluster.k8s.io/v1alpha1/cluster-name\u0026#34; = \u0026#34;hanhorang2\u0026#34; } tags_all = { \u0026#34;alpha.eksctl.io/cluster-name\u0026#34; = \u0026#34;hanhorang2\u0026#34; \u0026#34;alpha.eksctl.io/eksctl-version\u0026#34; = \u0026#34;0.154.0\u0026#34; \u0026#34;alpha.eksctl.io/nodegroup-name\u0026#34; = \u0026#34;ng1\u0026#34; \u0026#34;alpha.eksctl.io/nodegroup-type\u0026#34; = \u0026#34;managed\u0026#34; \u0026#34;eksctl.cluster.k8s.io/v1alpha1/cluster-name\u0026#34; = \u0026#34;hanhorang2\u0026#34; } update_config { max_unavailable = \u0026#34;1\u0026#34; } # 노드 그룹 업그레이드 version = \u0026#34;1.25\u0026#34; } 새 클러스터 구성 후 베스천 서버에서 새 클러스터에 접근할 수 있도록 구성을 변경하자.\n1 2 3 4 5 6 7 eksctl get cluster --region=ap-northeast-2 -- NAME REGION EKSCTL CREATED hanhorang ap-northeast-2 True hanhorang2 ap-northeast-2 False aws eks update-kubeconfig --region ap-northeast-2 --name hanhorang2 kubectl 로 접근시 새 클러스터 파드에 aws-node가 붙지 않는다. 로그를 확인하면 노드 그룹에 연결된 iam role에 필요 정책이 없어 발생한 에러였다.\nEC2 role에 관리형 정책인 AmazonEKS_CNI_Policy 를 추가하자.\n1 2 3 4 5 6 7 8 9 kubectl get pods -A --- NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-5sn6g 1/1 Running 0 62s kube-system aws-node-s2jwf 1/1 Running 0 79s kube-system coredns-76b4dcc5cc-hp5sb 1/1 Running 0 50m kube-system coredns-76b4dcc5cc-nwg44 1/1 Running 0 50m kube-system kube-proxy-hq2m8 1/1 Running 0 41m kube-system kube-proxy-vl7nv 1/1 Running 0 41m 접근이 확인되면, 선수 작업에 진행한 네트워크 addon 과 예제 애플리케이션을 배포하자.\n1 2 3 4 5 # 새 클러스터 OIDC 활성화 export cluster_name=hanhorang2 eksctl utils associate-iam-oidc-provider --cluster $cluster_name --approve # .. 이후 선수작업의 네트워크 addon 과 예제 애플리케이션 배포 클러스터 마이그레이션 두 개의 다른 클러스터에서 같은 도메인을 등록하여 애플리케이션을 등록하였다. 등록한 어플리케이션에 트래픽 조절을 구 클러스터에서 새 클러스터로 바꾸면 마이그레이션은 완료다. 새 클러스터에서 vote 애플리케이션을 똑같이 배포하면, route53에 등록된 dns가 구 클러스터에서 새 클러스터에서 수정되며 등록되는 시간 약 5분동안 순단이 발생한다. 무중단으로 트래픽을 전환하기 위한 작업이 필요하다.\nRoute53 가중치 라우팅을 통한 트래픽 전환 route53 의 가중치 기반 라우팅을 통해 blue-green 버전 사이의 트래픽을 기호에 맞게 옮길 수 있다. 이를 위해 구 클러스터의 ingress annotation 에 가중치 주석을 추가하자. (기타 라우팅의 경우 external-dns 깃 참고)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # vote-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: vote-ingress annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip external-dns.alpha.kubernetes.io/hostname: hanhorang.link # 사용자 도메인 입력 external-dns.alpha.kubernetes.io/set-identifier: vote # RecordID 등록, 라우팅 식별자 external-dns.alpha.kubernetes.io/aws-weight: \u0026#39;1\u0026#39; # 가중치 기반 설정값 spec: rules: - http: paths: - path: /vote pathType: Prefix backend: service: name: vote port: number: 5000 external-dns.alpha.kubernetes.io/aws-weight : 0~255 의 정수값이 들어가며, 상대적인 비율을 나타낸다. 예를 들어, A 서비스와 B서비스의 트래픽 비율을 1:1로 맞추고 싶다면, A 서비스 값을 1 설정하고 B 서비스의 값을 1로 설정하자. 새 클러스터에서 예제 Ingress 배포를 진행하면 A레코드와,TXT 가 중복되어 route53이 업데이트 되지 않는다. 해결 방법을 찾아보니 기존의 리소스를 삭제하여 중복을 없애라는데.. 무중단으로 진행이 안된다.\n1 2 3 4 5 6 7 time=\u0026#34;2023-09-01T13:46:50Z\u0026#34; level=error msg=\u0026#34;Failure in zone hanhorang.link. [Id: /hostedzone/Z08463751O7YNWD79KKIX] when submitting change batch: InvalidChangeBatch: [RRSet with DNS name hanhorang.link., type A cannot be created as other RRSets exist with the same name and type., RRSet with DNS name hanhorang.link., type TXT cannot be created as other RRSets exist with the same name and type.]\\n\\tstatus code: 400, request id: 64a5953f-902c-4e8c-9bd7-2062a22c8109\u0026#34; time=\u0026#34;2023-09-01T13:46:51Z\u0026#34; level=error msg=\u0026#34;failed to submit all changes for the following zones: [/hostedzone/Z08463751O7YNWD79KKIX]\u0026#34; time=\u0026#34;2023-09-01T13:47:50Z\u0026#34; level=info msg=\u0026#34;Applying provider record filter for domains: [hanhorang.link. .hanhorang.link.]\u0026#34; time=\u0026#34;2023-09-01T13:47:50Z\u0026#34; level=info msg=\u0026#34;Desired change: CREATE hanhorang.link A [Id: /hostedzone/Z08463751O7YNWD79KKIX]\u0026#34; time=\u0026#34;2023-09-01T13:47:50Z\u0026#34; level=info msg=\u0026#34;Desired change: CREATE hanhorang.link TXT [Id: /hostedzone/Z08463751O7YNWD79KKIX]\u0026#34; time=\u0026#34;2023-09-01T13:47:50Z\u0026#34; level=error msg=\u0026#34;Failure in zone hanhorang.link. [Id: /hostedzone/Z08463751O7YNWD79KKIX] when submitting change batch: InvalidChangeBatch: [RRSet with DNS name hanhorang.link., type A cannot be created as other RRSets exist with the same name and type., RRSet with DNS name hanhorang.link., type TXT cannot be created as other RRSets exist with the same name and type.]\\n\\tstatus code: 400, request id: 6f961338-b1bd-4ddd-93ad-1bb169fc686d\u0026#34; time=\u0026#34;2023-09-01T13:47:51Z\u0026#34; level=error msg=\u0026#34;failed to submit all changes for the following zones: [/hostedzone/Z08463751O7YNWD79KKIX]\u0026#34; 해결을 위해서는 클러스터별로 각기 다른 Route53 레코드ID를 갖도록 설정해야 한다.\n1.24 EKS 클러스터(green) 설정\nexternal-dns configure\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl edit deploy external-dns -n kube-system -- .. spec: containers: - args: - --events - --source=service - --source=ingress - --domain-filter=hanhorang.link - --provider=aws - --aws-zone-type=public - --registry=txt - --txt-owner-id=blue # hostzone ID에서 수정 vote-Ingress.yaml\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: vote-ingress annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip external-dns.alpha.kubernetes.io/hostname: vote.hanhorang.link # 도메인 입력 external-dns.alpha.kubernetes.io/set-identifier: green external-dns.alpha.kubernetes.io/aws-weight: \u0026#39;1\u0026#39; 1.25 EKS 클러스터(blue) 설정\nexternal-dns\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl edit deploy external-dns -n kube-system -- .. spec: containers: - args: - --events - --source=service - --source=ingress - --domain-filter=hanhorang.link - --provider=aws - --aws-zone-type=public - --registry=txt - --txt-owner-id=green # hostzone ID에서 수정 vote-Ingress.yaml\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: vote-ingress annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip external-dns.alpha.kubernetes.io/hostname: vote.hanhorang.link # 도메인 입력 external-dns.alpha.kubernetes.io/set-identifier: blue external-dns.alpha.kubernetes.io/aws-weight: \u0026#39;1\u0026#39; 각 클러스터에서 예제 애플리케이션을 배포하면 콘솔에서 같은 도메인의 레코드 정보가 2개씩 생겼음을 확인할 수 있다.\n예제 어플리케이션 접속시 가중치에 맞게 변환되어 호출되는 것을 확인할 수 있다. 확인하면서 주의할 점이 있는데 DNS캐싱으로 한 번 DNS에 연결되면 설정된 TTL 동안 같은 DNS로 연결된다.\n설정한 가중치 라우팅을 1:1 에서 0:1로 수정하여 테스트하면 새 버전의 서버에서만 트래픽을 전달하는 것을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 kubectl edit ingress vote-ingress -- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip external-dns.alpha.kubernetes.io/aws-weight: \u0026#34;0\u0026#34; # 1에서 0으로 수정 external-dns.alpha.kubernetes.io/hostname: vote.hanhorang.link external-dns.alpha.kubernetes.io/set-identifier: green kubectl.kubernetes.io/last-applied-configuration: | 마치며 테라폼과 Route53 라우팅 정책을 통해 무중단 클러스터 마이그레이션 작업을 진행하였다. 내용이 복잡했지만, External DNS 로 Route53 가중치 정책 설정 방법과 Terraformer Import 이후 수정 작업만 기억하면 된다. 추후 실무에서 도입시 기대되는 부분이다.\n","date":"Sep 01","permalink":"https://HanHoRang31.github.io/post/t101-multicluster-migration/","tags":["T101","EKS","cloud","AWS","Terraform","multi-cluster","migration","Terraformer","Route53"],"title":"[T1013] 테라폼을 활용한 EKS 클러스터 마이그레이션"},{"categories":null,"contents":" 1 2 3 T101(=Terraform 101 Study)는 Terraform 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하시며, 책 \u0026#34;Terraform Up \u0026amp; Running\u0026#34;을 기반으로 진행하고 있습니다. 22년 9월 기준 학습 내용입니다. 블로그 이주로 다시 포스트합니다. 지난 글에서는 테라폼으로 EKS Private 클러스터를 구성하였다. 오늘은 EKS 클러스터 구축 이후, 운영적인 부분으로 테라폼으로 EKS 관리를 위한 추가 설정에 대해 소개하고자 한다.\n추가 설정은 총 3가지로 나눠 다룰 예정이다. 테라폼 상태 관리, 상태 격리와 마지막으로 AWS 서비스에 접근하기 위한 보안 설정이다. 테라폼 상태 관리와 상태 격리는 테라폼에서 인프라(EKS) 관리를 위한 기능이다. 구체적으로는 테라폼 상태 파일을 팀 단위에서 관리하기 위한 방법과 모듈 분리에 대한 방법을 소개할 것이다. AWS 서비스에 접근하기 위한 보안 설정은 EKS 의 기능 확장을 위한 사전 작업으로 필요한 과정이다. 기능 확장을 위해서는 Addon 이라는 모듈의 설치가 필요하며 각 Addon 들은 AWS 서비스들 이용하기에 필요한 보안 작업이다.\n마찬가지로 이번 포스트에 대한 코드들은 필자의 깃허브 레파지토리에서 확인이 가능하다.\n상태 관리 테라폼에서 선언한 상태 정보들은 실행 디렉토리 내 terraform.tfstate 파일에 저장된다. terraform.tfstate 파일은 테라폼 내 프라이빗 API 로 오직 테라폼 내부에서만 사용이 가능하며 직접 편집하거나 직접 읽는 코드로 작성해서는 안된다.\n이처럼 테라폼에서는 tfstate 파일을 통해 상태를 관리하지만, 팀 단위의 버전 관리시 추가 작업이 필요하다. 예를 들면, 팀 단위로 테라폼을 사용했을 시 관리자마다 테라폼에 대한 상태파일이 독립적으로 생성되어 인프라 유지에 어려움이 있을 것이다.\n이에 대한 해결책으로 원격 상태 스토리지를 이용한다. 원격 상태 스토리지를 이용하면 테라폼 상태 파일이 원격 스토리지에 저장되며 관리자들은 동일한 스토리지에서 상태 파일을 바라보기 때문에 버전 관리가 가능해진다.\n기본 상태 파일과 원격 상태 파일에 대한 비교 (출처.StakSimplify)\n상태 관리 구현\n상태 관리는 테라폼 내 기능인 Backend 을 통해 구현이 가능하다.Backend 는 상태 파일을 저장하고 동시 수정을 차단(locking)할 수 있는 기능을 제공한다. 필자 또한 해당 기능을 사용하여 상태 파일을 원격 스토리지에 저장시키도록 설정하였다. 원격 스토리지는 AWS에서 제공하는 객체 스토리지 서비스인 S3와 동시 수정을 제어할 수 있는 DynamoDB를 사용하였다. Backend 기능을 사용하기 전 원격 스토리지인 S3, DynamoDB 구축이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # 00-ekscluster-remote-stoarge/c0-remote-storage.tf provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;t101-remote-bucket\u0026#34; { bucket = \u0026lt;Bucket_NAME\u0026gt; } # Enable versioning so you can see the full revision history of your state files resource \u0026#34;aws_s3_bucket_versioning\u0026#34; \u0026#34;t101-remote-bucket_versioning\u0026#34; { bucket = aws_s3_bucket.t101-remote-bucket.id versioning_configuration { status = \u0026#34;Enabled\u0026#34; } } resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;t101-remote-dynamodbtable\u0026#34; { name = \u0026#34;terraform-locks\u0026#34; billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;LockID\u0026#34; attribute { name = \u0026#34;LockID\u0026#34; type = \u0026#34;S\u0026#34; } } output \u0026#34;s3_bucket_arn\u0026#34; { value = aws_s3_bucket.t101-remote-bucket.arn description = \u0026#34;The ARN of the S3 bucket\u0026#34; } output \u0026#34;dynamodb_table_name\u0026#34; { value = aws_dynamodb_table.t101-remote-dynamodbtable.name description = \u0026#34;The name of the DynamoDB table\u0026#34; } Backend 사용 방법은 간단하다. 위에서 생성한 S3 버켓과 DynamoDB 이름을 작성하면 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # # Terraform Settings Block terraform { required_version = \u0026#34;\u0026gt;= 1.0.0\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 3.70\u0026#34; } } # Adding Backend as S3 for Remote State Storage backend \u0026#34;s3\u0026#34; { bucket = \u0026lt;Bucket_NAME\u0026gt; key = \u0026#34;t101/eks-cluster/terraform.tfstate\u0026#34; region = \u0026#34;ap-northeast-2\u0026#34; # For State Locking dynamodb_table = \u0026#34;t101-remote-dynamodbtable\u0026#34; } } # Terraform Provider Block provider \u0026#34;aws\u0026#34; { region = var.aws_region } 상태 격리 상태 파일 격리을 이용하면 이미 구축한 인프라 정보들을 기반으로 모듈을 분리시킬 수 있다. 이는 유지 보수성과 관리측면에서 정말 유용한 기능이다. 만약 하나의 상태 파일에서 모든 인프라 구성을 관리하게 된다면 코드에 대한 유지보수성이 떨어지며, 인프라 수정시 전체 인프라에 대한 구성 정보를 확인하게 되어 테라폼 실행 속도가 현저히 늦어질 것이다.\n이번 글에서도 상태 파일을 이용하여 모듈을 분리하여 애드온을 설치한 내용들을 소개할 예정이다. 구체적으로는 지난 시간에 구축한 EKS 클러스터에 대해 독립적으로 테라폼 상태 파일을 구성하여 애드온을 설치할 예정이다. 밑의 그림은 상태 격리를 통한 모듈 분리 방법을 도식화한 그림이다. 상태 격리의 핵심은 상태 파일을 읽어오는 것으로 상태 관리에서 원격 스토리지에 저장한 상태파일을 읽어오는 것으로 이해하면 된다.\n테라폼 구축과 addon 설치에 대한 모듈 분리\n상태 파일 격리 구현\n상태 파일 격리 구현은 모듈별 실행 디렉토리 분리하는 것으로 시작한다. 모듈 분리로 필자에 대한 디렉터리을 일부 확인하면 지난 시간에 구축한 EKS 클러스터 내용을 확장하였다. 또한, 앞서 구성한 원격 스토리지 저장에 대한 모듈(00-ekscluster-remote-stoarge)을 분리하였고 바로 뒤에 소개할 보안 모듈(02-eks-irsa-terraform-manifests)을 분리하였다.\n모듈 분리 디렉터리\n격리 파일 구현의 핵심은 프로바이더를 통한 상태 읽기다. 원격 스토리지에 저장한 EKS 상태 파일을 읽는 방법으로 테라폼 Kubernetes 프로바이더와 Helm 프로바이더를 사용하였다. 아래 코드처럼 data를 선언하여 EKS 클러스터의 상태 파일이 저장된 파일들을 불러온다.\n1 2 3 4 5 6 7 8 # Terraform Remote State Datasource - Remote Backend AWS S3 data \u0026#34;terraform_remote_state\u0026#34; \u0026#34;eks\u0026#34; { backend = \u0026#34;s3\u0026#34; config = { bucket = \u0026lt;Bucket_NAME\u0026gt; key = \u0026#34;t101/eks-cluster/terraform.tfstate\u0026#34; region = var.aws_region } 그 다음 과정으로 프로바이더를 이용하여 EKS 클러스터 접근 정보를 등록한다. 접근 등록 이후 프로바이더 내 리소스들을 사용하면 자동으로 EKS 클러스터 내부에 쿠버네티스 리소스가 배포된다.\n1 2 3 4 5 6 # Terraform Kubernetes Provider provider \u0026#34;kubernetes\u0026#34; { host = data.terraform_remote_state.eks.outputs.cluster_endpoint cluster_ca_certificate = base64decode(data.terraform_remote_state.eks.outputs.cluster_certificate_authority_data) token = data.aws_eks_cluster_auth.cluster.token } IRSA (IAM Roles for Service Accounts) EKS addon 서비스를 사용하기 위한 추가 설정으로 AWS 서비스들에 대한 권한을 허용받기 위한 IRSA 설정이 필요하다. 설정 이유는 addon 서비스가 AWS 서비스를 이용하여 DNS, 로드밸런서, 볼륨 등의 기능을 확장하기 때문이다.\nAddon 서비스 사용 AWS 서비스 목적 EBS CSI Controller AWS EBS Volumes 영구 블록 스토리지 볼륨 사용 EFS CSI Controller AWS EFS Volumes 공용 파일 스토리지 볼륨 사용 Load Balancer Controller AWS Elastic Load Balancer(ELB) 네트워크 로드밸런스 사용 External DNS Controller AWS Route53 DNS 사용 표에 보면 알겠지만 EKS addon을 요약하자면, AWS 서비스를 사용한다. 하지만 EKS 내부 클러스터에서 AWS 서비스를 사용하기 위해선 AWS 서비스를 사용할 권한이 필요하다. 예를 들어 EKS 내부 클러스터의 리소스에서 AWS S3를 사용한다고 하면 access denied 으로 S3의 접근이 안될 것이다.\nS3 접근 에러 (출처.StakSimplify)\n따라서, AWS 서비스를 접근하기 위해 IRSA 설정이 필요하다. EKS IRSA은 EKS 클러스터 내 서비스 어카운트가AWS 서비스의 권한을 위임받는 과정이다. 권한 위임은 AWS 자격 증명 서비스인 IAM를 통해 진행되며 권한 위임은 다음과 같이 진행된다.\nIRSA를 통한 AWS 서비스 접근 과정\nPre. AWS 서비스 이용 권한을 가진 IAM Role 를 k8s Service Account에 할당한다.\nAWS 서비스 접근시, k8s Service Account 가 IAM에 보안 자격 증명 토큰(JWT 토큰)을 요청한다. IAM 은 AWS STS를 통해 임시 자격 증명 토큰 발급한다. AWS STS는 발급한 접근 토큰을 k8s Service Account 에 위임한다. 접근 토큰을 가진 k8s Service Account 는 AWS 서비스에 접근이 가능해진다. IRSA 설정\nIRSA 설정하기 위해서 가장 먼저 EKS에 OIDC 자격 증명 프로바이더를 연결해야한다. 여기서 OIDC 자격 프로바이더는 EKS에서 AWS 서비스를 이용할 수 있도록 지원하기 위해 필요로 한다. OIDC 자격 증명 프로바이더 생성과 연결으로 테라폼 AWS 리소스인 aws_iam_openid_connect_provider 를 사용하였다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #01-ekscluster-terraform-manifests/c6-02-iam-oidc-connect-provider.tf data \u0026#34;aws_partition\u0026#34; \u0026#34;current\u0026#34; {} # Resource: AWS IAM Open ID Connect Provider resource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;oidc_provider\u0026#34; { client_id_list = [\u0026#34;sts.${data.aws_partition.current.dns_suffix}\u0026#34;] thumbprint_list = [var.eks_oidc_root_ca_thumbprint] url = aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer # 1 tags = merge( { Name = \u0026#34;${var.cluster_name}-eks-irsa\u0026#34; }, local.common_tags ) data.aws_partition 는 현재 테라폼이 운영 중인 파티션 (지역) 정보를 가져오는 것에 사용된다. resource.aws_iam_openid_connect_provider.client_id_list : 자격 증명(STS) 정보가 입력된다. resource.aws_iam_openid_connect_provider.thumbprint_list : 자격 증명 CA로 기본 할당(2037년까지 유효) 정보를 사용하였다. resource.aws_iam_openid_connect_provider.url : 공급자로 EKS 정보가 입력된다. IRSA 생성 시 AWS 콘솔 - IAM - 자격증명 공급자에서 다음과 같이 확인할 수 있다.\nOIDC 자격 증명 프로바이더 설정이 끝났다. 이제 IAM 정책 생성 및 k8s ServiceAccount 에 할당하면 서비스에 접근이 가능해진다. 앞으로의 addons 서비스들도 마찬가지로 필요한 권한(IAM role)을 가진 정책을 생성하여 ServiceAccount 에 할당할 것이다. 이번 장에서는 예제로 파드에서 AWS S3에 접근하기 위한 데모를 진행하겠다.\n다음의 테라폼 코드는 Iam role, policy 생성하는 코드이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #data.terraform_remote_state.eks.outputs.aws_iam_openid_connect_provider_arn #data.terraform_remote_state.eks.outputs.aws_iam_openid_connect_provider_extract_from_arn # Resource: Create IAM Role and associate the EBS IAM Policy to it resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;irsa_iam_role\u0026#34; { name = \u0026#34;${local.name}-irsa-iam-role\u0026#34; # Terraform\u0026#39;s \u0026#34;jsonencode\u0026#34; function converts a Terraform expression result to valid JSON syntax. assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Action = \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34; Effect = \u0026#34;Allow\u0026#34; Sid = \u0026#34;\u0026#34; Principal = { Federated = \u0026#34;${data.terraform_remote_state.eks.outputs.aws_iam_openid_connect_provider_arn}\u0026#34; } Condition = { StringEquals = { \u0026#34;${data.terraform_remote_state.eks.outputs.aws_iam_openid_connect_provider_extract_from_arn}:sub\u0026#34;: \u0026#34;system:serviceaccount:default:irsa-demo-sa\u0026#34; } } }, ] }) tags = { tag-key = \u0026#34;${local.name}-irsa-iam-role\u0026#34; } } # Associate IAM Role and Policy resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;irsa_iam_role_policy_attach\u0026#34; { policy_arn = \u0026#34;arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\u0026#34; role = aws_iam_role.irsa_iam_role.name } resource.aws_iam_role.irsa_iam_role 는 IRSA 에서 STS 을 이용하기 위해 처음 role 생성시 K8s serviceaccount 에 STS 사용 위임정책을 선언한다. resource.aws_iam_role_policy_attachment.irsa_iam_role_policy_attach 는 S3 서비스를 읽기 위한 정책을 role에 할당한다. IAM 설정이 끝났다면 AWS 서비스를 사용할 K8s 리소스에 생성한 serviceaccount를 할당하면 IRSA 설정이 끝난다. 아래 코드는 S3 버켓을 확인하기 위한 잡 리소스이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Resource: Kubernetes Job resource \u0026#34;kubernetes_job_v1\u0026#34; \u0026#34;irsa_demo\u0026#34; { metadata { name = \u0026#34;irsa-demo\u0026#34; } spec { template { metadata { labels = { app = \u0026#34;irsa-demo\u0026#34; } } spec { service_account_name = kubernetes_service_account_v1.irsa_demo_sa.metadata.0.name container { name = \u0026#34;irsa-demo\u0026#34; image = \u0026#34;amazon/aws-cli:latest\u0026#34; args = [\u0026#34;s3\u0026#34;, \u0026#34;ls\u0026#34;] #args = [\u0026#34;ec2\u0026#34;, \u0026#34;describe-instances\u0026#34;, \u0026#34;--region\u0026#34;, \u0026#34;${var.aws_region}\u0026#34;] # Should fail as we don\u0026#39;t have access to EC2 Describe Instances for IAM Role } restart_policy = \u0026#34;Never\u0026#34; } } } } 쿠버네티스 리소스 선언 파일 상에서는 별도의 IRSA 설정 필요없이 serviceaccount만 할당하면 된다! 사족이지만 테라폼에서 쿠버네티스 리소스 생성시, context deadlin exceeded 에러를 볼 수 있다.\n해당 에러는 테라폼 실행 PC에서 EKS 컨트롤플레인의 API 서버에 접근하지 못할 때 발생한다. 이를 해결하기 위해서는 API 서버의 엔드포인트를 프라이빗으로 열고 테라폼 실행 PC에서만 접근이 가능하도록 설정하면 된다.\nServiceaccount 할당 후 쿠버네티스 잡을 테라폼으로 배포하면 다음과 같이 잡의 로그를 통해 S3의 버켓을 확인할 수 있다.\n끝으로 이것으로 테라폼에서 EKS 관리를 위한 3가지의 과정을 끝냈다! 다음 포스트 글에서는 EKS 기능 확장을 위한 addon 설치 방법을 다룰 예정이다.\n","date":"Aug 28","permalink":"https://HanHoRang31.github.io/post/t101-eks-manage/","tags":["T101","EKS","cloud","AWS","Terraform"],"title":"[T101] 테라폼으로 EKS 관리하기"},{"categories":null,"contents":" 1 2 3 T101(=Terraform 101 Study)는 Terraform 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하시며, 책 \u0026#34;Terraform Up \u0026amp; Running\u0026#34;을 기반으로 진행하고 있습니다. 22년 9월 기준 학습 내용입니다. 블로그 이주로 다시 포스트합니다. 테라폼 스터디 4주차가 지났다. 스터디를 진행하면서 테라폼 기능과 툴을 학습하였고, 예제로 AWS 서비스 3티어 아키텍처를 구성할 정도가 되었다. 그렇다면 EKS 처럼 AWS 서비스가 복잡하게 얽혀있는 것도 테라폼으로 구축이 가능할까? 라는 생각으로 글을 작성한다.\n구글에 검색해보니 테라폼을 통해 EKS 구축하는 자료들을 쉽게 접할 수 있었다. 자료를 참고하는 것도 많은 도움이 되었지만, 실제 애플리케이션을 운영한다는 것을 가정하고 예제 코드를 수정하고 추가하는 것이 스스로의 실력 향상에 도움이 된다고 생각하여 가상의 주제를 정하였다.\nEKS 구축 목적은 가상의 사내 클라우드 저장소 운영으로 정하였다. 여러 글에 나눠 구축 경험을 공유할 예정이다. 여기서의 클라우드 저장소는 사내에서만 접근가능하며, 관리자만이 외부에서 베스천 서버를 통해 클러스터 접근이 가능하도록 한정하였다.\n먼저 구성 아키텍처는 다음과 같다.\n본 포스터 글에서는 테라폼 모듈들로 AWS 서비스를 구축하는 것까지 진행하였다. 실제 애플리케이션이나 운영 add-on서비스들은 추후 블로그 글을 통해 정리할 예정이다. 구성 아키텍처의 테라폼 코드들은 깃허브 링크를 통해서 확인이 가능하다. 레파지토리의 내용을 보면 EKS 구축에 필요한 AWS 서비스들과 사용 변수들을 파일들을 나눠 구성하였다.\nc1-version.tf : AWS 프로바이더와 지역을 설정하였다. c2-01,c2-02 : AWS 지역과 EKS 구성시 태그 정보나 지역 정보를 지역 변수로 정의한 파일이다. c3 : VPC 서비스를 구축하기 위한 모듈들을 정리했다. c4 : 베스천 서버를 목적으로 EC2를 구성한 파일이다. 또한, EKS 워크 노드 접속 설정과 외부에서 베스천 서버로 접속하기 위해 설정 모듈들이 정의되어 있다. c5 : EKS 구축을 위한 리소스를 정의했다. ~vars 파일들 : 실제 사용 변수들을 정의했다. EKS 구축에 사용한 모듈은 테라폼 레지스트리에서 인증된 AWS 모듈과 리소스를 사용했다. 테라폼 레지스트리에는 여러 모듈이 있지만, 외부 노출과 사용 안정성 면에서 공인적으로 인증된 모듈 사용을 적극 추천한다.\n공식 인증, 검증 마크 표시\n옆 마크 표시를 확인하자!\n테라폼 레지스트리에서는 여러 예제를 제공한다. 필자는 테라폼 학습시 예제를 통해 학습한 모듈의 내용을 확인하였고 추가 옵션 필요시 레지스트리 내 변수를 확인하여 옵션을 설정하였다.\nec2_instance 모듈 예제\n모듈 내 key_name 옵션 확인\n다음의 글은 필자가 커스터마이징을 진행한 부분과 정리가 필요한 내용들을 작성하였다.\nEKS 구성 필자는 사내 클라우드 저장소 구성을 목표로 EKS를 구축했다. 이를 위해 EKS 워크 노드 그룹을 프라이빗 서브넷에 배포하여 VPC 외부에서 접근하지 못하게 구성하였습니다. 또한, API 서버 액세스는 프라이빗 액세스로 설정했다.\nEKS 보안 그룹은 크게 3가지로 설정이 가능하다. 클러스터 보안 그룹, 추가 보안그룹(컨트롤 노드 보안 그룹), 워크 노드 보안 그룹으로 나눠지며 세부적으로 포트 제어가 가능하다. 클러스터 보안 그룹은 컨트롤 노드 그룹과 워크 노드 그룹 동시에 적용되는 보안 그룹이다. 해당 보안 그룹을 통해 컨트롤 노드와 워크 노드의 통신이 가능하도록 기본적으로 제공되는 보안 그룹이다. 공식 문서를 통해서는 최소 네트워크 허용 포트를 다음의 그림과 같이 권고한다.\nhttps://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/network_connectivity.md\n다만, 이번 포스트에서는 EKS 구축과 테스트 용으로 진행하는 것이 목적이라 클러스터 보안 그룹을 0.0.0.0/0 으로 설정하였다.\n노드 그룹 설정 운영 단위로 나눠 워크 노드 그룹을 구축하였다. 각 노드 그룹 분리는 실제 인프라간 영향을 최소화를 목적으로 분리하였다. 애플리케이션 운영을 위한 노드 그룹인 ops 와 클러스터 addon 서비스 운영을 위한 addon 로 구성하였고, 추후 모니터링이나 CI/CD 필요시 노드 그룹을 추가 구성할 예정이다.\n관리자 서버 접근 설정 EKS 관리를 위해 베스천 서버를 추가로 구성하였다. 베스천 서버는 외부에서 접근이 가능하도록 구성하였고, 베스천 서버에서 EKS API server 와 워크 노드에 접근이 가능하도록 구성했다.\n빨간 선은 관리자가 접근하게 되는 통신 경로이다. 이를 구성하기 위해 3가지 작업을 진행하였다.\n베스천 서버 접근 SSH 설정\n먼저 SSH 접속을 위해 AWS에서 pem key를 발급받아 private-key 폴더에 저장하였다. 이후 테라폼에서 베스천 서버 생성시 테라폼 프로비저너(provisioner)을 통해 로컬과 원격에서 SSH 키를 등록하도록 구성하였다. 프로비저너를 통한 스크립트는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Create a Null Resource and Provisioners resource \u0026#34;null_resource\u0026#34; \u0026#34;copy_ec2_keys\u0026#34; { depends_on = [module.ec2_public] # Connection Block for Provisioners to connect to EC2 Instance connection { type = \u0026#34;ssh\u0026#34; host = aws_eip.bastion_eip.public_ip user = \u0026#34;ec2-user\u0026#34; password = \u0026#34;\u0026#34; private_key = file(\u0026#34;private-key/eks-terraform-key.pem\u0026#34;) } ## File Provisioner: Copies the terraform-key.pem file to /tmp/terraform-key.pem provisioner \u0026#34;file\u0026#34; { source = \u0026#34;private-key/eks-terraform-key.pem\u0026#34; destination = \u0026#34;/tmp/eks-terraform-key.pem\u0026#34; } ## Remote Exec Provisioner: Using remote-exec provisioner fix the private key permissions on Bastion Host provisioner \u0026#34;remote-exec\u0026#34; { inline = [ \u0026#34;sudo chmod 400 /tmp/eks-terraform-key.pem\u0026#34; ] } ## Local Exec Provisioner: local-exec provisioner (Creation-Time Provisioner - Triggered during Create Resource) provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo VPC created on `date` and VPC ID: ${module.vpc.vpc_id} \u0026gt;\u0026gt; creation-time-vpc-id.txt\u0026#34; working_dir = \u0026#34;local-exec-output-files/\u0026#34; #on_failure = continue } } resource.connection : EC2 SSH 접속 정보 설정 resource.file : EC2 서버 내 파일 저장, 접근 키를 서버내로 복사 rresource.remote-exec : EC2 서버 내 스크립트 실행, 접근 키에 대한 권한 설정 resource.local-exec : 로컬에서 스크립트 실행, 서버 생성 후 VPC 정보를 local-exec-output-files/ 폴더에 저장 워크 노드 접근 SSH 설정\n베스천서버에서 워크노드로 접근하기 위해 다음과 같이 테라폼 코드 내에서 앞서 생성한 접근 키 설정하였다.\n1 2 3 remote_access { ec2_ssh_key = \u0026#34;eks-terraform-key\u0026#34; } API server 접근 설정\nAPI Server를 접근하기 위해서는 네트워크 확인과 config 키 등록이 필요하다. 먼저 네트워크는 베스천 서버와 EKS 클러스터를 동일 VPC에 생성하였고, 보안 그룹을 0.0.0.0/0 으로 설정하여 접근이 가능하도록 구성하였다. config 등록은 클러스터 구축 이후 테스트 단계에서 진행하겠다.\n구현 실제 구축은 테라폼 명령어를 통해 쉽게 구축이 가능하다. 다음의 그림은 테라폼 명령어 세부 동작에 대해 자세히 정리되어 있는 그림이 있어 공유한다.\n@StackSimplfy\nterraform init : 코드에서 정의한 프로바이더를 테라폼 레지스트리에서 확인하고 다운받는다. terraform validate : 테라폼 코드에 대해 유효 테스트를 진행해준다. terraform plan : 테라폼 코드가 어떻게 구현되는지 예측 결과를 보여준다. 예측 결과는 desired state로 정의되어 terraform.tfstate 파일로 저장된다. terraform apply : 구현 단계이다. 앞서 단계에서 생성한 파일을 토대로 실제 서비스를 구축한다. terraform destroy : 삭제 명령어다. 실행시 서비스를 삭제한다. 필자 또한 테라폼 명령어를 통해 EKS를 구축하였다. EKS 구축까지 대략 20분이 소비되었다. 구축이 완료되면 AWS 콘솔로 접속하여 구축한 EKS 정보를 확인할 수 있다.\nEKS 구축 확인\n테스트 EKS 구축이 완료되었으면 실제 테스트를 진행해보자. 관리자 입장에서 베스천 서버를 접근하고 클러스터 접근이 가능한지 확인하겠다. 먼저, 베스천서버의 EIP를 확인하고 발급받은 키를 통해 베스천서버로 접속하겠다.\nEIP 확인\n1 ssh -i private-key/eks-terraform-key.pem ec2-user@3.35.41.234 베스천 서버에서 일부 워크 노드 접근도 바로 가능하다. 확인을 위해 일부 워크 노드 IP(10.0.1.161)로 SSH 접근해보았다.\n베스천 서버에서 워크 노드로 접근\n마지막으로 클러스터 관리를 위해 API Server 접근하기 위해서는 쿠버네티스 명령어 도구인 kubectl 설치와 클러스터에 접근하기 위한 키 등록이 필요하다. kubectl 설치는 링크를 참고했고, EKS 클러스터 접근 키는 AWS 명령어를 통해 kubeconfig를 생성했다.\n1 aws eks --region ap-northeast-2 update-kubeconfig --name \u0026lt;cluster-name\u0026gt; EKS 접근 키 발급시 AWS 인증이 필요하다, 액세스 키와 보안키를 등록하고 EKS 클러스터 접근 키를 발급받자.\nkube 접근 토큰 발급\n마지막으로 클러스터 접근 키를 통해 클러스터 노드 정보를 확인하고, 노드그룹 확인을 위해 노드 그룹 라벨을 확인하면 테스트가 완료된다.\n노드 정보 확인\n끝으로 사족이지만, T101 스터디를 통해서 여러 테라폼 툴을 접해보았다. EKS 구축을 진행하면서 사용했던 툴에 대한 경험을 공유하고자 한다.\n테라폼 시각화 툴, pluralith(https://www.pluralith.com/)\n테라폼 시각화 툴이지만, 아직 알파 버전으로 업데이트가 필요하였던 툴이다. 특히 EKS 같이 여러 리소스가 혼합되어 복잡해지면 시각화에 한계가 있었다. 또한, pluralith 서버 내 시각화 실행 메모리가 낮게 측정되어 있었다. 처음 시각화 진행시 버그 이슈로 시각화가 안되어 깃허브 이슈를 통해서 원인을 요청하였고 pluralith 측에서 메모리가 낮게 설정되어 있던 것을 확인하였다. 해당 툴 사용시 시각화 단계에서 버그를 뱉는다면 깃허브 이슈로 문의해보자.\n테라폼 비용 계산 툴, infracost(https://www.infracost.io/docs/)\n테라폼에서 정의한 서비스들에 대해 비용을 측정해주는 툴이다. 사용하기 편했고 비용 측정에 있어 한눈에 알아볼 수 있어 유용했다. 아래는 이번 포스트에서 구축한 EKS 운영 비용이다. 206달러라니.. 비용이 상당하다.\n이로써 EKS 구축이 끝났다! 다음 블로그 글에서는 EKS addon를 테라폼으로 구축하는 경험을 공유할 예정이다.\n","date":"Aug 28","permalink":"https://HanHoRang31.github.io/post/t101-eks-build/","tags":["T101","EKS","cloud","AWS","Terraform"],"title":"[T101] 테라폼으로 EKS 구축하기"},{"categories":null,"contents":"이번 블로그 글의 주제로는 AWS 환경에서 시스템 규모의 확장에 따른 아키텍처에 대해 글을 쓰고자 한다. 시스템 규모의 확장은 백명의 사용자에서 수백만 사용자를 지원하는 시스템 설계를 뜻한다. 그렇다면, 처음부터 수백만의 사용자를 지원하는 시스템을 설계하면 되지 않을까라고 생각되지만, 확장에 필요한 서비스들의 비용, 작업 오버헤드 및 러닝 커브때문에 추천하지 않는다. 결국 소수의 사용자에서 몇 백만으로 이르기까지의 지속적인 계랑과 끝없는 개선이 요구되는 과정이라 볼 수 있다.\n그렇기 때문에 시스템 규모 확장를 이해하면 아키텍처 설계 및 이해에 큰 도움이 될 것이라 생각하며 정리한 글을 공유할 예정이다. 규모 확장을 주제로 AWS 유튜브 및 서적을 참고로 작성하였으며, 직접 실습 예제를 배포하여 규모의 확장 내용을 다룰 것이다. 또한, 컨테이너 오케스트레이션 환경인 EKS가 규모의 확장 과정에서 어떤 역할을 하는 지 살펴보겠다. 규모의 확장 환경으로는 AW 이번 글에서는 AWS 클라우드 환경 안에서를 한정하여 작성한다.\n시스템 규모의 확장 100명 이상의 사용자 : 기본 3티어 아키텍처 아래 그림은 시스템 규모의 초기 시스템 아키텍처이다. 아키텍처는 DNS 서비스인 Route53 과 웹 서버, 데이터베이스으로 구성된다. 기본 통신 과정은 사용자가 Rouet53으로 도메인을 질의하여 웹 서버 IP 주소를 반환받고 통신된다.\n기본 시스템임에도 불구하고 수백까지의 트래픽을 감당할 수 있다. 하지만 천의 레벨로 넘어가면 어떻게 될까? 다음과 같은 고려사항이 필요하다.\nSingle Point of Failure (SPOF) : 시스템 내에서 단 한 곳에서 발생한 장애가 전체 시스템의 작동을 중단시키는 구성 요소이다. 위 아키텍처 처럼 단일 서버에서 문제가 발생하면 전체 웹 서비스가 작동을 멈출 것이다. 이를 해결하기 위해 고가용성을 유지해야 한다. 사용자에 따른 시스템 병목 현상 및 스케일링 : 사용자가 동시에 몰리게 되면 각 레이어에서 처리량이 증가하여 속도를 저하시기고 응답 시간이 느려지게 된다. 또한, 데이터베이스에서는 많은 사용자 처리시 동시 연결 수를 초과하여 연결 요청이 거부될 수 있고, 무결성 문제가 일으킬 수 있다. 10,000명 이상의 사용자 : 이중화와 CDN 캐싱 AWS 서비스를 사용하면 위 고려사항을 모두 해결할 수 있다. AWS 의 서비스는 관리형 서비스로 이중화 기능이 제공되기 때문이다. 또한, 기능 확장 서비스를 연계하여 추가 기능을 쉽게 추가할 수 있다. 각 레이어 별로 추가한 서비는 다음과 같이 나눌 수 있다.\n서버 수평 확장 : 오토스케일링을 통해 서버 수를 증가, 추가시킬 수 있는 서비스이다. 이를 통해 부하, 장애 대응, 비용 절감 효과를 부를 수 있다. ELB(Elastic Load Balancing) 을 통한 수평 확장 : 여러 서버가 생김에 따라 트래픽을 분산시켜야 한다. AWS에서는 트래픽 부하를 고르게 분산해주는 로드밸런싱 서비스로 ELB를 제공한다. ELB는 다중 가용 영역을 지원하며, 자동으로 용량이 확장된다. 주로 서버 수평 확장에서 사용된 오토스케일링 서비스를 연계하여 사용한다. RDS 을 통한 데이터베이스 이중화 : RDS는 관리형 데이터베이스 서비스로 간편한 관리, 뛰어난 확장성, 가용성 및 내구성, 성능을 가진다. 특징 중 가용성이 이중화를 뜻하는데, 다중 가용 영역에서 master-replica 구조로 운영됨을 뜻한다. RDS에서는 Aurora라는 DB가 지원되는데 샤드 스토리지 볼륨을 통해 기본 DB 엔진에 비해 뛰어난 성능 및 확장성을 제공한다. CloudFront \u0026amp; S3 를 통한 캐싱 : CloudFront 는 정적 컨텐츠를 전송하는데 쓰이는 CDN(Content Delivery Network)서비스이다. CloudFront를 통해 컨텐츠를 제공하면 사용자는 빠른 응답 시간과 서버 측에서의 과부하를 줄일 수 있다. 사용자가 컨텐츠 요청시 S3에 해당 컨텐츠가 있다면 서버를 거칠 필요없이 CloudFront에서 처리되기 때문이다. 100,000 명 이상의 사용자 : 컨테이너 서비스 전환 및 DB 성능 개선 100,000명 이상의 사용자를 가진 시스템이라면 시스템 자체의 구조 개선과 DB 성능 개선이 필요하다. 이를 위한 방법으로 컨테이너 서비스와 인메모리 DB를 활용할 수 있다.\n컨테이너 서비스 전환\n100,000 명 이상의 사용자가 시스템을 사용한다면 컨테이너 전환를 고려할 때이다. 컨테이너란 코드, 의존성, 런타임을 묶어서 만든 하나의 Immutable 단일 객체이다. Immutable 란 운영 중 시스템을 변경하는 것이 아니라 새로운 것으로 교체하는 것을 뜻한다. 또한, 컨테이너는 표준화 및 경량화되어 있을 뿐 아니라 이식하기 쉽고 배포 측면에서 강점을 이루기 때문에 마이크로 서비스 아키텍처를 구현하는데 필수적인 플랫폼으로 여겨진다.\nAWS 에서는 이러한 컨테이너를 관리하는 오케스트레이션 서비스인 서비스를 제공한다. 각 컨테이너를 묶어 파드로 구성하며 관리 모델에 따라 여러 서버스(Fargate, EKS, ECS) 로 제공된다. 그 중 EKS 서비스는 쿠버네티스를 기반으로 하는 관리형 컨테이너 서비스로 쿠버네티스 구성 중 컨트롤 플레인을 관리한다. 다른 컨테이너 서비스와 달리 EKS는 쿠버네티스 API를 따르기 때문에 기존의 쿠버네티스 환경과 호환성을 보장하여 다른 클라우드 환경과 온-프레미스와 통합이 가능하다.\n인메모리 DB를 통한 읽기 성능 개선\n인메모리 DB란 데이터를 컴퓨터 메인 메모리(RAM)에 저장하여 데이터베이스보다 훨씬 빠른 성능을 제공하는 DB이다. 인메로리 DB를 당장의 아키텍처로 적용시 읽기 성능을 높일 수 있는 캐싱으로 적용할 수 있다. AWS 에서는 인메모리 데이터스토리지 서비스로 ElastiCache를 제공한다. ElastiCache 는 관리형 Memcached 및 redis 엔진을 제공하며 자동 확장성, 자가 복구, 한 자리수 ms 을 가진다. 일반적으로 캐쉬, 세션 저장소, 채팅, 게임 리더 보드등에 활용되어진다.\n서버 구조가 EKS로 전환됨에 따라 로깅 통합 도구가 필요하다. 아키텍처에서는 AWS 로깅 통합 서비스인 opensearch를 사용하였다. 1,000,000 만 이상의 사용자를 넘어서 : 데이터베이스 분산 및 재해 복구, 멀티리전 서비스 구현 위 아키텍처를 확인하면 규모에 따라 서버가 확장되어 분산 처리되는 반면, DB는 Master에서만 쓰기를 담당하기에 기능 개선을 위해 데이터베이스 분리 및 샤딩이 필요하다. 또한, 가용성을 보장하고 데이터 손실을 예방하기 위해 재해 복구 및 멀티 리전 구성이 필요하다.\nDB 분산\nDB 분산 방법 중 하나로 용도에 맞는 DB 서비스를 나눠 적용시켜야 한다. AWS에서는 용도에 따라 DB 서비스를 다음과 같이 선택할 수 있다.\n아래 소개할 아키텍처에서는 비관계형 DB인 DynamoDB를 사용하였는데 쓰기 작업이 많이 필요한 경우(장바구니, 위시 리스트)에 적합하기 때문에 구성하였다. DynamoDB는 대규모 요청에도 한자릿수 ms 응답시간을 보장하며, 페타라이트 규모 스토리지, 다중 리전 글로벌 테이블 복제 및 DAX 를 통한 자체 읽기 및 쓰기 캐시를 제공한다. 또한, 10,000,000 명 이상의 규모 확장시 DB 자체 내 글로벌 기능을 활성화시켜 운영하는 것을 추천한다.\n재해 복구 및 멀티리전 서비스 구현\n스토리지 서비스인 S3의 교차 리전 기능과 DB, 서버 스냅샷을 활용하여 멀티 리전을 구성할 수 있다. 이를 통해 리전 1에서 문제가 발생할 경우 리전 2에서 빠르게 리소스를 복구시킬 수 있다. 리소스 복구 방법은 두 가지 단계가 필요하는데 DB는 글로벌 RDS나 스냅샷으로 그외 서비스는 CDK 나 클라우드포메이션, 테라폼으로 복구 시킬 수 있다.\nEKS 기반의 서비스 애플리케이션 구성 앞 장에서 소개한 아키텍처 중 100,000 단위의 사용자 규모의 MSA 아키텍처를 구현해보겠다. MSA 는 쿠버네티스 AWS 관리형 서비스인 EKS를 활용하여 구성하였다. 구성 예제는 사용자 투표 기반의 애플리케이션으로 투표 기능와 결과 화면을 제공한다. 구성 아키텍처는 다음과 같다.\nMSA 로 구성 기능이 나눠지고 언어별로 나눠진 것을 확인할 수 있다. 기능별로 구성부분을 나누자면 두 가지로 분류할 수 있다.\nVote 출력 : DB 조회(Postgres) → result (node.js) Vote 입력 : work(python) → 인메모리DB(redis)→ worker(.NET) → DB(Postgres) 그럼, 예제 애플리케이션을 배포하여 연동, 구성 부분을 확인하자.\n사전 작업인 EKS 구성 및 Addon 은 구성 내용이 많아 필자의 블로그 글로 대체하였다. 본 블로그 글에서는 애플리케이션 배포와 구성 부분을 살펴볼 것이다. EKS 구성과 Addon 배포는 밑의 링크를 참고하자.\nEKS 구성 : 필자 블로그 글 [AEWS] EKS 아키텍처와 Private EKS 클러스터 배포하기 EKS addon(로드밸런싱, Route53) : 필자 블로그 글 [AEWS] EKS VPC CNI Deep Dive EKS addon까지 구성을 완료하였으면 투표 애플리케이션을 배포하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # git clone git clone https://github.com/HanHoRang31/blog-share.git cd k8s-app/vote-app tree # 네임스페이스 생성 후 변경 kubectl create ns vote # 서비스 배포 kubectl apply -f . # ExternaDNS 추가 ## 각자 자신의 도메인 정보 입력 MyDOMAIN1=\u0026lt;각자 자신의 nginx 도메인 지정\u0026gt; MyDOMAIN1=vote.hanhorang.link MyDOMAIN2=result.hanhorang.link kubectl annotate service vote \u0026#34;external-dns.alpha.kubernetes.io/hostname=$MyDOMAIN1.\u0026#34; kubectl annotate service result \u0026#34;external-dns.alpha.kubernetes.io/hostname=$MyDOMAIN2.\u0026#34; 애플리케이션 배포 후 입력한 도메인에서 기능을 확인할 수 있다.\n투표 입력과 투표 결과가 정상적으로 작동한다. 기능을 확인했으니, 기능별로 각 구성의 핵심 부분을 살펴보자\nDB 구성 애플리케이션 구성시 선수 작업으로 DB 구성이 필요하다. 예제 애플리케이션에서는 PostgresDB 를 통해 DB를 구성하였다. 특별히 유심히 본 부분은 DB 스키마를 서비스 배포시 자동적으로 구성하도록 진행하였는데, 매니페스트파일에서도 확인할 수 없어 코드 레벨까지 살펴보았다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # PostgreDB 접속 kubectl run my-release-postgresql-ha-client --rm --tty -i --restart=\u0026#39;Never\u0026#39; --namespace default --image docker.io/bitnami/postgresql-repmgr:15.3.0-debian-11-r23 --env=\u0026#34;PGPASSWORD=postgres\u0026#34; \\ --command -- psql -h db.default.svc.cluster.local -p 5432 -U postgres -d postgres # DB 확인 postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | ICU Locale | Locale Provider | Access privileges -----------+----------+----------+------------+------------+------------+-----------------+----------------------- postgres | postgres | UTF8 | en_US.utf8 | en_US.utf8 | | libc | template0 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | | libc | =c/postgres + | | | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | | libc | =c/postgres + | | | | | | | postgres=CTc/postgres PostgresDB 배포시 기본 DB만 배포되었다. 테이블 구성 부분을 찾아보자.\n테이블 구성 부분은 데이터 입력 구성 서비스인 .NET에서 DB 연결 및 스키마를 구성하였다. DB 연결 함수의 코드는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // k8s-app/vote-app/worker/ private static NpgsqlConnection OpenDbConnection(string connectionString) { NpgsqlConnection connection; while (true) { try { connection = new NpgsqlConnection(connectionString); connection.Open(); break; } catch (SocketException) { Console.Error.WriteLine(\u0026#34;Waiting for db\u0026#34;); Thread.Sleep(1000); } catch (DbException) { Console.Error.WriteLine(\u0026#34;Waiting for db\u0026#34;); Thread.Sleep(1000); } } Console.Error.WriteLine(\u0026#34;Connected to db\u0026#34;); var command = connection.CreateCommand(); // DB 스키마 구성 command.CommandText = @\u0026#34;CREATE TABLE IF NOT EXISTS votes ( id VARCHAR(255) NOT NULL UNIQUE, vote VARCHAR(255) NOT NULL )\u0026#34;; command.ExecuteNonQuery(); return connection; } DB 연결은 C#의 Npgsql 라이브러리를 통해 연결하였다. 함수 호출의 매개변수는 다음과 같이 string 값을 넣어서 진행하였다.\n1 var pgsql = OpenDbConnection(\u0026#34;Server=db;Username=postgres;Password=postgres;\u0026#34;); 예제 애플리케이션이라 DB 정보가 하드코딩되어 있다. 실제 애플리케이션에서는 쿠버네티스 시크릿이나 키 관리 서비스를 활용해야 한다.\nVote 결과 출력 DB 조회(Postgres) → result (node.js) 연동 부분이다.\n살펴볼 것은 Vote 조회 부분이다. node.js 내 데이터 조회 부분은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // k8s-app/vote-app/result/ var express = require(\u0026#39;express\u0026#39;), async = require(\u0026#39;async\u0026#39;), pg = require(\u0026#39;pg\u0026#39;), // DB 연결 var pool = new pg.Pool({ connectionString: \u0026#39;postgres://postgres:postgres@db/postgres\u0026#39; }); . // DB 데이터 조회 function getVotes(client) { client.query(\u0026#39;SELECT vote, COUNT(id) AS count FROM votes GROUP BY vote\u0026#39;, [], function(err, result) { if (err) { console.error(\u0026#34;Error performing query: \u0026#34; + err); } else { var votes = collectVotesFromResult(result); io.sockets.emit(\u0026#34;scores\u0026#34;, JSON.stringify(votes)); } setTimeout(function() {getVotes(client) }, 1000); }); Vote 입력 예제 애플리케이션의 차별점을 확인할 수 있는 부분이다. 차별점은 인메모리 DB가 중간에 껴서 DB 과부하 분산 및 입력 성능을 올려주는 것인데, 코드 레벨로 내려가 각 연동 부분을 확인하겠다. 구성은 vote(python) → 인메모리DB(redis)→ worker(.NET) → DB(Postgres) 로 되어 있으므로 순차적으로 확인하자.\nvote(python)\n플라스크로 서버를 구성하였으며, redis 라이브러리를 통해 인메모리DB에 데이터를 전달한다. 코드 구성으로 확인해보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 라이브러리 from flask import Flask, render_template, request, make_response, g from redis import Redis . . # redis 연결 def get_redis(): if not hasattr(g, \u0026#39;redis\u0026#39;): g.redis = Redis(host=\u0026#34;redis\u0026#34;, db=0, socket_timeout=5) return g.redis # redis 데이터 로 전송 @app.route(\u0026#34;/\u0026#34;, methods=[\u0026#39;POST\u0026#39;,\u0026#39;GET\u0026#39;]) def hello(): voter_id = request.cookies.get(\u0026#39;voter_id\u0026#39;) if not voter_id: voter_id = hex(random.getrandbits(64))[2:-1] vote = None if request.method == \u0026#39;POST\u0026#39;: redis = get_redis() vote = request.form[\u0026#39;vote\u0026#39;] app.logger.info(\u0026#39;Received vote for %s\u0026#39;, vote) data = json.dumps({\u0026#39;voter_id\u0026#39;: voter_id, \u0026#39;vote\u0026#39;: vote}) redis.rpush(\u0026#39;votes\u0026#39;, data) worker(.NET)\n0.1 초마다 인메모리 DB를 읽어 싱크 작업을 진행한다음 DB로 전달한다. 여기서 데이터 쓰기시 인메모리 DB를 사용하면 쓰기 성능가 올라가고, 분산 처리로 여러 서버에서의 데이터를 전달받아 동시에 작업을 수행할 수 있다. 이를 통해 다수의 사용자가 투표시 직관적으로 결과를 바로바로 확인할 수 있을 것이다. 핵심 로직 코드는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 . . while (true) { // Slow down to prevent CPU spike, only query each 100ms Thread.Sleep(100); // Reconnect redis if down if (redisConn == null || !redisConn.IsConnected) { Console.WriteLine(\u0026#34;Reconnecting Redis\u0026#34;); redisConn = OpenRedisConnection(\u0026#34;redis\u0026#34;); redis = redisConn.GetDatabase(); } string json = redis.ListLeftPopAsync(\u0026#34;votes\u0026#34;).Result; if (json != null) { var vote = JsonConvert.DeserializeAnonymousType(json, definition); Console.WriteLine($\u0026#34;Processing vote for \u0026#39;{vote.vote}\u0026#39; by \u0026#39;{vote.voter_id}\u0026#39;\u0026#34;); // Reconnect DB if down if (!pgsql.State.Equals(System.Data.ConnectionState.Open)) { Console.WriteLine(\u0026#34;Reconnecting DB\u0026#34;); pgsql = OpenDbConnection(\u0026#34;Server=db;Username=postgres;Password=postgres;\u0026#34;); } else { // Normal +1 vote requested UpdateVote(pgsql, vote.voter_id, vote.vote); } } 핵심 부분은 ListLeftPopAsync 함수이다. ListLeftPopAsync 함수는 Redis 라이브러리에서 제공하는 메서드로, Redis의 리스트에서 가장 왼쪽(즉, 가장 먼저 입력된) 요소를 제거하고 그 값을 반환하는 기능이다. 비동기적으로 작동하기 때문에 쓰레드 적용이 가능하다. 끝으로 이번 글에서는 시스템 규모 확장에 초점을 맞추어 AWS 아키텍처를 확인하고, 예제 애플리케이션을 통해 100,000 규모의 사용자 애플리케이션의 아키텍처를 일부 구현하였다. 이번 시간에는 인메모리를 통해 DB 단의 성능을 챙겼지만 프론트앤드에서의 성능 이슈로 애플리케이션 운영시 CDN 연동까지 고려해야 한다.\n다음 시간에는 시스템 규모 확장을 테스트하기 위해 트래픽 테스트 툴을 살펴볼 예정이다.\n","date":"Jul 26","permalink":"https://HanHoRang31.github.io/post/archi-scale-up/","tags":["EKS","cloud","AWS","Architecture"],"title":"[아키텍처] AWS로 이해하는 시스템 규모의 확장"},{"categories":null,"contents":"\n이번 글에서는 최근에 취득한 AWS Devops Professnal(DOP-C02) 자격증 관련하여 취득 후기를 공유하고자 한다. 해당 자격증은 23년 4월 이후로 업데이트된 자격증으로 인터넷에 관련 취득 글이 없다. 이 글을 보고 자격증 취득에 도움이 되었으면 좋겠다는 마음으로 글을 작성한다.\nDOP-C02 ? AWS DOP-C02 자격증의 명칭은 AWS Certified DevOps Engineer - Professional 로 Devops 분야에 특화된 전문 지식을 요구하는 자격증이다. 공식 문서에서 자격증 소개는 다음과 같이 소개되어 있다.\nDevops 시험에서 요구하는 전문 지식과 구성은 다음과 같다.\nSDLC 자동화 : 소프트웨어 개발 생명 주기 (Software Development Life Cycle, SDLC) 자동화는 소프트웨어 개발 과정의 다양한 단계를 자동화시켜주는 프로세스이다. 프로세스로 지속적인 통합(CI), 지속적인 배포(CD), 지속적인 테스팅(CT), 인프라스트럭쳐 자동화가 포함되어 있다. 시험에서는 해당 프로세스의 AWS 서비스인 Opsworks, CodeStack의 내용이 주가 되었다. 구성 관리 및 IaC : 구성 관리는 시스템의 설정을 통제하고 추적하여 일관성을 유지하는 프로세스이다. 시험에서는 AWS Cloudformation 기능 관련 문제가 많았다. 탄력적인 클라우드 솔루션 : 요구사항에 따라서 자동으로 리소스를 확장하거나 축소할 수 있는 클라우드 컴퓨팅 모델을 의미한다. 시험에서는 특히 고가용성 및 자연 재해 대비를 위한 지역 간 복제 인프라 구성 및 운영 문제가 특히 많이 나왔다. DB 와 기타 리소스 인프라의 구성법이 다르니 반드시 확인할 것을 추천한다. 모니터링 및 로깅 : 모니터링은 시스템의 성능과 가용성을 실시간으로 추적하고 분석하는 것을 의미하며, 로깅은 시스템에서 발생하는 이벤트를 기록하고 저장하는 것을 의미한다. 시험에서는 주로 어떤 서비스를 운영하는데 모니터링, 로깅 솔루션을 어떻게 구축할 것인지에 대해 물어봤는데 결국 Cloudwatch, Cloudwatch log, kinesis \u0026amp; S3 , Cloudtrail 서비스 중 어떤 서비스를 이용하여 구축할 것인지에 대한 질문으로 각 서비스별 기능 확인이 필요하다. 인시던트 및 이벤트 대응 : 시스템에서 발생하는 문제나 이벤트에 대한 적절한 대응과 조치를 수행하는 것을 의미한다. 이 부분이이전 DOP-C01과 많이 달라진 분야다. 결국에는 서비스별 어떻게 연동할 것 인지에 대해 물어보는데 DOP-C01에서는 람다로 연동을 처리했다면, DOP-C02에서는 EventBridge 로 연동한다고 보면 된다. 기간이 지나면서 서비스가 고도화되는데 이 부분에서 내용이 공유된 것 같다. 보안 및 규정 준수 : 시스템을 보호하는 것뿐만 아니라, 개인 정보 보호 및 기타 규정 준수와 관련된 법률을 준수하는 것을 포함하여 관련 기능을 물어본다. AWS 서비스 중 AWS Inspector(취약점), AWS Trusted Advisor(최적화 방법 안내), AWS Config(히스토리 추적 및 변화 감사), AWS GuardDuty(워협 탐지) 가 주로 나온다. 각 서비스별로 특징만 기억해두면 문제없이 넘어갈 수 있는 분야다. 시험 문제가 업데이트되었다고 하여 EKS, Docker 모델에서 모니터링, 이벤트 대응, CI \u0026amp; CD 가 많이 나올 것이라 예상했지만,, 3문제 정도 그것도 구성 문제정도만 출제되었다. 모델별로 봤을 때는 3티어 애플리케이션 및 하이브리드 구성에서 마이그레이션과 운영 내용이 주가 된 것으로 봤을 때 실제 수요를 반영하여 문제를 업데이트한 것이 아닐까 예상한다.\n자격증 취득 후기 아마 많은 분들이 이 부분에 대하여 찾지 않을까 생각된다. 필자 또한 취득 공유기를 얻고자 인터넷을 많이 서치하였고, 이전 버전(DOP-C01) 후기로는 덤프만 보고 학습하면 통과할 수 있는 수준으로 이해하였다. DOP-C02 학습법도 동일했는데 문제는 덤프의 답이 필자 생각과 너무 달랐던 것이 문제였다.\n이 부분이 가장 걱정이였다. 필자가 작년 AWS Solution Architect Professional 자격증을 취득했을 때를 회고해보면 덤프에서 나온 문제와 답을 그대로 찍었음에도 턱걸이로 합격했었는데, 덤프 답이 애초에 오답이 아닐까 라고 생각했었다. 아니나 다를까 이번에도 어김없이 덤프 답이 이상할 정도로 오답이 많다.\n이를 해결하기 위해 문제 풀이에 많은 시간을 소비하였다. 며칠 굴러보니 이론적인 학습에 너무나 시간이 많이 들어 비효율적이라 느껴 새로운 접근 방법으로 접근하였다. 다른 덤프 사이트를 서치하니 집단 지식을 활용하여 덤프 답을 파악할 수 있는 사이트가 존재하였고, 해당 사이트를 통해 답을 비교하며 학습을 진행하였다.\n해당 사이트는 examtopics 라는 사이트인데 아래 사진과 같이 문제를 확인함과 동시에 댓글로 여러 사람들의 답과 생각을 확인할 수 있어 학습에 큰 도움이 되었다.\n답을 비교하고, 체크하면서 학습하였고 시험을 본 결과 덤프의 문제가 80%이상 나왔다. 문제풀이한 답이 맞으면 상관이 없다고 생각하여 나머지 문제를 푼 결과 아래와 같이 좋은 점수로 자격증을 취득할 수 있었다.\n사실 시험보기 전까지도 걱정이 많았는데 전혀 예상치 못한 점수가 나왔다. 취득한 점수를 토대로 덤프 보완점을 고려하자면 다음과 같다.\n덤프 답에 오답이 너무 많다. 집단 지식을 활용하자. 집단 지식에서의 답 비율이 많다고 해서 무조건 정답은 아니다. 필자의 경우 최신 댓글과 답이 다른 것들은 직접 기능을 확인해봤는데 최신 댓글이 답인 경우가 많았다. 나머지 20% 문제는? 결국에는 같은 분야의 문제가 나오기 때문에 덤프 문제를 학습하면서 이해할 정도면 푸는 것에 크게 어려움이 없을 것이라 생각된다. DOP-C01 덤프의 출제 비중? 비교하니 대략 20%정도되는 것 같다. 물론 위 사이트의 DOP-C02 덤프에서 이전 버전의 문제도 확인할 수 있으니 C01 덤프까지 보지 않는 것을 추천한다. 시험 문제는 75문제이고 23년 7월 기준 덤프 문제는 134문제이다. 절반 정도이지만 출제 비율이 80프로이니 반드시 학습하자. 문제 외적으로 유의 사항은 다른 취득 후기와 다를 것이 없다. 취득 후기 관련하여서는 이만 글을 줄이겠다.\n자격증 취득이 끝이 아니다. 경험한 바 덤프로 공부해서 학습한 내용들은 이론적이고 결국에는 문제의 답을 그대로 외우기 때문에 이해를 못한 기능들이 많아 아쉬웠다. 이대로 놔두면 아마 1달내로 관련 지식이 초기화될 것이 뻔하다. 이를 실무에서 활용하기위해 덤프에서 학습 내용들을 정리하고 실습한 내용들을 공유할까 한다. 다음 글에서는 EventBridge 와 보완 인프라 구축 및 블루/그린 배포 관련하여 작성할 예정이다.\n","date":"Jul 10","permalink":"https://HanHoRang31.github.io/post/aws-devops-professnal/","tags":["devops","cloud","AWS"],"title":"[자격증] AWS Devops Professnal(DOP-C02) 취득 후기"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. 스터디 마지막 시간에는 EKS Automation 을 주제로 진행하였다. 여기서 Automation 단어는 인프라 프로비저닝 및 코드 배포를 포함하여 소프트웨어 제공 수명 주기의 일부를 자동화를 의미한다. 스터디에서는 모임장님께서 AWS 서비스 기반의 Automation(ACK)를 소개해주셨는데 실무에 도입하기에는 아직 보완할 서비스라 느껴, 다른 주제(gitops ci / cd 파이프라인 구성)를 선정하여 블로그를 작성한다.\n사실 지난 스터디에서 다룬 주제였지만, 내부 트러블슈팅으로 절반(CD 파이프라인 구성) 정도 밖에 구성하지 못하였다. 다시 스터디를 통해 계기가 생긴 만큼 이번 글에서는 CI 파이프라인 구성 확장 및 CD 연결까지 다룰 예정이다. 사용한 코드들은 필자의 레파지토리에서 확인이 가능하다.\nCI / CD 코드가 변경될 때마다 자동적으로 코드 통합(CI)와 배포(CD)를 수행하는 소프트웨어 개발 방식이다. 이를 통해 애플리케이션을 보다 짧은 주기로 서비스를 제공할 수 있어 비즈니스 이점이 큰 기술로 최근 devops 핵심 기술이라 할 수 있다.\nhttps://server-engineer.tistory.com/800\nGitops는 CI / CD일까? 엄밀히 말하면 다르다. GitOps는 이런 CI/CD 프로세스에 git를 사용하여 인프라와 애플리케이션의 설정을 관리하는 방법이다. 달리 말하면, CI/CD는 코드 변경을 효과적으로 관리하고 배포하는 프로세스이며, GitOps는 이런 프로세스를 git을 통해 조정하고 관리하는 방식이라고 할 수 있다. 이 둘은 상호 보완적인 개념으로, GitOps는 CI/CD 파이프라인을 통해 애플리케이션을 배포하고 관리하는 데 사용될 수 있다.\nhttps://blogs.vmware.com/cloud/2021/02/24/gitops-cloud-operating-model/\n이번 시간에는 EKS 환경에서 CI / CD 구성을 목표로 애플리케이션을 코드에서 배포까지 자동화하는 작업을 진행하고자 한다. CD 구성 같은 경우 지난 블로그 글을 통해 진행하였고, 이어서 CI 구성과 gitops ci/cd 까지 확장하여 다룰 것이다. CI 구성에 사용한 도구는 보편적으로 가장 많이 사용하는 젠킨스(jenkins)를 사용핬고 gitops ci cd 구성을 위해 CD(Argocd), container registry(harbor), gitops(gitlab) 을 이용하였다. 구성 아키텍처는 다음과 같다.\n복잡해보이지만, 각 도구들을 나눠서 확인하고 도구별 연동 방법에 대해 소개하겠다.\n젠킨스(Jenkins) CI 빌드 도구가 많지만, 젠킨스를 선택한 이유는 확장성과 커뮤니티 활성도가 압도적으로 높아서다. 젠킨스는 지속적 통합을 위한 오픈 소스 자동화 도구이다. 파이프라인 중 도커 이미지빌드 및 테스트, git에 업데이트(통합)까지의 과정을 구성해주는 도구이다. 젠킨스는 다양한 플러그인이 제공되어 기능 확장이 쉽게 가능하다. 대표적으로 쿠버네티스 플러그인을 통해서 쿠버네티스 APi 서버와 통신하여 코드 테스트(unit test)을 담당하는 Slave 파드를 동적으로 생성하고 관리할 수 있다. 이를 통해서 얻을 수 있는 이점은 다음과 같다.\n동적 스케일링: 쿠버네티스는 Jenkins 내 유닛 테스트(Jenkins Slaves)를 파드로 실행할 수 있다. 이렇게 하면 빌드/테스트 워크로드에 따라 파드를 동적으로 스케일링 할 수 있다. 즉, 추가적인 빌드 요구 사항이 있을 때마다 쿠버네티스는 새로운 유닛 테스트 파드를 생성하고, 작업이 완료되면 이를 제거할 수 있고 요청량에 따라 파드를 조절할 수 있다. 이를 통해 리소스를 효율적으로 사용할 수 있다. 고가용성: 쿠버네티스의 내장된 고가용성 기능 덕분에, Jenkins 마스터의 다운타임을 크게 줄일 수 있다. 이는 Jenkins 마스터가 다운되더라도 쿠버네티스가 자동으로 새 인스턴스를 시작하여 서비스 중단 시간을 최소화할 수 있다는 것을 의미한다. https://devopscube.com/jenkins-architecture-explained/\n아키텍처도 간단하다. jekins 배포시 마스터 파드가 배포되며 해당 파드에서 jenkins https 서버(UI) 실행 및 빌드 스케쥴러, 유저, 플러그인, 보안 정보들을 관리한다.\nJenkins 배포 헬름으로 젠킨스를 배포하면서 구성 아키텍처를 확인하겠다. 먼저, 차트를 가져오자.\n1 2 3 helm repo add jenkins https://charts.jenkins.io helm repo update helm fetch jenkins/jenkins --untar --version 4.3.9 차트에는 마스터(컨트롤러), Agent(노드), 플러그인 설정, PV 및 백업 설정, 보안적 요소가 들어가있다. 또한, 젠킨스 차트에는 VALUES_SUMMERY.md 이 포함되는데 전체 파라미터에 대한 설명이 알기 쉽게 들어가있다.\n구성 차트는 다음과 같이 수정했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # values-jenkins.yaml controller: adminUser: \u0026#34;admin\u0026#34; adminPassword: \u0026#34;admin1234\u0026#34; jenkinsUriPrefix: \u0026#34;/jenkins\u0026#34; installPlugins: - kubernetes - workflow-aggregator - git - configuration-as-code - pipeline-stage-view - gitlab ingress: enabled: true apiVersion: \u0026#34;networking.k8s.io/v1\u0026#34; annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: \u0026lt;cert ARN 입력\u0026gt; alb.ingress.kubernetes.io/healthcheck-path: /login kubernetes.io/ingress.class: alb ingressClassName: alb hostName: \u0026lt;host domain 입력\u0026gt; 1 2 kubectl create ns jenkins helm install jenkins jenkins/jenkins -f values-jenkins.yaml --namespace jenkins --version 4.3.9 배포 후 jekins 페이지에 접속이 잘되는 지 확인해보자. 위 차트에서 확인했듯이 admin 아이디와 비밀번호는 admin / admin1234 이다.\nJenkins \u0026amp; 쿠버네티스 연동 젠킨스 파이프라인 작업을 쿠버네티스에서 작업시키려면 쿠버네티스와 연동이 필요하다. 다음 그림과 같이 젠킨스에서 쿠버네티스 주소를 입력하고, 쿠버네티스 접근 정보를 입력하자.\njenkins 관리 → 노드 관리 → Configure Clouds 접근 후 쿠버네티스 연동을 진행하자, 연동을 위해 kubernetes URL과 접근 정보가 필요하다. kubernetes URL과 접근 정보는 베스천 서버에 접근 파일에서 확인할 수 있다. 이를 복사해서 젠킨스 마스터 서버에 등록하자.\n1 2 3 4 5 6 7 8 9 10 # 베스천 서버 vi ~/.kube/config --- apiVersion: v1 clusters: - cluster: certificate-authority-data: \u0026lt;auth-data\u0026gt; server: https://api.hanhorang.link name: hanhorang.link contexts: 쿠버네티스 URL 입력 후 접근 파일(config)을 등록하자.\nTest Connection 버튼을 통해 연동 테스트를 진행하자.\n그림과 같이 Connected to kubernetes가 나오면 연동이 완료된 것이다. Save를 통해 Configure Clouds 정보를 저장시키자.\nCI \u0026amp; CD 구성 전 사전 작업 사전 작업으로 추가 구성이 필요합니다. 아래 블로그 글을 참고하여 필요 툴을 설치해주세요. 컨테이너 이미지 레지스트리(harbor) : https://hanhorang31.github.io/post/tech-private-docker/ gitlab \u0026amp; argocd : https://hanhorang31.github.io/post/pkos2-3-gitops/\n구성전 도구 연동 [Gitlab-Jenkins-Harbor] 을 위해 사전 작업이 필요하다.\nGItlab 설정 사용자, 그룹 생성 \u0026amp; 접근 토큰 발급 (사용자 : han / 토큰 glpat-yznZ6zHSNN7UaoSHZ-rr)\nadmin→ users→ 계정생성 비밀번호는 user 생성 후 edit 에서 설정할 수 있다.\nusers→ 사용자 → Impersonation Token을 통해 토큰 발급\n프로젝트 생성 (test) 및 옵션 설정\n프로젝트 → Setting → CI / CD → Devops 파이프라인 설정\nRepository 접근 제어 설정\n프로젝트 → Setting → Repository 접근 아래와 같이 설정\n빌드 코드 작성(예제 코드) 및 Repo에 Push\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 git config --global user.email han@gmail.com git config --global user.name han git add . git commit -m \u0026#34;update code\u0026#34; [main 01f9418] update code 3 files changed, 29 insertions(+) create mode 100644 Dockerfile create mode 100644 app.py create mode 100644 requirements.txt git push origin main Username for \u0026#39;https://gitlab.hanhorang.link\u0026#39;: han Password for \u0026#39;https://han@gitlab.hanhorang.link\u0026#39;: Enumerating objects: 6, done. Counting objects: 100% (6/6), done. Delta compression using up to 2 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (5/5), 824 bytes | 824.00 KiB/s, done. Total 5 (delta 0), reused 0 (delta 0), pack-reused 0 To https://gitlab.hanhorang.link/gitlab-instance-0d144142/test.git 347dcae..01f9418 main -\u0026gt; main Harbor 설정 사용자 생성 (ID : han / PW)\n버킷 생성 (bucket name : aews) 및 버킷 접근 제어 설정(han에 권한 부여)\nJenkins 설정 Credentials 추가 ( Gitlab, Harbor ID \u0026amp; PWD)\nJenkins \u0026amp; gitlab webhook 연동 gitlab 에서 이벤트 발생(코드 변경)시 jenkins에 알려 코드 파이프라인이 바로 실행 될 수 있도록 webhook 을 구성하겠다. 다음과 같이 configure system → gitalb 에서 gitlab 연결을 테스트하여 등록한다.\n해당 메뉴가 보이지 않을 경우 플러그인 gitlab 을 설치하자 이어서 gitlab 과 jenkins 파이프라인을 연결하자. 우선 아래 메뉴에서 파이프라인을 생성하자.\n그 다음 general에서 아래와 같이 설정한 후, [빌드 유발]에서 Secret token 토큰을 발급받고 webhook URL을 복사하자, 복사한 정보는 gitlab webhook 등록에 사용할 것이다.\n확인한 정보값을 gitlab project → setting → webhook 에 입력하여 연동을 진행하자.\nCI \u0026amp; CD 구성 1. CI 파이프라인 구성 위 단계에서 생성한 파이프라인에서 스크립트를 다음과 같이 작성하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 podTemplate(yaml: \u0026#39;\u0026#39;\u0026#39; kind: Pod metadata: name: kaniko-image-build-pod spec: containers: - name: yq image: linuxserver/yq:amd64-latest imagePullPolicy: Always tty : true command: - sleep args: - 99d - name: kaniko image: gcr.io/kaniko-project/executor:v1.6.0-debug imagePullPolicy: Always command: - sleep args: - 99d volumeMounts: - name: docker-config mountPath: /root/.docker tty: true volumes: - name: docker-config secret: secretName: regcred items: - key: .dockerconfigjson path: config.json \u0026#39;\u0026#39;\u0026#39; ) { node(POD_LABEL) { stage(\u0026#39;Build with Kaniko\u0026#39;) { //git tag를 가져오기 위한 clone git branch: \u0026#39;main\u0026#39;, credentialsId: \u0026#39;gitlab\u0026#39;, url: \u0026#39;https://gitlab.hanhorang.link/gitlab-instance-f7cee683/aews.git\u0026#39; script(){ GIT_TAG = sh ( script: \u0026#39;git describe --always\u0026#39;, returnStdout: true ).trim() } //Image build container(\u0026#39;kaniko\u0026#39;) { //kaniko 에서 빌드하기 위해 소스코드 clone git branch: \u0026#39;main\u0026#39;, credentialsId: \u0026#39;gitlab\u0026#39;, url: \u0026#39;https://gitlab.hanhorang.link/gitlab-instance-f7cee683/aews.git\u0026#39; // kaniko 실행 sh \u0026#39;/kaniko/executor -f Dockerfile -c `pwd` --insecure --skip-tls-verify --cache=true --destination=harbor.hanhorang.link/aews/test:\u0026#39; + GIT_TAG } } } } 파이프라인에서 중요하게 볼 점은 도커 빌드를 위한 도구로 kaniko 를 사용한 점이다. kaniko 는 Google Cloud에서 개발한 오픈 소스 도구로 Docker 데몬이 없는 환경에서 Docker 이미지를 빌드할 수 있다. 이 Docker 데몬이 없는 환경 라는 단어가 쿠버네티스에서는 특히 중요하다.\n호환성 제공 : eks 1.24 이상 버전에서 docker 가 deprecated 되어 더 이상 docker 이미지 빌드를 할 수 없다는 점(커스텀 노드면 docker 설치해서 가능하긴 하다)에서 보안 이슈 제거 : Docker 데몬은 루트 권한으로 실행되기 때문에, 악의적인 이미지가 Docker 데몬에 액세스하게 되면 시스템 전체를 위험에 빠뜨릴 수 있다. 이와 반면 Kaniko는 사용자 공간(user-space)에서 실행되므로 이러한 보안 문제를 완화한다. 효율성 : Kaniko는 이미지 빌드 과정을 Kubernetes 팟 내에서 직접 수행하므로, 별도의 빌드 서버를 유지할 필요가 없다. 이는 관리 오버헤드를 줄이고 리소스 사용을 최적화할 수 있어 효율적이다. 이러한 점에서 필자는 kaniko를 사용하여 이미지를 빌드하였다. kaniko에서는 또한 docker login 기능을 제공한다. 베스천 서버에서 harbor 도커레지스트리에 접근하기 위한 시크릿 정보를 생성하고 파이프라인을 빌드하자. 빌드가 정상적으로 처리되면 jenkins에서의 결과 화면과 harbor에서 푸쉬된 이미지를 확인할 수 있다!\n1 2 3 4 5 # 시크릿 생성 kubectl create secret docker-registry regcred -n jenkins \\ --docker-username=admin \\ --docker-password=Harbor12345 \\ --docker-server=harbor.hanhorang.link 2. ArgoCD 를 통한 CD 구성 CI 구성은 이미지 푸쉬가 끝이 아니다. 이미지 푸쉬가 완료되고 배포 차트에 이미지 태그 업데이트까지 시켜줘야 한다. 차트의 이미지 태그를 업데이트해야 Argocd를 통해서 자동으로 쿠버네티스에 배포되기 때문이다. 이를 위해 깃랩에서 새로운 프로젝트를 생성해서 배포 차트를 구성하자. 차트 구성은 필자의 깃 레파지토리를 확인하자. 차트를 확인하면 kustomize 를 통해 차트를 구성한 것을 확인할 수 있다. Kustomize는 기존 YAML 파일을 수정하지 않고, 오버레이와 패치를 통해 구성을 수정하거나 확장하는 기능을 제공하는 도구로서 추후 dev,ops 환경에서 배포를 고려하여 도입하였다.\n1 2 # 설치 curl -s \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash 1 2 3 4 5 6 7 8 9 # kustomize 차트 구성 ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml ├── overlays │ └── dev │ └── kustomization.yaml └── README.md 1 2 3 4 # 배포 # overlays/dev kubectl apply -k . -- 애플리케이션 동작을 확인했으면 argocd를 통해 sync 작업을 진행하겠다.\nArgoCD 에 접속하여 gialab 접근을 위한 시크릿 정보를 입력하겠다. Argocd → Setting → Repo 에서 다음의 사진처럼 k8s-chart 레파지토리와 gitlab 접근 토큰을 입력하자.\n연결이 정상적이면 Succeesful로 표 될 것이다. 이어서 APP sync 를 진행하자. 왼쪽 Application에서 다음과 같이 정보를 입력하자.\nSource : 차트 타켓이 되는 정보를 입력한다. 중요한 것은 Path 인데 실행 경로로 overlays/dev 로 설정하자. Destination : 배포 클러스터 정보의 입력란이다. 다음과 같이 클러스터 서비스 이름과 네임스페이스를 입력하자. 입력이 완료되면 다음과 같이 확인할 수 있다.\n3. CI \u0026amp; CD 연동 드디어 마지막 단계이다! 앞에서 구성한 파이프라인의 stage 단계를 추가하여 CD 구성 차트의 이미지 태그를 업데이트하는 부분을 추가할 것이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Jekins pipeline . . . stage(\u0026#39;Update k8s chart\u0026#39;) { withCredentials([usernamePassword(credentialsId: \u0026#39;gitlab\u0026#39;, usernameVariable: \u0026#39;GITLAB_USERNAME\u0026#39;, passwordVariable: \u0026#39;GITLAB_ACCESS_TOKEN\u0026#39;)]) { dir(\u0026#34;~/\u0026#34;) { git branch: \u0026#39;main\u0026#39;, credentialsId: \u0026#39;gitlab\u0026#39;, url: \u0026#39;https://gitlab.hanhorang.link/han/k8s-chart.git\u0026#39; sh \u0026#39;echo $GITLAB_ACCESS_TOKEN $GITLAB_USERNAME\u0026#39; sh \u0026#39;git config --global credential.helper store\u0026#39; sh \u0026#39;git config --global user.name \u0026#34;han\u0026#34;\u0026#39; sh \u0026#39;git config --global user.email \u0026#34;han@gmail.com\u0026#34;\u0026#39; sh \u0026#39;git remote set-url origin https://han:glpat-yznZ6zHSNN7UaoSHZ-rr@gitlab.hanhorang.link/han/k8s-chart.git\u0026#39; sh \u0026#39;sed -i \u0026#34;s|newTag:.*|newTag: \u0026#39; + GIT_TAG + \u0026#39;|\u0026#34; overlays/dev/kustomization.yaml\u0026#39; sh \u0026#39;git add .\u0026#39; sh \u0026#39;git commit -m \u0026#34;updated the image tag to \u0026#39; + GIT_TAG + \u0026#39;\u0026#34; || true\u0026#39; sh \u0026#39;git push origin main\u0026#39; } } } 트러블슈팅\n해당 단계에서 깃 푸쉬가 안되어 많은 시행 착오를 겪었다. 케이스별로 묶어 정리하니 참고하자.\ngit not\nConsole Output 에서 로그가 다음과 같이 git을 인식하지 못하는 문제이다.\n1 2 3 . . /home/jenkins/agent/workspace/aews-pipeline-test/~@tmp/durable-bda4026d/script.sh: line 1: git: not 근본적인 원인은 git이 stage container 에 설치되지 않았기 때문이다. 커맨드로 git 설치하는 방법도 있지만, 필자의 경우 리소스 오버헤드를 고려하여 jenkins 옵션에서 git을 활성화하였다.\nURL using bad/illegal format or missing URL , fatal: could not read Username for 'https://gitlab.hanhorang.link': No such device or address\ngitlab 원격저장소를 잘못 설정하여 생긴 문제이다. 원인을 찾는 것은 쉬웠지만, 어디서 잘못되었는 지 테스트에 오래걸렸다. 필자의 경우 깃랩 토큰 노출을 고려하여 withCredentials 를 사용하였는데 다음과 같이 **** 로 표시로 원격저장소를 인식하지 못하였다.\n해결 방법으로 하드 코딩이나 환경 변수로 설정하는 것을 추천한다.\nGitops 기반 CI \u0026amp; CD 테스트 앞 단계에서 필자는 CI \u0026amp; CD 구성을 완료하였다. 이제 본격적인 운영 시나리오로 들어가서 GItops 기반에서 CI \u0026amp; CD 프로세스가 정상적으로 작동되는 지 확인해보겠다. 밑의 아키텍처처럼 소스코드 레파지토리에 코드를 수정하여 푸쉬시 jenkins를 통해서 이미지가 새로 푸쉬되고, k8s-chart 의 이미지 태그가 업데이트되어 최종적으로 쿠버네티스 애플리케이션이 수정되는 것을 확인해보겠다.\nCommit Change\n예제 파일 구성시 플라스트 실행 파일에 대해 수정이 필요하여 파일을 수정하고 Commit을 진행하였다.\nJenkins 파이프라인 확인\nCommit 진행시, Webhook를 통해 다음과 같이 자동으로 설정한 파이프라인이 실행된다.\n각 파이프라인의 로그를 확인하면 레파지토리와 k8s-chart에 대해 작업을 수행한 것을 확인할 수 있다.\nArgoCD Sync 확인\nArgoCD 의 설정한 Sync 옵션에 따라 자동으로 k8s에 배포된 것을 확인할 수 있다.\n마치며 이번 주에는 CI / CD 에 대한 본격적인 맛보기로 예제를 들어 gitops ci \u0026amp; cd를 구성하였다. 이것을 기반으로 확장하여 애플리케이션 트래픽 관리를 위한 argo rollout 적용, 개발 \u0026amp; 운영 환경에 대한 관리, 이미지 태그 관리 등등.. 확장 기능들이 많다. 또 나중에 기회가 된다면 정리해서 공유하도록 하겠다!\n","date":"Jun 09","permalink":"https://HanHoRang31.github.io/post/aews-gitops/","tags":["AEWS","kubeflow","cloud","AWS","eksctl","eks","GitOps","Gitlab","ArgoCD","CI/CD","jenkins","kaniko"],"title":"[AEWS] Gitops CI \u0026 CD 구성"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. 지난 글에 이어서 AWS 서비스를 통해서 EKS 보안 구성을 한 내용들을 공유한다. 보안 구성 내용은 EKS 워크샵 내용을 참고하여 정리하였다. 보안 구성 내용은 다음과 같다.\nEKS Secret 시크릿 키 관리 : Sealed Secrets + AWS KMS EKS 이상 탐지 모니터링 : Amazon GuardDuty + 사고 대응 EKS 애플리케이션 트래픽 보호 : AWS WAF EKS Secret Key 관리 참고 : EKS 워크샵, AWS 블로그\nKubernetes Secret은 워크로드의 암호, OAuth 토큰 및 ssh 키 등과 같은 민감한 정보의 배포를 관리하는 리소스이다. Secret은 파드에 볼륨으로 마운트되거나 컨테이너 이미지를 가져오는 데 사용되는 자격 증명으로 사용된다. 그러나 Secret은 기본적으로 Base64 인코딩되어 저장되지만, 이는 실제로 암호화를 제공하는 것은 아니다. 이말은 Git 레파지토리에 시크릿을 업로드하게 되면 그대로 민감 데이터가 노출된다는 뜻으로, 시크릿 키에 대한 추가 관리를 통해 레파지토리에 시크릿 키가 업로드 되도 문제가 없도록 구성해야 한다. 이를 해결하기 위해 본 장에서는 Sealed Secrets + AWS KMS 를 이용하여 쿠버네티스 워크로드의 시크릿 키를 관리하여 레파지토리에서도 시크릿 키를 관리할 수 있도록 보안 구성을 진행할 것이다.\nSealed Secrets 원리 Sealed Secrets는 Kubernetes에서 Secret을 안전하게 관리하기 위한 도구로 시크릿 키에 대해 암호화를 해주는 도구이다. 암호화 원리 이해를 위해 Sealed Secrets 구성을 먼저 확인하면 다음과 같다.\nSealed Secrets 두 가지 파트에서 시크릿 키에 대해 암호화 / 복호화를 하여 암호화된 키 관리를 구성한다.\nA client-side utility (kubeseal ) : CLI 도구로 시크릿 키에 대해 암호화를 진행해준다. 암호화에 대한 공개 키 / 비밀 키는 Cluster-side controller에서 진행한다. A cluster-side controller / operator : 쿠버네티스 클러스터에서 설치되며, 고유한 공개 키/비밀 키 쌍을 생성하여 키 관리를 진행한다. Kubeseal 에서 암호화한 시크릿 키에 대해 복호화를 해준다. 각 파트를 통해 시크릿 키를 관리하는 과정은 다음과 같다.\nhttps://auth0.com/blog/kubernetes-secrets-management/\n(왼쪽 위 컨트롤러) Sealed Secrets 컨트롤러는 클러스터에 설치되며, 고유한 공개 키/비밀 키 쌍을 생성한다. 이 키 쌍은 SealedSecret 리소스를 암호화하고 복호화하는 데 사용된다. (아래 과정) 사용자는 kubeseal CLI 도구를 사용하여 Secret을 SealedSecret으로 변환(암호화)한다. 이 변환은 로컬에서 수행되며, 원본 Secret은 클러스터에 전송되지 않는다. 변환된 암호화된 시크릿 키를 통해 git 레파지토리에 업로드시 안전하게 키를 관리할 수 있다(store) (오른쪽 위 과정) 시크릿 키를 통한 워크로드 배포시 Sealed Secrets 컨트롤러는 SealedSecret 리소스를 감지하고 해당 Secret을 복호화하여 클러스터에 배포한다. Sealed Secrets 설치 Sealed Secrets 를 사용하기 위해서는 Client-Side와 Cluster-Side 의 툴을 각각 설치해야 한다.\nClient-Side 설치 (kubeseal CLI) - 바이너리 확인\n1 2 3 4 # 리눅스 wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.21.0/kubeseal-0.21.0-linux-amd64.tar.gz tar -xvzf kubeseal-0.21.0-linux-amd64.tar.gz sudo install -m 755 kubeseal /usr/local/bin/kubeseal Cluster-Side 설치 (Sealed Secrets Controllers)\n1 kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.21.0/controller.yaml 컨트롤러를 설치하면 컨트롤러 로그를 통해 인증서와 공개 키, 비밀 키를 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 인증서 확인 kubectl logs deployments/sealed-secrets-controller -n kube-system -- controller version: 0.21.0 2023/06/03 02:57:08 Starting sealed-secrets controller version: 0.21.0 Searching for existing private keys New key written to kube-system/sealed-secrets-keynllnv Certificate is -----BEGIN CERTIFICATE----- MIIEzDCCArSgAwIBAgIQCbNur2lY5wvwqAZYH7KKsTANBgkqhkiG9w0BAQsFADAA MB4XDTIzMDYwMzAyNTcxMloXDTMzMDUzMTAyNTcxMlowADCCAiIwDQYJKoZIhvcN AQEBBQADggIPADCCAgoCggIBALqtMiIVQUHCT7EEcTGzRPNOK3CH8hwhOX8gxDcA 3uN0sLYGRhWyHh+tTDf6BeFvT/K44OU1MjpcyityArXkXizXT5G6Xehl5YY5lHJI f/3V71kxzSo/iwj4nn900NbMZ8hhFmd4tm23GjLjx02U6D0frC7qFXfZLy4f+qAO # 키 확인 kubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml --- apiVersion: v1 items: - apiVersion: v1 data: tls.crt: ... # 키 tls.key: ... # 키 kind: Secret metadata: creationTimestamp: \u0026#34;2023-06-03T02:57:12Z\u0026#34; generateName: sealed-secrets-key labels: sealedsecrets.bitnami.com/sealed-secrets-key: active name: sealed-secrets-keynllnv namespace: kube-system resourceVersion: \u0026#34;36062\u0026#34; uid: 178a33bf-14b4-4cde-8687-76e08e5f2b2f type: kubernetes.io/tls kind: List metadata: resourceVersion: \u0026#34;\u0026#34; Sealed Secrets 를 통한 키 관리 시크릿 키 관리를 위한 예제로 mysql를 배포하고 sealed secret 을 통해 mysql 접근 패스워드를 관리해보겠다.\nmysql 예제 배포\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # mysql-depeloyment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mysql labels: app: mysql spec: replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:8.0.26 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-root key: password ports: - containerPort: 3306 1 2 3 4 5 6 7 8 # mysql-secret.yaml apiVersion: v1 kind: Secret metadata: name: mysql-root type: Opaque data: password: cm9vdA== 시크릿 키를 디코딩하면 패스워드는 root 이다 mysql 접근 시크릿에 대하여 sealed secrets를 통해 암호화를 진행하자.\n1 2 3 4 # 로컬에 컨트롤러 인증서 저장하기 kubeseal --fetch-cert \u0026gt; mycert.pem # 저장한 컨트롤러 인증서를 통한 암호화 cat mysql-secret.yaml | kubeseal --cert mycert.pem -o yaml \u0026gt; sealed-secret.yaml 암호화 후 키를 확인하면 다음과 같이 패스워드가 달라진 것을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat sealed-secret.yaml --- apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: creationTimestamp: null name: mysql-root namespace: default spec: encryptedData: password: AgBGUSjNgqCLMGKI/y/IwlhgNVzF/wrUFxx00eS/33Q4ZniweRu9Wfwlu9Pq16uBmECmVBPlN7US/gX0wV/hqxYrfWULTaByDnNsrRhKaRFGg0mD5DHCxFLqzTZKzUMZwYZovPxREcMt/d1yx3tWViVsqnz5VnNZYONeyhaZvU059Tj1EWyvfoow19YZM9V3iiyxraX2VB1luTPSbt9JZ355hfWxgDXUHTsheKOVW8GUC1fBi15pBFJXwovoqa6cQ1YcYOKSslJORf+WcDB836ikgykJiHoFpgcLVPS/3uaR73RBAN93gt52hHRpwqFj7aMczqm4aruz6tB7ugpSpArjYDkBHaO5NuaYoeeIGApqsjMQUc/ASuZGXf8rPBZiQeDS5QFLZR3II7gVyyYYQXwMmOy7BCmzBSbzZhKKq9wmD7/uEVBmmH9yJFcCC+FyfzlUyycsANCAWJO12z0MzdVSNr7Lbhb2GUStq6VDlQLndCn1MDAwKbYtkxdwGB5H4PcSiM1+TNkwpwOY7O59XwcWNAVtfPFMLK62lgTCFm0RjR7M/oJX32yWFLfiAifnYVJuNK8ic33J2fFyQvZ3TxWVsZzryOI3+vsR+cUbMh3DCXfMduJ/VtPHUe3zsbbYVOA/Fs/+MpP9GkqkndvQx6kyLhGnIUOOU4iIR2loAPXIjhq1ZUn5XPY7jbrO27yr0clQxM2B template: metadata: creationTimestamp: null name: mysql-root namespace: default type: Opaque 암호화된 시크릿 키를 배포하면 mysql 이 정상적으로 배포된 것을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 kubectl apply -f sealed-secret.yaml --- sealedsecret.bitnami.com/mysql-root created kubectl get pods -A --- NAMESPACE NAME READY STATUS RESTARTS AGE default mysql-9fd5797cc-l8szh 1/1 Running 0 4m23s default sealed-secrets-855f5fbf78-d2j4b 1/1 Running 0 23m kube-system aws-node-j6nq2 1/1 Running 0 3h18m kube-system aws-node-lg25d 1/1 Running 0 3h18m kube-system coredns-6777fcd775-68cql 1/1 Running 0 3h16m kube-system coredns-6777fcd775-jzxr2 1/1 Running 0 3h16m kube-system ebs-csi-controller-67dccdf78f-65hr5 6/6 Running 0 3h15m kube-system ebs-csi-controller-67dccdf78f-hjgjm 6/6 Running 0 3h15m kube-system ebs-csi-node-4jzmh 3/3 Running 0 3h15m kube-system ebs-csi-node-68n58 3/3 Running 0 3h15m kube-system kube-proxy-8bgrm 1/1 Running 0 3h17m kube-system kube-proxy-wvnfs 1/1 Running 0 3h17m kube-system sealed-secrets-controller-b97869575-d7prq 1/1 Running 0 14m kubectl exec mysql-9fd5797cc-l8szh -it /bin/sh -- mysql -u root -p # root 입력 Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 11 Server version: 8.0.26 MySQL Community Server - GPL Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. Sealed Secrets 키 관리 앞 서 과정을 확인하니 Sealed Secrets 컨트롤러 내 키를 통해 시크릿 키를 암호화 / 복호화하는 것을 확인하였다. 말 그대로 이중으로 암호화를 진행한 것으로 이해하면 되는데 만약 클러스터를 옮긴다고 가정하거나 재해상태로 새로운 Sealed Secrets 컨트롤러를 생성하는 경우가 생긴다면 시크릿 키의 복호화가 안될 것이고 통일성이 깨져 관리가 힘들어질 것이 분명하다. 그렇다면 Sealed Secrets 컨트롤러 키 관리가 필요한데, AWS Systems Manager Parameter Store 를 통해서 키 관리를 위임할 수 있다.\nSealed Secrets 컨트롤러 키는 다음의 명령어를 통해 키를 파일로 저장시킬 수 있다.\n1 kubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml \u0026gt; master-sealing-key.yaml 해당 파일에서 tls.crt 와 tls.key 에 대해 키 관리가 필요하다. AWS Systems Manager Parameter Store 에서 키를 생성하면 되는데 AWS 콘솔 → Systems Manager → Parameter Store 에서 해당 키를 입력하고 저장시키면 된다.\n저장한 키는 다음의 명령어로 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 aws ssm get-parameter --name \u0026#34;master-sealing-key-tls-crt\u0026#34; --with-decryption --- { \u0026#34;Parameter\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;master-sealing-key-tls-crt\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;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\u0026#34;, \u0026#34;Version\u0026#34;: 1, \u0026#34;LastModifiedDate\u0026#34;: \u0026#34;2023-06-03T12:48:22.354000+09:00\u0026#34;, \u0026#34;ARN\u0026#34;: \u0026#34;arn:aws:ssm:ap-northeast-2:955963799952:parameter/master-sealing-key-tls-crt\u0026#34;, \u0026#34;DataType\u0026#34;: \u0026#34;text\u0026#34; } } Amazon GuardDuty AWS 의 지능형 위협 탐지 서비스이다. 이 서비스는 AWS 계정, 워크로드, 그리고 AWS에서 호스팅되는 데이터를 보호하기 위해 고급 머신 러닝을 활용하여 악의적인 활동이나 비정상적인 행동을 탐지하는데 사용된다. 탐지는 다음의 활동에서 식별한다.\n인프라(EKS)와 계정(IAM)을 위협하는 악성 소스로부터의 공격과 위협 계정 내부의 위협, 즉 AWS 리소스를 위협하는 잠재적으로 악의적인 또는 무단 행동 AWS 리소스를 위협하는 알려진 악의적인 활동이나 알려진 비정상 행동 GuardDuty는 AWS 로그 데이터, 예를 들어 AWS CloudTrail, Amazon VPC Flow Logs, DNS 로그 등을 분석하여 이러한 위협을 탐지한다. 별도의 설치도 필요 없으며, 사용자는 수동으로 활성화하거나 관리할 필요 없이 GuardDuty를 사용하여 보안 통찰력을 얻을 수 있다.\nhttps://www.youtube.com/watch?v=t3rVVilJWEk\n활성화도 간단하다. AWS 콘솔에서 GuardDuty로 접속하여 EKS 보호 기능을 활성화시키면 끝이다.\n활성화 이후 결과에서 탐지 결과를 확인할 수 있다.\n다음은 워크샵에서 제공하는 이상 탐지 예제로 몇 가지 예제를 따라해보고 가드 듀티에 적용되는 지 확인해보겠다.\n파드 내에서 이상 실행 명령어 감지\n1 kubectl -n kube-system exec $(kubectl -n kube-system get pods -o name -l app=efs-csi-node | head -n1) -c efs-plugin -- pwd 익명 사용자에 대한 접근 감지 (호출, 정책, 파드 생성)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 EKS_CLUSTER_NAME=\u0026lt;클러스터 이름 입력\u0026gt; AWS_DEFAULT_REGION=ap-northeast-2 # AnonymousAccessGranted kubectl create role pod-create --verb=get,list,watch,create,delete,patch --resource=pods -n default kubectl create rolebinding pod-access --role=pod-create --user=system:anonymous # Discovery: SuccessfulAnonymousAccess API_URL=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query \u0026#34;cluster.endpoint\u0026#34; --region $AWS_DEFAULT_REGION --output text) curl -k $API_URL/api/v1/pods -- { \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: {}, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;pods is forbidden: User \\\u0026#34;system:anonymous\\\u0026#34; cannot list resource \\\u0026#34;pods\\\u0026#34; in API group \\\u0026#34;\\\u0026#34; at the cluster scope\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;pods\u0026#34; }, \u0026#34;code\u0026#34;: 403 } # Impact:Kubernetes/SuccessfulAnonymousAccess API_URL=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query \u0026#34;cluster.endpoint\u0026#34; --region $AWS_DEFAULT_REGION --output text) curl -k -v $API_URL/api/v1/namespaces/default/pods -X POST -H \u0026#39;Content-Type: application/yaml\u0026#39; -d \u0026#39;--- apiVersion: v1 kind: Pod metadata: name: nginx namespace: default spec: containers: - name: nginx image: nginx ports: - containerPort: 80 \u0026#39; -- ... \u0026lt; { \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: {}, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;pods \\\u0026#34;nginx\\\u0026#34; is forbidden: PodSecurityPolicy: unable to admit pod: []\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;pods\u0026#34; }, \u0026#34;code\u0026#34;: 403 * Connection #0 to host 76A102F3C34E6587C0300AFD756C2316.gr7.ap-northeast-2.eks.amazonaws.com left intact 서비스 사용자 계정에 cluster-admin가 부여된 경우 감지\n1 kubectl create rolebinding sa-default-admin --clusterrole=cluster-admin --serviceaccount=default:default --namespace=default 내부 대시보드가 로드밸런서로 변경되어 외부로 서비스가 노출될 때 감지\n1 2 3 4 5 6 # 예제 대시보드 배포 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml # 서비스 타입 로드밸런서로 변경 kubectl patch svc kubernetes-dashboard -n kubernetes-dashboard -p=\u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; 루트 수준 액세스 권한이 있는 컨테이너가 실행 감지\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: ubuntu-privileged spec: selector: matchLabels: app: ubuntu-privileged replicas: 1 template: metadata: labels: app: ubuntu-privileged spec: containers: - name: ubuntu-privileged image: ubuntu ports: - containerPort: 22 securityContext: privileged: true 호스트 경로 볼륨 마운트 감지\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: apps/v1 kind: Deployment metadata: name: ubuntu-privileged spec: selector: matchLabels: app: ubuntu-privileged replicas: 1 template: metadata: labels: app: ubuntu-privileged spec: containers: - name: ubuntu-privileged image: ubuntu ports: - containerPort: 22 securityContext: privileged: true volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # 호스트 볼륨 마운트 path: /etc 예제를 배포하면 다음과 같이 Guardduty 화면에서 이상 감지 결과와 정보를 확인할 수 있다.\n해당 이벤트에 대해 알람 구성도 가능하다. AWS Guardduty 공식 문서를 참고하면 Cloudwatch 에 규칙 생성으로 이벤트를 받아 알람을 설정할 수 있다.\n설정 → 결과 내보내기 옵션을 통해 Cloudwatch 에 이벤트 제공\nCloudwatch → 이벤트 → 규칙 생성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 { \u0026#34;source\u0026#34;: [ \u0026#34;aws.guardduty\u0026#34; ], \u0026#34;detail-type\u0026#34;: [ \u0026#34;GuardDuty Finding\u0026#34; ], \u0026#34;detail\u0026#34;: { \u0026#34;severity\u0026#34;: [ 4, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6, 6.0, 6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 7, 7.0, 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7, 7.8, 7.9, 8, 8.0, 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9 ] } } 해당 코드는 severity 필드가 4.0에서 8.9 사이인 GuardDuty Finding 이벤트를 필터링하도록 설정되어 있다. 구독 SNS 설정하고 규칙을 생성하자\n구성이 완료되면 패턴 기반으로 이벤트가 4~8.9 발생시 SNS로 알람이 오는 것을 확인할 수 있다.\n운영 레벨에서 사용함에 있어 비용적인 측면에서도 고려해야 한다. 23년 6월 기준 30일동안 프리티어로 일정 사용량에 대한 비용을 무료로 제공하고 있다. 문제는 그 다음인데 해당 기능을 위해서는 Cloudtrail, VPC flowlog, EKS runtime 모니터링에 대한 비용을 추가로 산정해야 하기 때문에 도입시 반드시 고려해야 한다.\nhttps://aws.amazon.com/ko/guardduty/pricing/\n사고 대응 참고 : EKS 모범 사례 - 보안 - 사고 대응\nAmazon GuardDuty를 통해 악의적인 활동 및 이상 동작을 모니터링하고 알람을 거는 방법까지 확인하였다. 다음은 알람의 다음 단계로, 실제 보안 사고 발생시 대응할 수 있는 방법들을 정리한다. 실제 사고가 발생하면 영향을 받는 컨테이너를 폐기하고 교체할지 또는 컨테이너를 격리하고 검사할지 결정해야 한다. 말이 결정이지.. 결국 재발 방지를 위해 컨테이너 격리 및 검사(사고 근본 원인 분석 및 포렌식 조사)가 필수적으로 진행되어야 한다. 사고 대응의 과정은 식별 → 격리 → 포렌식 분석 및 재배포 으로 과정별 대응 계획을 공유하겠다.\n식별 가장 먼저 문제가 되는 파드나 노드를 식별하는 것이 중요하다. 식별 방식은 문제 파드나 서비스 계정, 취약 및 손상된 이미지를 기준으로 노드를 식별한다. 아래는 식별 방식별 명령어이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 잘못된 파드 이름과 네임스페이스를 알고 있을 경우, 파드가 실행되는 노드를 식별하는 명령어 kubectl get pods \u0026lt;name\u0026gt; --namespace \u0026lt;namespace\u0026gt; -o=jsonpath=\u0026#39;{.spec.nodeName}{\u0026#34;\\n\u0026#34;}\u0026#39; # 파드가 워크로드(예. 디플로이먼트)로 동작하는 경우 모든 포드와 실행 중인 노드를 나열하는 명령어 selector=$(kubectl get deployments \u0026lt;name\u0026gt; \\ --namespace \u0026lt;namespace\u0026gt; -o json | jq -j \\ \u0026#39;.spec.selector.matchLabels | to_entries | .[] | \u0026#34;\\(.key)=\\(.value)\u0026#34;\u0026#39;) kubectl get pods --namespace \u0026lt;namespace\u0026gt; --selector=$selector \\ -o json | jq -r \u0026#39;.items[] | \u0026#34;\\(.metadata.name) \\(.spec.nodeName)\u0026#34;\u0026#39; # 서비스 계정을 사용하여 문제가 있는 파드나 노드를 식별하는 명령어 kubectl get pods -o json --namespace \u0026lt;namespace\u0026gt; | \\ jq -r \u0026#39;.items[] | select(.spec.serviceAccount == \u0026#34;\u0026lt;service account name\u0026gt;\u0026#34;) | \u0026#34;\\(.metadata.name) \\(.spec.nodeName)\u0026#34;\u0026#39; # 취약하거나 손상된 이미지 및 작업자가 있는 파드나 노드를 식별 명령어 IMAGE=\u0026lt;Name of the malicious/compromised image\u0026gt; kubectl get pods -o json --all-namespaces | \\ jq -r --arg image \u0026#34;$IMAGE\u0026#34; \u0026#39;.items[] | select(.spec.containers[] | .image == $image) | \u0026#34;\\(.metadata.name) \\(.metadata.namespace) \\(.spec.nodeName)\u0026#34;\u0026#39; 격리 문제 노드나 파드를 식별하였으면 해당 리소스에 대한 격리가 필요하다. 격리 과정은 다음과 같다.\n네트워크 정책을 통해 식별 파드의 인그래스 \u0026amp; 이그래스 트래픽을 거부시킨다.\n다음과 같이 문제있는 파드의 레이블(예제 레이블, app:web ) 을 찾아 모든 트래픽을 거부시키는 네트워크 정책을 생성하여 공격을 중지시킬 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: matchLabels: app: web policyTypes: - Ingress - Egress 만약 노드 호스트가 뚫린 경우, AWS 보안 그룹을 통해 식별 노드의 네트워크 인그래스를 막아 다른 호스트로 부터 격리할 수 있다. 다만, 보안 그룹을 변경할 때 실행 중인 모든 컨테이너가 영향을 받으니 사전 테스트를 통해 식별 IP 및 포트를 확인하는 작업이 필요하다.\nIAM 보안 자격 증명 취소를 통해 추가 손상을 방지한다.\n파드가 다른 AWS 리소스에 액세스할 수 있도록 허용하는 IAM 역할이 할당된 경우 추가 손상을 막기 위해 식별 노드에서 해당 IAM 역할을 제거한다. 이때도 다른 워크로드에 영향을 주지 않고 역할에서 IAM 정책을 안전하게 제거할 수 있는 지 평가가 필요하다.\n노드 차단을 통해 더 이상 파드가 노드에 예약하지 않도록 설정한다.\nkubernetes cordon명령어를 통해 노드에 더 이상 파드를 예약하지 않도록 한다. 이는 다른 워크로드에 영향을 주지 않고 포렌식 연구를 하기 위함이다.\n종료 방지 기능을 활성화하여 노드 축소 이벤트로 부터 노드를 보호시킨다.\n해킹 공격으로 이상 감지의 파드를 지워 문제 원인을 지우려고 시도할 수 있다. 이때 ASG 의 종료 방지 기능을 활용하면, 인스턴스 축소 이벤트로 부터 노드를 보호시킬 수 있다.\n포렌식 분석 및 재배포 다음은 원인 분석을 위해 식별 노드에 대한 휘발성 아키텍츠 캡처가 필요하다.\n노드에서 실행 중인 도커 프로세스와 메모리 및 포트 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # 파드별 프로세스 확인 sudo pstree --- systemd─┬─2*[agetty] ├─amazon-ssm-agen─┬─ssm-agent-worke───8*[{ssm-agent-worke}] │ └─8*[{amazon-ssm-agen}] ├─auditd───{auditd} ├─chronyd ├─containerd───11*[{containerd}] ├─containerd-shim─┬─aws-vpc-cni─┬─aws-k8s-agent───8*[{aws-k8s-agent}] │ │ └─4*[{aws-vpc-cni}] │ ├─pause │ └─11*[{containerd-shim}] ... # 파드별 프로세스 및 메모리 확인 ps afxuwww --- USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ... root 5699 0.0 0.3 721420 15284 ? Ssl 05:32 0:01 \\_ /livenessprobe --csi-address=/csi/csi.sock root 5409 0.1 0.2 712480 10924 ? Sl 05:32 0:11 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id eb55d159bad848ea2179132ee8d8d26dcb768aba4098331045b5943fa3981120 -address /run/containerd/containerd.sock ec2-user 5430 0.0 0.0 972 4 ? Ss 05:32 0:00 \\_ /pause ec2-user 5502 0.0 1.0 765196 39908 ? Ssl 05:32 0:01 \\_ /bin/aws-ebs-csi-driver controller --endpoint=unix:///var/lib/csi/sockets/pluginproxy/csi.sock --k8s-tag-cluster-id=hanhorang --logging-format=text --user-agent-extra=eks --v=2 ec2-user 5646 0.0 1.0 755660 42648 ? Ssl 05:32 0:08 \\_ /csi-provisioner --csi-address=/var/lib/csi/sockets/pluginproxy/csi.sock --v=2 --feature-gates=Topology=true --extra-create-metadata --leader-election=true --default-fstype=ext4 ec2-user 5788 0.0 0.9 751060 35884 ? Ssl 05:32 0:03 \\_ /csi-attacher --csi-address=/var/lib/csi/sockets/pluginproxy/csi.sock --v=2 --leader-election=true ec2-user 5852 0.1 0.8 750784 32136 ? Ssl 05:32 0:17 \\_ /csi-snapshotter --csi-address=/var/lib/csi/sockets/pluginproxy/csi.sock --leader-election=true --extra-create-metadata ec2-user 5927 0.0 0.8 751100 34652 ? Ssl 05:32 0:00 \\_ /csi-resizer --csi-address=/var/lib/csi/sockets/pluginproxy/csi.sock --v=2 --handle-volume-inuse-error=false ec2-user 5961 0.0 0.4 721676 16452 ? Ssl 05:32 0:02 \\_ /livenessprobe --csi-address=/csi/csi.sock root 13499 0.0 0.2 712224 10252 ? Sl 05:57 0:01 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id cb6ebee42e28416dce159859edd5c0c2ca894bdae5fab6af4e51f8b26c21dc28 -address /run/containerd/containerd.sock 65535 13521 0.0 0.0 972 4 ? Ss 05:57 0:00 \\_ /pause docker 13651 0.0 0.8 741808 34400 ? Ssl 05:58 0:03 \\_ /dashboard --insecure-bind-address=0.0.0.0 --bind-address=0.0.0.0 --auto-generate-certificates --namespace=kubernetes-dashboard root 14138 0.0 0.2 712480 10652 ? Sl 05:59 0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 8e865c7d2f0dc0a8ca9061aea5f89df362d07d749b8f984884d900ca77845011 -address /run/containerd/containerd.sock 65535 14160 0.0 0.0 972 4 ? Ss 05:59 0:00 \\_ /pause root 9717 0.0 0.2 712480 10028 ? Sl 07:01 0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 76439116dd121ba3c644c38ff9e0b09c01810dfff8c5adbeb41812abe9871ce7 -address /run/containerd/containerd.sock 65535 9738 0.0 0.0 972 4 ? Ss 07:01 0:00 \\_ /pause root 9808 0.0 0.1 7880 4512 ? Ss 07:01 0:00 \\_ nginx: master process nginx -g daemon off; 100 9820 0.0 0.0 8336 2200 ? S 07:01 0:00 \\_ nginx: worker process 100 9821 0.0 0.0 8336 1548 ? S 07:01 0:00 \\_ nginx: worker process # 포트 확인 sudo netstat -tulnp --- Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN 2940/kubelet tcp 0 0 127.0.0.1:61679 0.0.0.0:* LISTEN 3431/./aws-k8s-agen tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 1821/rpcbind tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 2419/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2262/master tcp 0 0 127.0.0.1:46847 0.0.0.0:* LISTEN 2826/containerd tcp 0 0 127.0.0.1:50051 0.0.0.0:* LISTEN 3431/./aws-k8s-agen tcp6 0 0 :::10249 :::* LISTEN 4213/kube-proxy tcp6 0 0 :::10250 :::* LISTEN 2940/kubelet tcp6 0 0 :::61678 :::* LISTEN 3431/./aws-k8s-agen tcp6 0 0 :::111 :::* LISTEN 1821/rpcbind tcp6 0 0 :::10256 :::* LISTEN 4213/kube-proxy tcp6 0 0 :::22 :::* LISTEN 2419/sshd udp 0 0 0.0.0.0:68 0.0.0.0:* 2064/dhclient udp 0 0 0.0.0.0:111 0.0.0.0:* 1821/rpcbind udp 0 0 127.0.0.1:323 0.0.0.0:* 1848/chronyd udp 0 0 0.0.0.0:720 0.0.0.0:* 1821/rpcbind udp6 0 0 :::111 :::* 1821/rpcbind udp6 0 0 ::1:323 :::* 1848/chronyd udp6 0 0 fe80::e2:4aff:fe3e::546 :::* 2099/dhclient udp6 0 0 :::720 :::* 1821/rpcbind 작업자 노드에서 컨테이너가 변경되기 전에 docker 명령을 실행하여 도커에 대한 설정 수집\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # docker ## 실행 컨테이너 확인 docker container top $CONTAINER ## 로그 확인 docker container logs $CONTAINER ## 포트 확인 docker container port $CONTAINER ## 파일 및 디렉토리 변경 사항 캡처 docker container diff $CONTAINER # containerd 1.24 이상 sudo nerdctl ps # 컨테이너 리스트 확인 sudo nerdctl logs [container-id] # 컨테이너 로그 확인 sudo nerdctl diff [container-id] # 컨테이너와 원본 이미지 간의 차이점 확인 포렌식 캡처를 위해 컨테이너를 일시중지하고, 인스턴스의 EBS 볼륨을 스냅샷하자.\n노드 IP를 식별하여 해당 노드에 연결된 볼륨에 들어가 스냅샷을 생성하도록 하자.\n손상된 포드 또는 워크로드 리소스 재배포\n분석을 위한 데이터를 수집한 후에는 손상된 포드 또는 워크로드 리소스를 재배포 할 수 있다. 수정 사항을 롤아웃하고 새 교체 파드로 시작한 다음,취약 파드를 삭제하면 끝난다.\n침해 대응 시뮬레이션 민첩한 사고 대응을 위해서는 사전의 침해 시뮬레이션이 필요하다. 시뮬레이션을 위한 유틸리티로 https://github.com/cyberark/kubesploit 가 있는데 아래 침해 스택에 따라 모듈을 구성하여 침해 방식을 테스트한다.\nhttps://github.com/cyberark/kubesploit/blob/assets/mitre_pic_full.png\n공격 스택이 별로 없는 것 같지만,, k8s 기반의 침해 시뮬레이션 툴 중에 가장 별이 많으며, 그나마 많은 침해 방식 기능을 제공한다. 또한, 깃허브 공식 문서에서는 침해 스택에 따른 대응 방식도 제공하니 나중에 침해 대응 시뮬레이션 진행시 참고하도록 하자.\nAWS WAF을 통한 애플리케이션 트래픽 보호 참고 :One Observability Workshop\nAWS WAF 는 웹 애플리케이션을 일반적인 웹 공격으로부터 보호하는 보안 서비스이다. 이 서비스는 공격자가 취약점을 이용하여 애플리케이션에 접근하거나 데이터를 유출하는 것을 막는다. (참고 : ChatGPT)\nAWS WAF는 다음과 같은 주요 기능을 제공한다:\n사용자 정의 보안 규칙: AWS WAF는 사용자 정의 필터를 생성하여 웹 요청을 허용하거나 차단할 수 있다. 이 필터는 IP 주소, HTTP 헤더, HTTP 바디, URI 문자열, SQL 삽입 공격, 스크립트 교차 사이트 공격 (XSS) 등에 기반한다. 비용 효율적인 보안: AWS WAF는 수요에 따라 가격이 책정되므로, 사용자는 실제로 사용하는 만큼만 비용을 지불한다. 실시간 지표: AWS WAF는 AWS CloudWatch와 통합하여 웹 트래픽에 대한 실시간 지표를 제공한다. 이를 통해 사용자는 웹 트래픽의 트렌드를 식별하고 필요한 보안 조치를 즉시 취할 수 있다. 보안 자동화: AWS WAF는 API를 통해 보안을 자동화할 수 있다. 이는 룰의 업데이트를 자동화하고, 새로운 애플리케이션에 보안 설정을 자동으로 적용하는 데 사용할 수 있다. AWS WAF는 Amazon CloudFront, AWS Application Load Balancer, Amazon API Gateway 등 다른 AWS 서비스와 함께 사용될 수 있어, 사용자의 AWS 웹 애플리케이션을 보호하는 데 유용하다. EKS 환경에서도 애플리케이션을 ALB로 구성하며 여기에 WAF를 연결하여 유입 트래픽에 대한 보안 구성을 설정할 수 있다.\nWAF 기능 확인을 위한 선행 작업으로 EKS 클러스터에서 ALB 컨트롤러와 예제 배포가 필요하다. 다음의 명령어를 통해 진행하도록 하자.\nWAF 구성 ALB 컨트롤러 설치\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # ALB controller 정책 설치 curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.7/docs/install/iam_policy.json aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json # 정책 arn 생성 확인 ACCOUNT_ID=000000000000 aws iam get-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --query \u0026#39;Policy.Arn\u0026#39; --- \u0026#34;arn:aws:iam::000000000000:policy/AWSLoadBalancerControllerIAMPolicy\u0026#34; # OIDC 서비스 어카운트 생성 CLUSTER_NAME=hanhorang eksctl create iamserviceaccount --cluster=$CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller \\ --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve # OIDC 서비스 어카운트 확인 kubectl edit sa/aws-load-balancer-controller -n kube-system --- apiVersion: v1 kind: ServiceAccount metadata: annotations: # annoation 에 role-arn이 추가된 것을 확인! eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/eksctl-hanhorang-addon-iamserviceaccount-kub-Role1-1MPK8ATNJVXQH creationTimestamp: \u0026#34;2023-05-08T05:32:22Z\u0026#34; labels: app.kubernetes.io/managed-by: eksctl name: aws-load-balancer-controller namespace: kube-system resourceVersion: \u0026#34;1236\u0026#34; uid: 5fe3e69b-8cfc-426c-a16a-509e1f7c4a86 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # ALB 설치 helm repo add eks https://aws.github.io/eks-charts helm repo update helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$CLUSTER_NAME \\ --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller # ALB 파드 확인 kubectl get pods -A --- NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-load-balancer-controller-7857849d69-qc9nr 1/1 Running 0 24m kube-system aws-load-balancer-controller-7857849d69-qjvkk 1/1 Running 0 24m # ALB 로그 확인 kubectl logs pods/aws-load-balancer-controller-7857849d69-qc9nr -n kube-system --- {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;version\u0026#34;,\u0026#34;GitVersion\u0026#34;:\u0026#34;v2.5.1\u0026#34;,\u0026#34;GitCommit\u0026#34;:\u0026#34;06abaed66e17a411ba064f34e6018b889780ac66\u0026#34;,\u0026#34;BuildDate\u0026#34;:\u0026#34;2023-04-17T22:36:53+0000\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.metrics\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Metrics server is starting to listen\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;:8080\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;setup\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;adding health check for controller\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/mutate-v1-pod\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/mutate-v1-service\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/validate-elbv2-k8s-aws-v1beta1-ingressclassparams\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/mutate-elbv2-k8s-aws-v1beta1-targetgroupbinding\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/validate-elbv2-k8s-aws-v1beta1-targetgroupbinding\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/validate-networking-v1-ingress\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;setup\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;starting podInfo repo\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:47Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook.webhooks\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Starting webhook server\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:47Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Starting server\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;health probe\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;[::]:61779\u0026#34;} ... Ingress 예제 배포\n1 2 3 4 5 6 7 8 9 # game-2048 배포 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/3/ingress1.yaml cat ingress1.yaml | yh kubectl apply -f ingress1.yaml kubectl get ingress -A --- NAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE game-2048 ingress-2048 alb * k8s-game2048-ingress2-91307666f8-2009146712.ap-northeast-2.elb.amazonaws.com 80 24s 해당 서비스에 WAF를 연결하겠다. AWS 콘솔에서 WAF 접근하여 새로운 서비스를 추가하도록 하자. 아쉽지만 WAF 콘솔 진입시 지역이 글로벌로 바뀌어서 그런가 한글 지원이 안된다.\n추가 리소스에서 위에서 생성한 ALB를 연결하자.\nAWS WAF에서 자체적으로 트래픽 보호를 위한 규칙을 제공한다.. 하지만 아래 그림과 같이 사용에 비용이 발생하니 참고하자. 규칙 또한 다양하고 구성시 설명이 상세하게 나오니 참고하자. 규칙 키워드로 정리하면 IP 주소 차단, SQL Injection 공격 차단, Cross-Site Scripting (XSS) 공격 차단, 유저 봇 차단, 크기 제한 규칙, 지역 블랙리스트, HTTP 헤더, HTTP 메소드, 문자열 매칭, 반복 요청 등이 있고 세부적으로 커스터마이징이 가능하다.\nWAF 관측 WAF을 구성하면 메트릭과 로깅 정보를 확인할 수 있다. 메트릭은 기본적으로 제공하지만, 로깅은 WAF 내 기능 탭 Logging and metrics 에서 로깅 기능을 활성화(로그 그룹 생성시 네이밍 aws-waf-logs- 로 시작해야 인식된다.)해야 한다. 활성화를 하면 로깅 관련 쿼리도 제공된다.\nWAF 기능 확인 기본적으로 WAF 의 설정된 룰에 조건이 도달되면 해당 IP는 접근이 블록(차단)된다. 상황에 따라 다르지만, 기본적으로 WAF는 각 규칙이 엄격하게 구성되어 있어 원치않는 차단 IP가 나올 수 있다. 이를 대비하기 위해 옵션이 제공되는 데 블록 대신 Count로 모니터링할 수 있는 기능을 제공한다. 기능 탭 Rule에서 규칙을 클릭하고 설정 부분에서 해당 옵션을 활성화시키면 된다.\nWAF에서 임계값에 대한 알람 구성 또한 가능하다. Cloudwatch 에서 알람 생성에서 WAF에서 확인한 메트릭 값에 대한 알람 설정을 손 쉽게 할 수 있다.\nWAF는 CloudWatch Contributor Insights와 연계하여 특정 조건의 상위 N 기여자를 가져와 대시보드에서 시각화하는 기능을 제공한다. 이를 통해서 상위 차단 IP 주소를 시각화하여 확인할 수 있다. Cloudwatch → contributor Ingihts → 규칙 생성으로 WAF에서 생성한 로그를 지정하고 로그 개수에 대한 필터링을 집계 설정하여 대시보드 구성하면 다음과 같이 확인할 수 있다.\n트래픽 테스트를 할 수 없어서 워크샵 결과 화면을 대체한다. 앞 서 생성한 규칙 키에서 설정한 값에 대한 로그 값을 합계하여 Count 값을 확인할 수 있다.\n","date":"Jun 02","permalink":"https://HanHoRang31.github.io/post/eks-security-allinone/","tags":["KANS","kubeflow","cloud","AWS","eksctl","eks","security","SealedSecrets"],"title":"[AEWS] AWS 서비스를 통한 EKS 보안 구성"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. 스터디도 어느덧 6주차다. 6주차에는 EKS 보안을 주제로 모임장님이 스터디를 진행해주셨다. 스터디 내용으로 EKS 인증 / 인가와 IRSA의 원리를 소개해주셨는데.. 개인적으로 여러 번 돌려볼 정도로 내용이 많이 어려웠다.\n필자 스스로 보안이 다른 분야에 비해 미숙한 부분도 있고,, 아는 내용도 별로 없기 떄문이라 본다. 아마 필자에게 EKS 보안에 대해 소개해달라고 요청받으면 인증 → 인가 → RBAC 수준으로 밖에 답을 못할 것이라 예상한다. 이번 기회에 EKS 보안 원리를 손 쉽게 학습할 수 있는 기회가 생겨 운이 좋았다고 생각한다. 이번 블로그 글에서는 EKS 보안에 대해 필자가 학습 내용을 공유할 것이다. 스터디에서 학습한 EKS 인증/인가, IRSA 에 대한 내용과 워크샵에서 학습한 AWS 서비스를 통한 EKS 보안 구성이 주가 될 예정이다.\n쿠버네티스 보안 체계 https://kubetm.github.io/k8s/07-intermediate-basic-resource/authentication/\n쿠버네티스에서는 API 서버가 인증→ 인가→ Admission Control을 통해 클러스터 접근에 대한 보안을 관리 및 제어한다.\n인증(Authentication): 사용자의 신원을 확인하는 단계이다. 쿠버네티스에서는 X.509 Client Cert 를 쿠버네티스 접근 파일(kubeconfig)에 저장시키거나 서비스 어카운트를 통해 인증 작업을 수행한다. 인가(Authorization): 사용자가 시스템의 특정 리소스에 접근하거나 특정 작업을 수행할 권한이 있는지확인하는 단계이다. 쿠버네티스에서는 Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC), Webhook, Node Auththorization 등의 메커니즘을 통해 인가 작업을 수행한다. Admission Control: 인증, 인가 요청을 처리하기 전에 요청을 검사하고 수정하거나 거부하는 일련의 플러그인 단계이다. Mutating Admission Webhooks, Validating Admission Webhooks 을 통해 요청된 오브젝트를 수정하여 추가 보안 플러그인(IRSA)에 확장 연결시켜주는 작업을 수행한다. EKS 인증/인가 EKS에서 인증/인가 작업은 나눠서 진행된다. 인증은 AWS IAM, 인가는 k8s RBAC이 수행한다. 각 작업의 대한 세부 동작은 다음 그림과 같이 진행된다.\nhttps://awskoreamarketingasset.s3.amazonaws.com/2022 Summit/pdf/T10S1_EKS 환경을 더 효율적으로 더 안전하게.pdf\n이해하기 쉽지 않다! 필자 또한 여러번 영상을 돌려보고 실습을 통해 겨우 이해했다. 쉬운 이해를 위해 직접 해당 과정을 실습해보도록 하겠다. 그림에 순서도가 나와있지 않지만 왼쪽 Authentication(인증) → 아래 Authorization(인가) 작업을 통해 진행된다. 여기서 봐야할 점은 인증 작업에서의 AWS IAM 와 연계 작업이다.\nhttps://awskoreamarketingasset.s3.amazonaws.com/2022 Summit/pdf/T10S1_EKS 환경을 더 효율적으로 더 안전하게.pdf\n인증 과정을 IAM 와 연계해서 확인해보자면 다음과 같이 요약할 수 있다.\nIAM Authenticator Client 에서 토큰을 발급받고 → 발급받은 토큰을 IAM Authenticator Server에 요청하여 인증 작업을 수행한다.\n토큰 발급\n앞 서 과정에서 IAM Authenticator Client가 토큰을 발급해주는 객체인 것을 확인하였다. 그렇다면 토큰 발급은 누가 요청하는 것일까? eks 접근 파일인 kubeconfig를 확인하면 이를 확인할 수 있다 .\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cat ~/.kube/config | yh --- ... - name: terraform-eks@hanhorang.ap-northeast-2.eksctl.io user: exec: apiVersion: client.authentication.k8s.io/v1beta1 args: - eks - get-token - --output - json - --cluster-name - hanhorang - --region - ap-northeast-2 command: aws env: - name: AWS_STS_REGIONAL_ENDPOINTS value: regional provideClusterInfo: false 위의 커맨드를 입력하면 그대로 입력하면 아래와 같이 토큰을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 aws eks get-token --cluster-name $CLUSTER_NAME | jq --- { \u0026#34;kind\u0026#34;: \u0026#34;ExecCredential\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;client.authentication.k8s.io/v1beta1\u0026#34;, \u0026#34;spec\u0026#34;: {}, \u0026#34;status\u0026#34;: { \u0026#34;expirationTimestamp\u0026#34;: \u0026#34;2023-06-03T07:17:27Z\u0026#34;, \u0026#34;token\u0026#34;: \u0026#34;k8s...\u0026#34; } } 발급 정보를 확인하면 expirationTimestamp 필드를 확인할 수 있는데 해당 토큰의 수명이다. 이는 발급받은 토큰이 임시 보안 자격 증명으로 활용되는 것을 확인할 수 있다. 발급받은 토큰을 jwt에서 확인하면 토큰 정보가 다음과 같이 구성됨을 확인할 수 있다.\nhttps://jwt.io/\nPAYLOAD 정보를 확인하면 해당 정보가 AWS API 호출시의 필드와 유사한 것을 확인할 수 있다. 자세히보면, AWS sts(임시자격증명 제공자)에 getcallerIdentity(토큰발급)을 요청(Action)한 것을 확인할 수 있다.\n토큰 리뷰\n발급받은 토큰을 IAM Authenticator Server에 요청하면 인증 작업이 완료된다. 토큰 리뷰 요청은 API 서버에서 진행되어 과정에 대해 세부 확인을 못하지만 AWS CloudTrail 콘솔에서 리뷰 작업을 확인할 수 있다.\n인가\n인증 작업이 완료되면 User/Role에 대한 ARN을 반환하게 되고 해당 ARN에 대한 RBAC 작업을 수행한다. 인가 작업에 대한 설정은 aws-auth 컨피그 맵에서 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl get cm -n kube-system aws-auth -o yaml | kubectl neat | yh -- apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::-:role/eksctl-hanhorang-nodegroup-ng1-NodeInstanceRole-1BT7EJGDT32FR username: system:node:{{EC2PrivateDNSName}} kind: ConfigMap metadata: name: aws-auth namespace: kube-system mapRoles에서 rolearn이 인증작업에서 반환된 arn이 입력되며 groups에서 설정한 역할에 바인딩되어 RBAC처리가 진행된다. RBAC 확인은 krew 플러그인 설치를 통해 자세히 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 플러그인 설치 kubectl krew install access-matrix rbac-tool rbac-view rolesum # EKS 설치한 IAM user 정보 kubectl rbac-tool whoami -- {Username: \u0026#34;kubernetes-admin\u0026#34;, UID: \u0026#34;aws-iam-authenticator:955963799952:AIDA55E7B7GIFCJDANOIO\u0026#34;, Groups: [\u0026#34;system:masters\u0026#34;, \u0026#34;system:authenticated\u0026#34;], Extra: {accessKeyId: [\u0026#34;AKIA55E7B7GIPMN3X7NP\u0026#34;], arn: [\u0026#34;arn:aws:iam::955963799952:user/terraform-eks\u0026#34;], canonicalArn: [\u0026#34;arn:aws:iam::955963799952:user/terraform-eks\u0026#34;], principalId: [\u0026#34;AIDA55E7B7GIFC # system:master RBAC 확인 kubectl rbac-tool lookup system:masters --- W0603 16:28:17.218380 9877 warnings.go:67] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ SUBJECT | SUBJECT TYPE | SCOPE | NAMESPACE | ROLE +----------------+--------------+-------------+-----------+---------------+ system:masters | Group | ClusterRole | | cluster-admin # role에 할당된 cluster-admin 확인 kubectl describe clusterrole cluster-admin --- Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- *.* [] [] [*] [*] [] [*] aws-auth 컨피그맵에는 클러스터 생성 IAM 사용자(kubernetes-admin) 의 정보가 없는 것을 확인할 수 있는데 보안 이슈(탈취 및 삭제) 로 AWS 가 자체적으로 숨겨둔 것 같다. EKS 에 사용자 추가하기 스터디에서 공유해주신 실습 예제로 EKS 클러스터 관리(모든 리소스 읽기 권한 부여)를 위한 사용자를 추가하고 새 베스천 서버에서 접근이 가능한 지 확인해보겠다.\n[베스천 서버-1] IAM 사용자 추가 및 클러스터 접근 권한 부여\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # testuser 사용자 생성 aws iam create-user --user-name testuser # 사용자에게 프로그래밍 방식 액세스 권한 부여 aws iam create-access-key --user-name testuser -- { \u0026#34;AccessKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;AccessKeyId\u0026#34;: \u0026#34;AKIA55E7B7GIOXM4OAVI\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;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\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2023-06-03T07:38:08+00:00\u0026#34; } } # testuser 사용자에 IAM 정책을 추가 aws iam attach-user-policy --policy-arn arn:aws:iam::aws:policy/AdministratorAccess --user-name testuser # 구별을 위해 베스천 서버 1의 arn 확인 \u0026#34;arn:aws:iam::0000000000:user/hanohnrag-eks\u0026#34; 다음은 클러스터 RBAC 권한(모든 리소스 읽기 권한) 을 생성하여 group:user 에 바인딩하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: readonly rules: - apiGroups: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: readonly-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: readonly subjects: - kind: Group name: user apiGroup: rbac.authorization.k8s.io 생성한 Group에 IAM 유저(testuser)를 매핑하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 eksctl create iamidentitymapping --cluster $CLUSTER_NAME --username testuser --group user --arn arn:aws:iam::$ACCOUNT_ID:user/testuser -- 2023-06-03 16:59:25 [ℹ] checking arn arn:aws:iam::0000000000:user/testuser against entries in the auth ConfigMap 2023-06-03 16:59:25 [ℹ] adding identity \u0026#34;arn:aws:iam::0000000000:user/testuser\u0026#34; to auth ConfigMap # 매핑 확인 kubectl get cm -n kube-system aws-auth -o yaml | kubectl neat | yh -- apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::0000000000:role/eksctl-hanhorang-nodegroup-ng1-NodeInstanceRole-1BT7EJGDT32FR username: system:node:{{EC2PrivateDNSName}} mapUsers: | - groups: - user userarn: arn:aws:iam::0000000000:user/testuser username: testuser [베스천 서버-2] test-user 권한 부여 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # IAM ARN 확인 aws sts get-caller-identity --query Arn --- Unable to locate credentials. You can configure credentials by running \u0026#34;aws configure\u0026#34;. # testuser 액세스 키 입력 aws configure -- AWS Access Key ID [None]: AKIA55E7B7GIOXM4OAVI AWS Secret Access Key [None]: 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 Default region name [None]: ap-northeast-2 Default output format [None]: json # 확인 aws sts get-caller-identity --query Arn --- \u0026#34;arn:aws:iam::955963799952:user/testuser\u0026#34; # 접근 파일 업데이트 aws eks --region ap-northeast-2 update-kubeconfig --name hanhorang # 확인 kubectl get pods -A --- NAMESPACE NAME READY STATUS RESTARTS AGE default mysql-9fd5797cc-d7r2g 1/1 Running 0 4h47m default sealed-secrets-855f5fbf78-d2j4b 1/1 Running 0 5h26m kube-system aws-node-j6nq2 1/1 Running 0 8h kube-system aws-node-lg25d 1/1 Running 0 8h kube-system coredns-6777fcd775-68cql 1/1 Running 0 8h kube-system coredns-6777fcd775-jzxr2 1/1 Running 0 8h kube-system ebs-csi-controller-67dccdf78f-65hr5 6/6 Running 0 8h kube-system ebs-csi-controller-67dccdf78f-hjgjm 6/6 Running 0 8h kube-system ebs-csi-node-4jzmh 3/3 Running 0 8h kube-system ebs-csi-node-68n58 3/3 Running 0 8h kube-system kube-proxy-8bgrm 1/1 Running 0 8h kube-system kube-proxy-wvnfs 1/1 Running 0 8h kube-system sealed-secrets-controller-b97869575-d7prq 1/1 Running 0 5h17m IAM User로 할당했지만 role로 할당도 가능하다. role로 할당하면 다수의 사용자에게 권한 부여 관리가 간편해지므로 추가로 작성한다.\ntest-role 생성\nrole과 쿠버네티스 RBAC 매핑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 매핑 eksctl create iamidentitymapping \\ --cluster $CLUSTER_NAME \\ --username testuser \\ --group user \\ --arn arn:aws:iam::$ACCOUNT_ID:role/test-role # 매핑 확인 kubectl get cm aws-auth -o yaml -n kube-system -- ... - groups: - user rolearn: arn:aws:iam::00000000000:role/test-role username: testuser 접근파일 업데이트 후 kubectl 사용\n접근 파일 업데이트 후 kubectl 사용 시 아래 와 같은 문제로 쿠버네티스에 접근이 안된다.\n1 2 3 4 5 aws eks --region ap-northeast-2 update-kubeconfig --name hanhorang --role-arn arn:aws:iam::955963799952:role/test-role kubectl get pods -A -- An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:iam::955963799952:user/testuser is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::955963799952:role/test-role 이유는 role의 신뢰 관계를 업데이트하지 않았기 때문인데 iam role에서 신뢰관계에 유저를 추가하면 정상적으로 작동한다.\n이번 예제에서는 사용자에게 role를 부여하여 클러스터에 접근했으나, role 을 베스천서버에 위임하여 사용할 수 있다.\nEKS 인증/인가 한계 사용하기 편하지만, 최소 권한 부여 원칙에 위배되어 보안상 권고하지 않는다. 이유는 파드가 뚫리면 메타데이터를 활용하여 노드에 부여된 IAM 정책을 이용할 수 있기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 메타데이터를 통해 토큰 발급 후 조회 TOKEN=$(curl -s -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34;) echo $TOKEN # 메타 정보를 통해 임시 접근 키 부여 curl -s -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; –v http://169.254.169.254/latest/meta-data/iam/security-credentials/eksctl-myeks-nodegroup-ng1-NodeInstanceRole-1DC6Y2GRDAJHK -- { \u0026#34;AccessKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;AccessKeyId\u0026#34;: \u0026#34;AKIA55E7B7GIOXM4OAVI\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;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\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2023-06-03T07:38:08+00:00\u0026#34; } } 파드가 뚫리면 노드에 등록된 IAM 정책의 권한이 다 뚫리므로 치명적이다. 이에 대해 새로운 인증/인가의 접근 방법(IRSA)이 필요하다.\nIRSA IRSA(Iam Role for Service Account)는 쿠버네티스 서비스 어카운트에 IAM role을 부여하여 AWS 서비스에 대한 사용을 제어하는 방법이다. 기존의 EKS 인증/인가 체계와는 반대로로 서비스 어카운트(RBAC) 단계에서 IAM role을 부여하여 IAM OIDC를 통해 인증/인가가 진행되는 방식이다.\nIRSA에 대한 원리는 다음의 그림을 통해 확인할 수 있다. 핵심은Admisstion Control 이다. Admisstion Control 의 Webhook으로 확장 API를 연결해서 IAM 에서 인증/인가를 대신 진행시켜준다라고 보면 된다.\nhttps://awskoreamarketingasset.s3.amazonaws.com/2022 Summit/pdf/T10S1_EKS 환경을 더 효율적으로 더 안전하게.pdf\n위 MutatingWebhook 이 Admisstion Control 의 구성 중 하나로 사용자가 요청한 request에 대해 관리가 임의로 값을 변경하는 해준다. 그림에 따라서 요청에 대해 환경 변수(AWS_ROLE_ARN, AWS_WEB_IDENTITY_TOKEN_FILE)와 토큰 데이터(볼륨) 을 추가하여 STS에서 인증과 인가 작업을 수행하는 것을 확인할 수 있다.\nhttps://tech.devsisters.com/posts/pod-iam-role/\n위 과정에서 OIDC IdP 란 OIDC Identity Provider (IdP)는 사용자를 대신 인증하는 서비스이다. 예를 들어, 구글과 페이스북에서 로그인을 하면 여러 앱이나 사이트에 접근할 수 있는 과정이라고 이해하면 된다. 여기서는 EKS 가 OIDC Identity Provider를 사용하여 쿠버네티스 서비스 어카운트에 IAM 역할을 부여하는 역할을 담당한다. 이를 통해 쿠버네티스 내부에서 실행되는 워크로드에서 AWS 리소스에 대한 접근 권한을 부여할 수 있게 된다.\n이 말은 MutatingWebhook 를 다른 보안 툴과 확장하여 사용할 수 있다는 말이다. 다른 보안 툴(dex, teleport, kube-oidc-prxo) 인증 및 액세스 관리 대체 할 수 있으므로 멀티 클러스터 구성시 활용해볼 수 있다.\ndex 를 인증 툴로 사용하기 https://aws.amazon.com/ko/blogs/containers/using-dex-dex-k8s-authenticator-to-authenticate-to-amazon-eks/ 이에 대한 실습은 이미 진행해왔다. 블로그 글에서 여러 번 다룬 ALB 컨트롤러 설치나 externalDNS설치시 IRSA를 활용했기 때문이다. 예제를 다시 확인하면 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # ALB controller 정책 설치 curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.7/docs/install/iam_policy.json aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json # 정책 arn 생성 확인 ACCOUNT_ID=000000000000 aws iam get-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --query \u0026#39;Policy.Arn\u0026#39; --- \u0026#34;arn:aws:iam::000000000000:policy/AWSLoadBalancerControllerIAMPolicy\u0026#34; # OIDC 서비스 어카운트 생성 CLUSTER_NAME=hanhorang eksctl create iamserviceaccount --cluster=$CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller \\ --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve # OIDC 서비스 어카운트 확인 kubectl edit sa/aws-load-balancer-controller -n kube-system --- apiVersion: v1 kind: ServiceAccount metadata: annotations: # annoation 에 role-arn이 추가된 것을 확인! eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/eksctl-hanhorang-addon-iamserviceaccount-kub-Role1-1MPK8ATNJVXQH creationTimestamp: \u0026#34;2023-05-08T05:32:22Z\u0026#34; labels: app.kubernetes.io/managed-by: eksctl name: aws-load-balancer-controller namespace: kube-system resourceVersion: \u0026#34;1236\u0026#34; uid: 5fe3e69b-8cfc-426c-a16a-509e1f7c4a86 eksctl create iamserviceaccount : 해당 명령어를 통해서 서비스 어카운트를 생성하고, IAM role에 정책을 부여하고 신뢰 관계를 설정시켜준다. OIDC Idp 에 등록된 과정은 서비스 어카운트에 annotations를 확인하면 된다. 해당 필드에 role-arn이 정상적으로 부여된 것을 확인할 수 있고 sts를 통해서 해당 role을 확인하여 서비스에 접근/제어를 해준다고 보면 된다. IRSA 도 잘 써야 한다 IRSA도 사용에 주의가 필요하다. 마찬가지로 정보 탈취(토큰과 role Arn)가 되면 임시 자격 증명이 되기 때문이다. role arn은 서비스 어카운트에서 쉽게 확인이 가능하며, 토큰은 IRSA 과정에서 환경 변수로 마운트되기 때문에 다음의 경로에서 확인이 가능하다. (아래 내용은 블로그 글을 참고하여 작성한다. )\n해당 토큰을 jwt 사이트에서 토큰을 분석하면 서비스 계정과 OIDC엔드 포인트를 확인할 수 있다.\n여기서 문제가 발생한다. AWS는 JWT 토큰의 유효성만 확인하지, 토큰 파일이나 서비스 계정에 지정된 IAM role에 대해 일관성을 보장하지 않기 때문에 아래와 같이 외부나 새로운 환경에서 AWS에 대한 자격 증명 발급이 가능하다!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 aws sts assume-role-with-web-identity \\ --role-arn arn:aws:iam::ACCOUNT_ID:role/eks-workshop-carts-dynamo \\ --role-session-name test --web-identity-token eyJhbGciOiJSUzI1\u0026lt;SNIP\u0026gt; { \u0026#34;Credentials\u0026#34; : { \u0026#34;AccessKeyId\u0026#34; : \u0026#34;ASIA\u0026lt;SNIP\u0026gt;\u0026#34; , \u0026#34;SecretAccessKey\u0026#34; : \u0026#34;KZ9V\u0026lt;SNIP\u0026gt;\u0026#34; , \u0026#34;SessionToken\u0026#34; : \u0026#34;IQoJb\u0026lt;SNIP\u0026gt;\u0026#34; , \u0026#34;만료\u0026#34; : \u0026#34;2023-06-03T01:00:43+00:00\u0026#34; }, \u0026#34;SubjectFromWebIdentityToken\u0026#34; : \u0026#34;system:serviceaccount:carts:carts\u0026#34;, \u0026#34;AssumedRoleUser\u0026#34; : { \u0026#34;AssumedRoleId\u0026#34;: \u0026#34;AROAWU22AXDEAGQGMY3XZ:test\u0026#34; , \u0026#34;Arn\u0026#34; : \u0026#34;arn:aws:sts::ACCOUNT_ID:assumed-role/eks-workshop-carts-dynamo/test\u0026#34; }, \u0026#34;제공자\u0026#34; : \u0026#34;arn:aws:iam::ACCOUNT_ID :oidc-provider/oidc.eks.us-east-2.amazonaws.com/id/9278920BDA929A8A54E92E18A4DECB2A\u0026#34; , \u0026#34;대상\u0026#34; : \u0026#34;sts.amazonaws.com\u0026#34; } 이를 방지하기 위한 방법으로는 role 의 신뢰 정책에서 반드시 네임스페이스와 서비스 계정을 지정해야 한다. 서비스 계정을 지정하면 해당 서비스 계정에서만 해당 IAM 역할을 위임하여 사용할 수 있기 때문이다.\nIRSA 사용하다면? 파드에서 메타데이터 접근을 허용하지 않도록 네트워크 정책을 통해 차단이 가능하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-metadata-access namespace: example spec: podSelector: {} policyTypes: - Egress egress: - to: - ipBlock: cidr: 0.0.0.0/0 except: - 169.254.169.254/32 특정 케이스가 생기면 다음과 같이 podselector를 통해서 해당 파드에서만 메타데이터 접근을 허용할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-metadata-access namespace: example spec: podSelector: matchLabels: app: myapp policyTypes: - Egress egress: - to: - ipBlock: cidr: 169.254.169.254/32 ","date":"Jun 02","permalink":"https://HanHoRang31.github.io/post/eks-security-concept/","tags":["KANS","kubeflow","cloud","AWS","eksctl","eks","security","IRSA"],"title":"[AEWS] EKS Security 이해"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. EKS workshop 아키텍처 이해와 트레이싱 기능 확인하기 스터디에서 관측시스템 관련 좋은 워크샵 링크를 공유해주셨다. 이번 시간에는 해당 워크샵 자료를 토대로 실습 아키텍처 이해와 워크샵 기능 중 트레이싱을 살펴볼 예정이다.\nEKS One Observability Workshop 워크샵 링크 : https://catalog.workshops.aws/observability/en-US\n관측시스템 아키텍처 워크샵에서 제공하는 아키텍처가 AWS 기반의 실 애플리케이션을 운영하는 예제로 좋은 것 같아 정리한다. 애플리케이션은 애완동물 입양 안내 사이트로 애완동물에 대한 검색 기능과 및 입양 여부에 따라 조회가 달라지는 기능을 제공한다.\n아키텍처 구성 이해 각 모듈은 AWS 서비스로 구성되어 있으며 아키텍처는 다음 아키텍처와 같다. 아키텍처가 많이 복잡하다.. 이해를 위해 아키텍처에서 AWS 서비스 구성 이유와 어떻게 서비스들이 연동하고 있는 지 정리하여 공유한다.\n각 API 서비스 앞 ALB(Application Load Balancer)를 배치한 이유?\n로드 밸런싱 : 서버 부하를 줄이고, 높은 가용성과 신뢰성을 제공하여 규모에 따라 자동으로 스케일링되도록 구성\n컨테이너화 된 워크로드 지원 : ALB는 ECS 및 EKS와 같은 컨테이너화된 워크로드를 지원한다.\n내구성 및 보안 : ALB은 각 가용 영역에서 실행되어 높은 내구성을 제공하며, AWS WAF, Shield, ACM 같은 보안 서비스와 통합하여 보안을 강화할 수 있다.\nALB 연동은 AWS 콘솔에서 손 쉽게 가능하다. [EC2] → [대상 그룹] 생성 후, 로드밸런서에서 ALB 생성을 진행하면 연동이 완료된다.\nPetSite-FrontEnd 에 SQS, SNS, StepFunctions를 연결한 이유 ?\nSNS 연결 이유 ? SNS (Simple Notification Service)는 웹서비스를 통해 모든 인프라 및 앱에 분산 메시징을 제공하는 완전관리형 퍼블리시/서브스크라이브 메시징 서비스이다. 예제에서는 프론트앤드 트래픽 및 상태 값에 알람을 걸고 등록된 이메일에 알람을 보내도록 구성되어 있다.\nStepFunction 연결 이유 ? StepFunction 은 람다 함수 서비스를 통합하여 워크로드를 구성할 수 있는 서비스이다. 본 예제에서는 호출 테스트를 목적으로 일정 시간마다 dynamodb 에 저장된 애완동물 값을 읽어 애플리케이션 상태를 확인하도록 구성되어 있다. AWS 콘솔에서는 다음과 같이 구성 및 호출 결과를 확인할 수 있다.\n각 람다 함수는 python 으로 구성되어 있으며 코드 이해를 돕기 위해 공유한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # ReadDynamoDB # DynamoDB 에 저장된 애완동물의 가격을 불러온다. import json import boto3 from boto3.dynamodb.conditions import Key ssm = boto3.client(\u0026#39;ssm\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) def lambda_handler(event, context): dynamodb_tablename = ssm.get_parameter(Name=\u0026#39;/petstore/dynamodbtablename\u0026#39;, WithDecryption=False) table = dynamodb.Table(dynamodb_tablename[\u0026#39;Parameter\u0026#39;][\u0026#39;Value\u0026#39;]) response = table.query( KeyConditionExpression=Key(\u0026#39;petid\u0026#39;).eq(event[\u0026#39;petid\u0026#39;]) \u0026amp; Key(\u0026#39;pettype\u0026#39;).eq(event[\u0026#39;pettype\u0026#39;]) ) response[\u0026#39;Items\u0026#39;][0][\u0026#39;price\u0026#39;] = int(response[\u0026#39;Items\u0026#39;][0][\u0026#39;price\u0026#39;]) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: response[\u0026#39;Items\u0026#39;][0] } 1 2 3 4 5 6 7 8 9 10 11 12 # PriceGreaterthan55 # 55 이상의 애완동물을 확인하고 200을 결과 값으로 반환한다. import json def lambda_handler(event, context): # TODO implement print(event) print(\u0026#39;ProcessLessthan55 - Execution complete\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;ProcessLessthan55 - Execution complete\u0026#39;) } 1 2 3 4 5 6 7 8 9 10 11 12 13 # Pricelessthan55 # 55 미만의 애완동물을 확인하고 200을 결과 값으로 반환한다. import json def lambda_handler(event, context): # TODO implement print(event) print(\u0026#39;ProcessGreaterThan55 - Execution complete\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;ProcessGreaterThan55 - Execution complete\u0026#39;) } SQS 사용 이유? 서버서리스 메시지 큐 서비스로 본 예제에서는 비동기 작업 처리(입양)에 대한 부하 분산 및 메세지 처리 순서를 보장하기 위해 구성되었다. 메세지 큐 사용은 payment에서 사용되며 코드에서는 아래와 같이 입양시 SQS에 메세지를 날려 처리하도록 구성되어 있다.\nSQS를 사용하면, AWS 콘솔에서 메세지 큐에 대한 메트릭 정보를 확인할 수 있다. 각 메세지에 저장된 값도 확인할 수 있으면 좋겠지만,, 생산/처리 시스템으로 설계되어 있어 메세지 확인은 불가하다.\n애플리케이션 API를 EKS 와 Fargate 로 구성한 이유?\nFargate와 EKS를 함께 사용하면 Kubernetes의 유연성과 Fargate의 서버리스 이점을 모두 활용할 수 있다고 한다. 함께 사용시 다음과 같은 이점이 있다.(참고 ChatGPT)\n서버리스 컨테이너 운영: Fargate로 구성시 인프라 자원(서버/클러스터)에 대한 관리는 AWS에서 관리하며, 트래픽에 따라 자동으로 스케일링이 된다. 이에 따라 개발자는 컨테이너 인프라 관리에 신경 쓸 비용이 줄어 개발에 집중할 수 있게 된다. 비용 효율: Fargate는 실제 사용량에 따라 요금이 청구되므로, 사용하지 않는 컴퓨트 리소스에 대해 지불할 필요가 없다. 본 예제에서는 API 구성이 Frontend 부분만 EKS 관리형 노드의 Pod로 나머지 API는 Fargate로 구성되어 있다. 필자가 생각하건데 Frontend 부분만 EKS Pod로 사용한 이유는 전체 서비스의 첫 대문을 계속 운영하여 서비스에 대한 초기 로딩 지연시간을 업애기 위해 사용된 것 같고, 나머지 부분을 Fargate로 대체하여 추가 API 기능에 대해 초기 로딩 지연시간은 있지만, 비용을 절감할 수 있도록 구성한 것 같다.\nAWS Lambda 와 EKS 를 같이 사용하는 이유?\n결국 서버리스 컴퓨팅과 쿠버네티스(컨테이너화)의 차이점으로 작업이 이벤트이냐 아니냐에 따라 구성을 달리 한 것 같다. 본 예제에서의 람다 함수들은 트래픽 테스트를 위해 일정 시간 정보를 확인하는 함수와 입양 상태를 업데이트하는 함수로 구성되어 있어 있다. 이벤트 기반의 함수들에 대해 람다 함수들로 구성되어 있음을 확인할 수 있다.\n아키텍처를 보면 람다와 동일한 서버리스 컴퓨팅인 Fargate를 사용한 것을 확인할 수 있다. 같은 서버리스 컴퓨팅 서비스이지만, 작업이 짧고 가벼운(호출/업데이트) 이벤트에 대해서는 람다로 구성하고 반대로 길고 컴퓨팅 자원이 무거운(조회) 이벤트에 대해서는 Fargate로 구성됨을 알 수 있다.\nPetSearch-API에 S3 하고 dynamodb를 사용한 이유?\n이미지를 저장하고 검색하는 경우, S3 스토리지를 사용하는 것이 일반적이다. 그러나 사용자 정보, 상품 카탈로그, 게임 상태 등과 같은 구조화된 데이터를 빠르게 검색하고 조작할 필요가 있을 때는 DynamoDB가 더 적합하다. 본 예제에서는 이 둘을 결합하여 애완동물 이미지 파일을 S3에 저장하고, 해당 이미지의 메타데이터(예: 펫 이름, 입양 상태)를 DynamoDB에 저장하도록 구성되어 있다. 이렇게 하면 빠르고 효율적인 검색을 통해 필요한 이미지를 신속하게 찾을 수 있다.\nS3 콘솔에서는 다음과 같이 이미지 파일들을 확인할 수 있다.\n다음 코드는 람다 서비스의 PetAdoptionStauteUpdate 코드이다. 코드를 보면 입양이 완료되면 dynamodb에 저장된 애완동물 메타데이터(availability) 를 업데이트하여 애완 동물의 입양 상태를 업데이트하도록 구성되어 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 \u0026#34;use strict\u0026#34;; // asset-input/index.js var AWSXRay = require(\u0026#34;aws-xray-sdk\u0026#34;); var AWS = AWSXRay.captureAWS(require(\u0026#34;aws-sdk\u0026#34;)); var documentClient = new AWS.DynamoDB.DocumentClient(); exports.handler = async function(event, context, callback) { var payload = JSON.parse(event.body); var availability = \u0026#34;yes\u0026#34;; if (payload.petavailability === void 0) { availability = \u0026#34;no\u0026#34;; } var params = { TableName: process.env.TABLE_NAME, Key: {12 \u0026#34;pettype\u0026#34;: payload.pettype, \u0026#34;petid\u0026#34;: payload.petid }, UpdateExpression: \u0026#34;set availability = :r\u0026#34;, ExpressionAttributeValues: { \u0026#34;:r\u0026#34;: availability }, ReturnValues: \u0026#34;UPDATED_NEW\u0026#34; }; await updatePetadoptionsTable(params); # 업데이트 부분 console.log(\u0026#34;Updated petid: \u0026#34; + payload.petid + \u0026#34;, pettype: \u0026#34; + payload.pettype + \u0026#34;, to availability: \u0026#34; + availability); return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;success\u0026#34; }; }; async function updatePetadoptionsTable(params) { await documentClient.update(params, function(err, data) { if (err) { console.log(JSON.stringify(err, null, 2)); } else { console.log(JSON.stringify(data, null, 2)); } }).promise(); } PayForAdoptions-API 에 API gateway를 사용한 이유 ?\nWS Lambda 앞에 API Gateway를 연결하는 것은 웹 또는 모바일 애플리케이션에서 서버리스 백엔드를 구성하는 일반적인 패턴이다. 람다에서는 자체적으로 HTTP(S) 엔드포인트를 제공하기 때문이다. 이렇게 하면 AWS Lambda를 사용하여 복잡한 백엔드 로직을 처리하고, API Gateway를 사용하여 HTTP 요청을 라우팅하고 보안을 관리할 수 있다.\n예제 배포 예제 배포는 AWS CDK로 배포한다. CDK 는 AWS 서비스에 대한 IaS 로 코드를 통해 AWS 서비스를 배포 할 수 있다. 배포를 위한 명령어로는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # CDK 필요 패키지 설치로 npm 설치 진행 curl -sL https://rpm.nodesource.com/setup_14.x | sudo bash - sudo yum install nodejs # 예제 깃 레파지토리 clone git clone https://github.com/aws-samples/one-observability-demo cd workshopfiles/one-observability-demo/PetAdoptions/cdk/pet_stack # cdk 환경 설정 cdk bootstrap # cdk 배포 CONSOLE_ROLE_ARN=\u0026lt;Enter your Role ARN\u0026gt; # 역할 role arn을 업데이트 하자 EKS_ADMIN_ARN=$(../../getrole.sh) echo -e \u0026#34;\\nRole \\\u0026#34;${EKS_ADMIN_ARN}\\\u0026#34; will be part of system\\:masters group\\n\u0026#34; if [ -z $CONSOLE_ROLE_ARN ]; then echo -e \u0026#34;\\nEKS Console access will be restricted\\n\u0026#34;; else echo -e \u0026#34;\\nRole \\\u0026#34;${CONSOLE_ROLE_ARN}\\\u0026#34; will have access to EKS Console\\n\u0026#34;; fi cdk deploy --context admin_role=$EKS_ADMIN_ARN Services --context dashboard_role_arn=$CONSOLE_ROLE_ARN --require-approval never cdk deploy Applications --require-approval never Trace (트레이싱) 트레이싱은 분산 시스템에서 요청의 생애 주기를 추적하는 기술로, 이를 통해 서비스 간 상호 작용의 가시성을 제공하고 문제를 진단하는 데 도움을 주는 기능이다. 트레이싱은 로그, 메트릭, 분산 트레이싱의 세 가지 관측 가능한 신호 중 하나로 관측 시스템에 큰 축을 담당하고 있다.\n트레이싱은 분산 시스템 및 마이크로서비스에서 기능이 크게 부각되는데 해당 시스템에서는 한 요청이 여러 서비스를 거쳐 처리되므로 문제가 발생했을 때 문제가 발생했는 지 파악하기가 어렵다. 이를 해결하기 위해 트레이싱은 서비스간의 요청마다 고유한 ID를 부여하여 이벤트를 발생시킬 때마다 이벤트의 시간 시간과 끝시간을 레이턴시를 파악한다.\nhttps://www.dynatrace.com/news/blog/open-observability-distributed-tracing-and-observability/\nX-Ray AWS 에서는 X-ray라는 서비스를 통해 트레이싱 기능을 제공한다. 워크샵에서는 X-ray를 사용하기 위한 코드 삽입 및 X-ray 파드가 배포되어 있는 상태이다. 본 블로그 글에서는 X-ray 설치는 스킵하고 트레이싱에 대한 기능 위주의 글을 작성할 예정이다.\n여기서 배포된 Xray-daemon 파드가 정보를 받아 AWS X-ray에 제공한다고 보면 이해가 쉽다.\n제공한 트레이싱 데이터들은 AWS 콘솔의 X-ray에서 확인할 수 있다. 예제 마이크로서비스에 대한 응답 시간과 메트릭(대기 시간, 요청, 장애) 를 한눈에 확인할 수 있다.\nX-ray 기능 확인하기 워크샵 내용을 토대로 X-ray 기능를 통해 확인하여 트레이싱 기능을 이해해보자. 가장 먼저, AWS 콘솔에 접근하면 다음과 같이 응답 시간을 한 눈에 확인할 수 있다. 먼저, 밑의 그림 처럼 장애난 부분의 화살표를 클릭하여 각 서비스 간의 응답 시간 및 에러 원인을 찾을 수 있다.\n에러 트레이싱하기 트레이싱 비교를 통해 원인 분석하기 다음은 X-ray Anlytics 의 기능으로 트레이스 데이터간 비교하여 문제 해결에 도움을 주는 기능을 확인하자. [AWS 콘솔] → [X-ray Anlytics] 에 접근하면 다음과 같이 트레이스 데이터를 확인할 수 있다. 다음 그림과 같이 응답 시간 분포에서 3초 이상의 데이터를 확인하여 분석을 위해 드래그를 통해 상세 정보를 확인하면 특정 API (조회)시 발생하는 것을 확인할 수 있다.\n특정 API 에 대한 응답 시간이 늦춰짐과 동시에 대한 원인 분석으로 해당 API가 실패했을 때 응답 시간이 3초이상 되는 지 확인할 수 있다. 아래 그림에서 필터링된 트레이스 세트 B를 클릭하고 HTTPS Stateus code를 클릭하면 트레이스 데이터간 비교가 가능하다. 아래 결과를 분석하면 실패했을 때는와 응답 시간에 대한 연관 관계는 없는 것을 확인할 수 있다.\n알람 구성 트레이싱 데이터에 대해 알람 구성이 가능하다. X-ray 의 그룹에 접근하여 필터 표현식을 통해 메트릭 생성이 가능하기 때문이다. 아래 예제는 응답 시간이 3초 이상에 대한 메트릭을 생성하여 Cloudwatch에서 해당 메트릭을 확인하는 예제이다.\n사용 케이스를 통한 트레이싱 이해하기 워크샵에서는 시스템 에러에 대한 트러블슈팅 과정으로 여러 사용 케이스를 제공한다. 실제 AWS 기반의 애플리케이션 운영시 트러블슈팅 과정에 도움이 되는 시나리오라 생각되어 구성 내용을 간략하게 정리하여 공유한다.\n트래픽 발생에 따른 트러블 슈팅 예제 애플리케이션 사이트에 과부하를 줘서 생기는 트러블슈팅 과정을 먼저 살펴보겠다. 다음의 명령어를 통해 과부화를 주자.\n1 2 3 PETLISTADOPTIONS_CLUSTER=$(aws ecs list-clusters | jq \u0026#39;.clusterArns[]|select(contains(\u0026#34;PetList\u0026#34;))\u0026#39; -r) TRAFFICGENERATOR_SERVICE=$(aws ecs list-services --cluster $PETLISTADOPTIONS_CLUSTER | jq \u0026#39;.serviceArns[]|select(contains(\u0026#34;trafficgenerator\u0026#34;))\u0026#39; -r) aws ecs update-service --cluster $PETLISTADOPTIONS_CLUSTER --service $TRAFFICGENERATOR_SERVICE --desired-count 5 과부화를 주면 아래 그림의 DynamoDB에 알람 표시가 나온 것을 확인할 수가 있다. 알람을 확인하면 해당 DB의 읽기 용량이 초과되었음을 확인할 수 있다.\n해결 방법은 간단하다. DynamoDB 테이블이 프로비저닝된 용량 모드로 요청이 제한되었기 때문에 온디맨드 용량 모드 옵션을 변경하면 처리가 된다. 옵션 명령어는 다음과 같다.\n1 2 3 DDB_TABLE=$(aws dynamodb list-tables | jq -r \u0026#39;.TableNames[] | select (. | contains(\u0026#34;ddbpetadoption\u0026#34;))\u0026#39;) aws dynamodb update-table --table-name ${DDB_TABLE} --billing-mode PAY_PER_REQUEST | jq -r \u0026#39;.TableDescription.TableStatus\u0026#39; 메모리 문제 발생시 트러블슈팅 다음은 메모리를 비롯한 인프라에 대한 문제 발생시 트러블슈팅 하는 과정이다. 인프라 자원의 문제가 생기면 애플리케이션에 대한 에러가 발생할 것이다.\n원인 분석으로 X-ray에서는 필터링 기능을 통해 에러 통신 값에 따른 트레이싱 정보를 확인할 수 있다.\n트레이싱 데이터를 클릭하면 각 요청에 대한 상세 정보를 확인할 수 있다. 데이터 내에서는 각 요청에 대한 로그도 확인할 수 있는 데 밑의 로그 기록처럼 메모리 관리 부분에 문제가 생긴 것을 확인할 수 있다.\n이를 증명하기 위해 클라우드워치에서 메모리 및 CPU 메트릭을 확인하면 다음과 같이 인프라 자원에 피크가 생긴 것을 확인할 수 있다.\n","date":"May 26","permalink":"https://HanHoRang31.github.io/post/eks-observability-tracing/","tags":["AEWS","kubeflow","cloud","AWS","eksctl","eks","x-ray","Observability","trace"],"title":"[AEWS] EKS Observability 워크샵 학습과 트레이싱 기능 확인하기"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. 스터디 5주차 시간에는 최근 핫한 노드 수명 주기 관리 솔루션인 Karpenter를 공부하였다. 타 기업 사례에서 자주 보는 주제로서 볼 때마다 시간나면 해야지 해야지.. 생각만 했었는데 이번 스터디로 계기가 되어 정리한다. 이번 블로그 글에서는 Karpenter 에 대해 중점적으로 정리하여 공유할 예정이다. 먼저 Karpenter에 대한 개념과 원리를 살펴볼 것이고, 실습으로 오버프로비저닝과 Kubeflow와 통합하여 테스트를 진행할 것이다.\nKarpenter ? 쿠버네티스에서 동작하는 오픈소스 노드 오토스케일러이다. 기존 노드 오토스케일러인 Cluster Autoscaler(CA) 의 진화 기술이라고 생각하면 생각하기 쉽겠다. CA에 비해 Karpenter 가 최근 노드 오토스케일러 기술로 각광받고 있는데 g이유를 정리하면 다음과 같다.\n실시간 노드 프로비저닝(Just in time) : 기존 CA 가 5~10분 정도의 프로비저닝 시간이 걸리는 반면, Karpenter 는 5~30초 단위의 시간으로 노드가 빠르게 프로비저닝된다. 이로 인해 운영 워크로드에서 예상하지 못한 트래픽에 발 빠른 대처가 가능하다. 기능 기반 프로비저닝(Optimized) : Karpenter는 인스턴스 가드레일 방식,PV 서브넷 인식을 지원한다. 가드레일 방식이란 사용자가 지정한 인스턴스 타입의 범위에서 노드가 프로비저닝된다는 것을 의미하며 여기에서 가장 저렴한 노드를 자동으로 선택하여 프로비저닝된다. 또한, 자동으로 PV를 인식하여 PV가 존재하는 서브넷에 노드를 프로비저닝 시켜준다. 노드 자동 조정 (Optimized) : 여유 컴퓨팅 자원이 있을 시 자동으로 노드를 정리해주며, 큰 노드 하나가 작은 노드 여러개 보다 비용이 저렴하면 자동으로 합쳐줘 비용 효율적으로 노드를 운영시킬 수 있다. 타 운영 관리 솔루션과 합쳐 다양한 노드 스케쥴링 가능 : 대표적으로 이벤트 기반의 파드 수를 조절하는 KEDA와 같이 사용하여 오버 프로비저닝이 가능하다. 기존의 EKS EC2 노드 관리를 생각하면 정말 강력한 기능이 아닐 수 없다! 기존 EKS 의 노드같은 경우 하나의 인스턴스 타입으로 노드 그룹을 구성하고 변경할 수 없었으며, 노드 프로비저닝에 기본 5분이 걸렸다.\nKarpenter가 이러한 기능을 제공할 수 있는 원리를 찾아보니 EKS의 노드를 노드 그룹이 아닌 EC2 Fleet으로 노드를 관리하기 때문이였다.\nEC2 Fleet은 EC2 인스턴스 유형과 가용 영역을 최대한 활용하여, 비용을 최적화하는 데 유용한 도구이다. 기능적으로는 Karpenter에서 확인한 기능 요소인 다양한 인스턴스 유형 프로비저닝, Spot 인스턴스 \u0026amp; 온디맨드 혼합, 자동 조정을 제공한다.\nKarpenter 배포 이어서 Karpenter 를 배포하고 실습해보겠다. 실습 내용은 공식문서와 스터디에서 모임장님이 공유해주신 내용을 참고하였다. 먼저 Cloudformation을 통해 베스천 서버를 구축하고 Karpenter을 활성화시키기 위해 EKS 클러스터 생성을 진행하겠다. EKS 생성이 끝나면 Karpenter을 설치하고, 예제를 통해 노드의 상태를 확인하겠다.\n환경 구성 베스천 서버 생성\n1 2 3 4 5 6 7 8 9 10 11 # 베스천 서버 YAML 파일 다운로드 curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/karpenter-preconfig.yaml # CloudFormation 스택 배포 파라미터 작성 # aws cloudformation deploy --template-file karpenter-preconfig.yaml --stack-name myeks2 --parameter-overrides KeyName=kp-gasida SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 MyIamUserAccessKeyID=AKIA5... MyIamUserSecretAccessKey=\u0026#39;CVNa2...\u0026#39; ClusterBaseName=myeks2 --region ap-northeast-2 # CloudFormation 스택 배포 완료 후 작업용 EC2 IP 출력 aws cloudformation describe-stacks --stack-name myeks2 --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text # 작업용 EC2 SSH 접속 ssh -i key.pem ec2-user@$(aws cloudformation describe-stacks --stack-name myeks2 --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text) EKS 클러스터 생성\nKarpenter 로 노드 수명을 관리하기 위해서는 IRSA 허용, 클러스터 태그 설정 , IAM 정책 설정 그리고 aws-auth 에 역할 연결이 필요하다. 필자는 위에서 구성한 베스천 서버에 접속하여 다음의 명령어를 통해 설치를 진행하였다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 # Karpenter 버전 및 임시 파일 생성 export KARPENTER_VERSION=v0.27.5 export TEMPOUT=$(mktemp) # Karpenter 설치를 위한 환경변수 확인 # 다 나와야 설치가 진행된다. echo $KARPENTER_VERSION $CLUSTER_NAME $AWS_DEFAULT_REGION $AWS_ACCOUNT_ID $TEMPOUT --- v0.27.5 hanhorang ap-northeast-2 0000000000 /tmp/tmp.ckgtAn0r5x # Karpenter IAM 정책 및 role, EC2 Instance Profile 생성 curl -fsSL https://karpenter.sh/\u0026#34;${KARPENTER_VERSION}\u0026#34;/getting-started/getting-started-with-karpenter/cloudformation.yaml \u0026gt; $TEMPOUT \\ \u0026amp;\u0026amp; aws cloudformation deploy \\ --stack-name \u0026#34;Karpenter-${CLUSTER_NAME}\u0026#34; \\ --template-file \u0026#34;${TEMPOUT}\u0026#34; \\ --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \u0026#34;ClusterName=${CLUSTER_NAME}\u0026#34; # EKS 클러스터 구성 eksctl create cluster -f - \u0026lt;\u0026lt;EOF --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} version: \u0026#34;1.24\u0026#34; tags: karpenter.sh/discovery: ${CLUSTER_NAME} iam: withOIDC: true serviceAccounts: - metadata: name: karpenter namespace: karpenter roleName: ${CLUSTER_NAME}-karpenter attachPolicyARNs: - arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME} roleOnly: true iamIdentityMappings: - arn: \u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME}\u0026#34; username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes managedNodeGroups: - instanceType: m5.large amiFamily: AmazonLinux2 name: ${CLUSTER_NAME}-ng desiredCapacity: 2 minSize: 1 maxSize: 10 iam: withAddonPolicies: externalDNS: true *## Optionally run on fargate # fargateProfiles: # - name: karpenter # selectors: # - namespace: karpenter* EOF eksctl 코드에서 karpenter를 배포하기 위해 몇 가지 설정을 진행하였다.\ntags : 카펜터를 사용할 곳에 태그를 지정시킨다. 해당 예제에서는 클러스터 전체에서 카펜터를 사용하도록 지정하였지만, 필요에 따라 서브넷, 보안 그룹에 태그를 지정하여 해당 태그에 있는 곳에만 카펜터를 사용할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 노드 그룹 서브넷 태그 추가 for NODEGROUP in $(aws eks list-nodegroups --cluster-name ${CLUSTER_NAME} \\ --query \u0026#39;nodegroups\u0026#39; --output text); do aws ec2 create-tags \\ --tags \u0026#34;Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}\u0026#34; \\ --resources $(aws eks describe-nodegroup --cluster-name ${CLUSTER_NAME} \\ --nodegroup-name $NODEGROUP --query \u0026#39;nodegroup.subnets\u0026#39; --output text ) done # 보안 그룹 태그 추가 NODEGROUP=$(aws eks list-nodegroups --cluster-name ${CLUSTER_NAME} \\ --query \u0026#39;nodegroups[0]\u0026#39; --output text) LAUNCH_TEMPLATE=$(aws eks describe-nodegroup --cluster-name ${CLUSTER_NAME} \\ --nodegroup-name ${NODEGROUP} --query \u0026#39;nodegroup.launchTemplate.{id:id,version:version}\u0026#39; \\ --output text | tr -s \u0026#34;\\t\u0026#34; \u0026#34;,\u0026#34;) # If your EKS setup is configured to use only Cluster security group, then please execute - SECURITY_GROUPS=$(aws eks describe-cluster \\ --name ${CLUSTER_NAME} --query \u0026#34;cluster.resourcesVpcConfig.clusterSecurityGroupId\u0026#34; --output text) # If your setup uses the security groups in the Launch template of a managed node group, then : SECURITY_GROUPS=$(aws ec2 describe-launch-template-versions \\ --launch-template-id ${LAUNCH_TEMPLATE%,*} --versions ${LAUNCH_TEMPLATE#*,} \\ --query \u0026#39;LaunchTemplateVersions[0].LaunchTemplateData.[NetworkInterfaces[0].Groups||SecurityGroupIds]\u0026#39; \\ --output text) aws ec2 create-tags \\ --tags \u0026#34;Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}\u0026#34; \\ --resources ${SECURITY_GROUPS} iam : OIDC 를 True로 설정함과 동시에 karpenter라는 쿠버네티스 서비스 어카운터에 앞 서 생성한 정책을 연결하였다. 이렇게 하면 생성한 정책에 따라 karpenter 사용자가 AWS 서비스를 관리할 수 있게 된다.\niamIdentityMappings: aws-auth configmap 업데이트 작업으로 노드 IAM 역할을 사용하는 노드가 클러스터에 가입하도록 허용시켜주는 작업이다. 예상이지만 노드를 ASG로 관리하는 것이 아닌 EC2 Fleet으로 관리하기에 추가로 필요한 작업인 것 같다.\nKarpenter 배포\nEKS 클러스터 생성 후, Karpenter 를 배포하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 카펜터 설치를 위한 환경 변수 확인 export CLUSTER_ENDPOINT=\u0026#34;$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \u0026#34;cluster.endpoint\u0026#34; --output text)\u0026#34; export KARPENTER_IAM_ROLE_ARN=\u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-karpenter\u0026#34; echo $CLUSTER_ENDPOINT $KARPENTER_IAM_ROLE_ARN # EC2 Spot Fleet 사용을 위한 정책 확인 : 이미 생성한 정책으로 결과와 같이 에러가 떠야 정상이다. aws iam create-service-linked-role --aws-service-name spot.amazonaws.com || true -- An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix. # public ECR 로그아웃, 익명의 상태로 이미지 다운로드하기 위함 docker logout public.ecr.aws # Karpenter 설치 helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version ${KARPENTER_VERSION} --namespace karpenter --create-namespace \\ --set serviceAccount.annotations.\u0026#34;eks\\.amazonaws\\.com/role-arn\u0026#34;=${KARPENTER_IAM_ROLE_ARN} \\ --set settings.aws.clusterName=${CLUSTER_NAME} \\ --set settings.aws.defaultInstanceProfile=KarpenterNodeInstanceProfile-${CLUSTER_NAME} \\ --set settings.aws.interruptionQueueName=${CLUSTER_NAME} \\ --set controller.resources.requests.cpu=1 \\ --set controller.resources.requests.memory=1Gi \\ --set controller.resources.limits.cpu=1 \\ --set controller.resources.limits.memory=1Gi \\ --wait # 설치 확인 kubectl get all -n karpenter kubectl get cm -n karpenter karpenter-global-settings -o jsonpath={.data} | jq kubectl get crd | grep karpenter Karpenter 모니터링 설정\nKarpenter 설치가 완료되었으면 예제를 통해 노드 프로비저닝을 직접 테스트해보겠다. 먼저 노드 프로비저닝을 확인하기 위해 그라파나와 노드 모니터링 도구인 eks-node-viewer를 설치하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # External DNS 추가 MyDomain=hanhorang.link # 각자 도메인 입력 echo \u0026#34;export MyDomain=\u0026lt;자신의 도메인\u0026gt;\u0026#34; \u0026gt;\u0026gt; /etc/profile *MyDomain=*hanhorang.link *echo \u0026#34;export MyDomain=gasida.link\u0026#34; \u0026gt;\u0026gt; /etc/profile* MyDnzHostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name \u0026#34;${MyDomain}.\u0026#34; --query \u0026#34;HostedZones[0].Id\u0026#34; --output text) echo $MyDomain, $MyDnzHostedZoneId curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/aews/externaldns.yaml MyDomain=$MyDomain MyDnzHostedZoneId=$MyDnzHostedZoneId envsubst \u0026lt; externaldns.yaml | kubectl apply -f - # IP 주소 확인 : 172.30.0.0/16 VPC 대역에서 172.30.1.0/24 대역을 사용 중 ip -br -c addr # EKS Node Viewer 설치 : 현재 ec2 spec에서는설치에 다소 시간이 소요됨 = 2분 이상 go install github.com/awslabs/eks-node-viewer/cmd/eks-node-viewer@latest # [터미널1] bin 확인 및 사용 tree ~/go/bin cd ~/go/bin ./eks-node-viewer -h ./eks-node-viewer # 그라파나 배포 helm repo add grafana-charts https://grafana.github.io/helm-charts helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update kubectl create namespace monitoring # 프로메테우스 설치 curl -fsSL https://karpenter.sh/\u0026#34;${KARPENTER_VERSION}\u0026#34;/getting-started/getting-started-with-karpenter/prometheus-values.yaml | tee prometheus-values.yaml helm install --namespace monitoring prometheus prometheus-community/prometheus --values prometheus-values.yaml --set alertmanager.enabled=false # 그라파나 설치 curl -fsSL https://karpenter.sh/\u0026#34;${KARPENTER_VERSION}\u0026#34;/getting-started/getting-started-with-karpenter/grafana-values.yaml | tee grafana-values.yaml helm install --namespace monitoring grafana grafana-charts/grafana --values grafana-values.yaml --set service.type=LoadBalancer # admin 암호 kubectl get secret --namespace monitoring grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo # 그라파나 접속 kubectl annotate service grafana -n monitoring \u0026#34;external-dns.alpha.kubernetes.io/hostname=grafana.$MyDomain\u0026#34; echo -e \u0026#34;grafana URL = http://grafana.$MyDomain\u0026#34; 모니터링 설치를 위한 추가 헬름 차트 구성은 다음과 같다. 설치한 메트릭 스택이 있다면 아래 부분을 추가하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 프로메테우스 헬름 차트 alertmanager: persistentVolume: enabled: false server: fullnameOverride: prometheus-server persistentVolume: enabled: false # karpenter 메트릭 수집 추가 extraScrapeConfigs: | - job_name: karpenter kubernetes_sd_configs: - role: endpoints namespaces: names: - karpenter relabel_configs: - source_labels: [__meta_kubernetes_endpoint_port_name] regex: http-metrics action: keep 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 그라파나 헬름 차트 datasources: # 메트릭 소스 설정 datasources.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus version: 1 url: http://prometheus-server:80 access: proxy dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \u0026#39;default\u0026#39; orgId: 1 folder: \u0026#39;\u0026#39; type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: # 대시보드 추가 default: capacity-dashboard: url: https://karpenter.sh/v0.27.5/getting-started/getting-started-with-karpenter/karpenter-capacity-dashboard.json performance-dashboard: url: https://karpenter.sh/v0.27.5/getting-started/getting-started-with-karpenter/karpenter-performance-dashboard.json 그라파나 대시보드 화면\neks-node-viewer 모니터링 화면\nKarpenter 테스트 모니터링 설정이 끝났으면 실제로 노드가 빠르게 프로비저닝되는 지 테스트해보겠다. 노드 관리에 따른 설정 옵션으로 Karpenter 에서는 Provisioner 라는 CRD 형태의 관리 형태를 제공한다. 제공하는 옵션이 많으므로 공식문서를 통해 옵션을 참고하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\u0026#34;spot\u0026#34;] limits: resources: cpu: 1000 providerRef: name: default ttlSecondsAfterEmpty: 30 --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: default spec: subnetSelector: karpenter.sh/discovery: ${CLUSTER_NAME} securityGroupSelector: karpenter.sh/discovery: ${CLUSTER_NAME} EOF spec : Spot 인스턴스를 추가하도록 설정하고 최대 CPU 제한을 1000개로 설정한다. ttlSecondsAfterEmpty : 노드가 비어 있을 때 해당 노드를 종료하기 전에 대기하는 시간이다. 본 예제에서는 30초로 설정하였다. 이제 예제 파드를 배포하고, 파드 수를 늘려 노드 프로비저닝을 확인하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # pause 파드 1개에 CPU 1개 최소 보장 할당 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.7 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 파드를 5개로 늘리니 실시간으로 Spot 인스턴스가 프로비저닝된다. 파드가 배치되기까지 약 1분정도 소요되며 그라파나에서 생성한 노드 수를 확인할 수 있다.\nKarpenter 활용 Karpenter와 타 쿠버네티스 플랫폼과 합쳐 노드 관리해보는 시나리오를 구성하여 활용해보겠다. 활용 시나리오는 다음과 같다.\nKarpenter + KEDA로 노드 오버프로비저닝 Karpenter + Kubeflow 로 필요시 GPU 기반의 SPOT 인스턴스 제공하기 Karpenter + KEDA로 노드 오버 프로비저닝 AWS 한국사용자모임 - AWSKRUG 에서 발표해주신 내용으로 직접 테스트해보겠다. 영상 PPT에서 확인할 수 있듯이, 노드 오버프로비저닝의 목적은 카펜터가 프로비저닝 하는 시간(1~2분)을 없애기 위한 목적이다.\nKarpenter 는 ASG를 사용하지 않기 때문에 CA 처럼 일정 시간에 노드를 증설하고 감소 시킬 수 없다. 이를 위한 해결 방법으로 이벤트 기반의 툴인 KEDA를 통해 파드를 특정 시간에 배치하여 깡통 노드를 증설시켜 해결할 수 있다.\nhttps://www.youtube.com/watch?v=FPlCVVrCD64\nKEDA ? 특정 이벤트를 기반으로 파드 수를 스케쥴링시켜주는 Autoscaler이다. Kubernetes의 cron 처럼 파드 수를 일정 시간에 수를 늘릴 수 도 있고, 특정 이벤트(task 수, kafka topic)에 의해 파드를 스케쥴링할 수 있다.\nhttps://keda.sh/docs/2.10/concepts/\n선수 작업으로 prom-operator 설치가 필요하다. 아래의 과정으로 설치를 진행하자.\n1 2 3 4 5 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts kubectl create ns monitoring helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.27.2 \\ --set prometheus.prometheusSpec.scrapeInterval=\u0026#39;15s\u0026#39; --set prometheus.prometheusSpec.evaluationInterval=\u0026#39;15s\u0026#39; \\ --namespace monitoring 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # KEDA 설치 cat \u0026lt;\u0026lt;EOT \u0026gt; keda-values.yaml metricsServer: useHostNetwork: true prometheus: metricServer: enabled: true port: 9022 portName: metrics path: /metrics serviceMonitor: # Enables ServiceMonitor creation for the Prometheus Operator enabled: true podMonitor: # Enables PodMonitor creation for the Prometheus Operator enabled: true operator: enabled: true port: 8080 serviceMonitor: # Enables ServiceMonitor creation for the Prometheus Operator enabled: true podMonitor: # Enables PodMonitor creation for the Prometheus Operator enabled: true webhooks: enabled: true port: 8080 serviceMonitor: # Enables ServiceMonitor creation for the Prometheus webhooks enabled: true EOT kubectl create namespace keda helm repo add kedacore https://kedacore.github.io/charts helm install keda kedacore/keda --version 2.10.2 --namespace keda -f keda-values.yaml 오버 프로비저닝 테스트 Karpenter 설치와 프로비저닝은 테스트에서 진행한 프로비저닝과 inflate 파드을 그대로 사용하겠다. 아래의 KEDA 이벤트를 정의하여 특정 시간에 파드를 늘리는 정책을 생성하고 노드 수를 모니터링하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # ScaledObject 정책 생성 : cron cat \u0026lt;\u0026lt;EOT \u0026gt; keda-cron.yaml apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: inflate-cron-scaled spec: # 파드 수 확인 minReplicaCount: 0 maxReplicaCount: 10 # 파드 대상 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: inflate # 트리거 설정 triggers: - type: cron metadata: timezone: Asia/Seoul start: 00,15,30,45 * * * * end: 05,20,35,50 * * * * desiredReplicas: \u0026#34;5\u0026#34; EOT kubectl apply -f keda-cron.yaml 확장 트리거를 cron으로 설정하여 특정 시간에 확장이 발생하도록 설정하였다. 매 시간 00, 15, 30, 45분에 5개의 파드으로 확장하고, 05, 20, 35, 50분에 확장을 종료하도록 설정하였다. 🧐 ScaledObject 트러블슈팅\n필자의 경우 ScaledObject 배포시 다음과 같은 스키마 에러가 발생하였다.\n1 2 3 kubectl logs ScaledObject/inflate-cron-scaled -n keda --- error: no kind \u0026#34;ScaledObject\u0026#34; is registered for version \u0026#34;keda.sh/v1alpha1\u0026#34; in scheme \u0026#34;pkg/scheme/scheme.go:28\u0026#34; 구글링하니 CRD문제라 해서 차트를 재설치하였는데 문제가 계속 되어 트러블슈팅에 시간이 걸렸다. 결론적으로 CRD문제는 아니고, ScaledObject 속한 네임스페이스(keda) 와 deployment 네임스페이스(default) 달라 생긴 문제였다. 네임스페이스를 올바르게 설정하고 배포하면 문제 없이 진행된다.\n1 2 3 4 kubectl get ScaledObject -A --- NAMESPACE NAME SCALETARGETKIND SCALETARGETNAME MIN MAX TRIGGERS AUTHENTICATION READY ACTIVE FALLBACK AGE default inflate-cron-scaled apps/v1.Deployment inflate 0 10 cron True False Unknown 9s 15분에 확인하니 노드가 아래와 같이 정상적으로 프로비저닝된 것을 확인할 수 있다.\nKarpenter + Kubeflow 로 필요시 GPU 기반의 SPOT 인스턴스 제공하기 다음은 karpenter 를 kubeflow와 연계하여 GPU 기반의 SPOT 노드를 프로비저닝하겠다. 목적은 비용 최적화로 kubeflow 머신러닝 워크로드에서 GPU 자원이 필요할 때 SPOT 인스턴스를 노드에 프로비저닝하여 사용하고자 한다. 결론부터 말하면, 현재 karpenter 메모리 limit 이상 문제로 동작하지 않는다. 깃 이슈에서 문제를 확인 중이며 해결시 업데이트하겠다.\nkubeflow 설치와 구성은 필자의 블로그 글을 기반으로 진행한다. karpenter를 설치하고 프로비저닝 파일은 다음과 같이 정의하여 배포한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 # Cost-Optimized EKS cluster for Kubeflow with spot GPU instances and node scale down to zero # Built in efforts to reducing training costs of ML workloads. # Supporting tutorial can be found at the following link: # https://blog.gofynd.com/how-we-reduced-our-ml-training-costs-by-78-a33805cb00cf # This spec creates a cluster on EKS with the following active nodes # - 2x m5a.2xlarge - Accomodates all pods of Kubeflow # It also creates the following nodegroups with 0 nodes running unless a pod comes along and requests for the node to get spun up # - m5a.2xlarge -- Max Allowed 10 worker nodes # - p2.xlarge -- Max Allowed 10 worker nodes # - p3.2xlarge -- Max Allowed 10 worker nodes # - p3.8xlarge -- Max Allowed 04 worker nodes # - p3dn.24xlarge -- Max Allowed 01 worker nodes apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: # Name of your cluster, change to whatever you find fit. # If changed, make sure to change all nodegroup tags from # \u0026#39;k8s.io/cluster-autoscaler/my-eks-kubeflow: \u0026#34;owned\u0026#34;\u0026#39; --\u0026gt; \u0026#39;k8s.io/cluster-autoscaler/your-new-name: \u0026#34;owned\u0026#34;\u0026#39; name: hanhorang # choose your region wisely, this will significantly impact the cost incurred region: ap-northeast-2 # 1.14 Kubernetes version since Kubeflow 1.0 officially supports the same version: \u0026#39;1.25\u0026#39; tags: # Add more cloud tags if needed for billing karpenter.sh/discovery: hanhorang # Add all possible AZs to ensure nodes can be spun up in any AZ later on. # THIS CAN\u0026#39;T BE CHANGED LATER. YOU WILL HAVE TO CREATE A NEW CLUSTER TO ADD NEW AZ SUPPORT. # This list applies to the whole cluster and isn\u0026#39;t specific to nodegroups vpc: id: vpc-032c30fdebbb69fd6 cidr: 192.168.0.0/16 securityGroup: sg-093be0632becd746b nat: gateway: HighlyAvailable subnets: public: public-2a: id: subnet-03bfdfe3c7d5aa2a4 cidr: 192.168.1.0/24 public-2c: id: subnet-078ee0d964d71e1f2 cidr: 192.168.2.0/24 private: private-2a: id: subnet-0958e380d34c306e3 cidr: 192.168.3.0/24 private-2c: id: subnet-0bd38833c317d5e2b cidr: 192.168.4.0/24 iam: withOIDC: true serviceAccounts: - metadata: name: karpenter namespace: karpenter roleName: hanhorang-karpenter attachPolicyARNs: - arn:aws:iam::955963799952:policy/KarpenterControllerPolicy-hanhorang roleOnly: true iamIdentityMappings: - arn: \u0026#34;arn:aws:iam::955963799952:role/KarpenterNodeRole-hanhorang\u0026#34; username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes nodeGroups: - name: ng-1 desiredCapacity: 4 minSize: 0 maxSize: 10 # Set one nodegroup with 100GB volumes for Kubeflow to get deployed. # Kubeflow requirement states 1-2 Nodes with 100GB volume attached to the node. volumeSize: 100 volumeType: gp2 instanceType: c5n.xlarge privateNetworking: true ssh: publicKeyName: eks-terraform-key availabilityZones: - ap-northeast-2a labels: node-class: \u0026#34;worker-node\u0026#34; tags: # EC2 tags required for cluster-autoscaler auto-discovery k8s.io/cluster-autoscaler/node-template/label/lifecycle: OnDemand k8s.io/cluster-autoscaler/node-template/label/aws.amazon.com/spot: \u0026#34;false\u0026#34; k8s.io/cluster-autoscaler/node-template/label/gpu-count: \u0026#34;0\u0026#34; k8s.io/cluster-autoscaler/enabled: \u0026#34;true\u0026#34; k8s.io/cluster-autoscaler/my-eks-kubeflow: \u0026#34;owned\u0026#34; iam: withAddonPolicies: awsLoadBalancerController: true autoScaler: true cloudWatch: true efs: true ebs: true externalDNS: true addons: - name: vpc-cni # no version is specified so it deploys the default version version: v1.12.6-eksbuild.1 attachPolicyARNs: - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy - name: kube-proxy version: latest # auto discovers the latest available - name: coredns version: latest # v1.9.3-eksbuild.2 서브넷에 태그 추가\n베스천 서버 구성에서 eks 클러스터를 배포함으로 서브넷과 보안 그룹에 카펜터 사용을 위한 태그가 필요하다. 다음과 같이 입력하다.\nEBS CSI Driver 배포\n앞 블로그 글에서의 트러블슈팅에서 다룬 내용이다. kubeflow 설치를 위해 EBS CSI driver를 배포하고 기본 스토리지 클래스를 변경하자\n베스천서버 보안그룹 인그래스 규칙 추가\n본 글에서는 베스천서버에서 포트포워딩을 해서 테스트한다. 이를 위해 필자는 베스천 서버의 보안 그룹 인그래스 포트 설정을 모두 허용(0.0.0.0/0)으로 바꿨다.\nnotebook 생성 트러블슈팅\n포트포워딩으로 jupyter notebook 생성시 추가 작업이 필요하다. 아래 작업을 통해 APP_SECURE_COOKIES 옵션을 false 로 변경하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 kubectl edit deploy/jupyter-web-app-deployment -n kubeflow --- ... maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: jupyter-web-app kustomize.component: jupyter-web-app spec: containers: - env: - name: APP_PREFIX value: /jupyter - name: UI value: default - name: USERID_HEADER value: kubeflow-userid - name: USERID_PREFIX - name: APP_SECURE_COOKIES value: \u0026#34;false\u0026#34; # ture 에서 false 로 수정 ! image: docker.io/kubeflownotebookswg/jupyter-web-app:v1.7.0 imagePullPolicy: IfNotPresent name: jupyter-web-app ports: - containerPort: 5000 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/config name: config-volume - mountPath: /src/apps/default/static/assets/logos name: logos-volume ... EKS 클러스터 구성 후 카펜터 프로비저너를 다음과 같이 정의하여 배포하였다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\u0026#34;spot\u0026#34;] - key: node.kubernetes.io/instance-type operator: In values: [\u0026#34;p2.xlarge\u0026#34;, \u0026#34;p3.2xlarge\u0026#34;, \u0026#34;p3.8xlarge\u0026#34;, \u0026#34;p3.16xlarge\u0026#34;] # Add your desired GPU instance types here - key: kubernetes.io/arch operator: In values: [\u0026#34;nvidia\u0026#34;, \u0026#34;amd64\u0026#34;] limits: resources: cpu: 1000 memory: 1000Gi nvidia.com/gpu: 10 providerRef: name: default ttlSecondsAfterEmpty: 30 --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: default spec: subnetSelector: karpenter.sh/discovery: ${CLUSTER_NAME} securityGroupSelector: karpenter.sh/discovery: ${CLUSTER_NAME} EOF GPU 기반의 인스턴스를 설정하고 spot 인스턴스만 설정하도록 프로비저닝하였다. 테스트 kubeflow 에서 GPU 1개를 사용하는 jupyter notebook을 생성하여 잘 동작되는 지 확인하겠다.\nGPU 자원을 사용하는 jupyer notebook 생성시 파드가 pending 상태로 있다가, karpenter에 의해 GPU 노드가 새로 프로비저닝되고 파드가 배치되는 것을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # ubuntu golang install (go \u0026gt;1.16) wget https://go.dev/dl/go1.20.2.linux-amd64.tar.gz sudo tar -xvf go1.20.2.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT=/usr/local/go export PATH=$GOPATH/bin:$GOROOT/bin:$PATH go version -- go version go1.20.2 linux/amd64 go install github.com/awslabs/eks-node-viewer/cmd/eks-node-viewer@latest tree ~/go/bin cd ~/go/bin ./eks-node-viewer -resources cpu,memory 🧐 워크로드 트러블슈팅\n그러나, 노드의 임시 저장 공간이 부족하여 파드가 배치되었다가 추방되는 과정이 반복된다. 이벤트 로그를 확인하면 노드 ephemeral-storage 가 부족하다는데..\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 3m16s default-scheduler 0/5 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/disk-pressure: }, 4 Insufficient nvidia.com/gpu. preemption: 0/5 nodes are available: 1 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod. Normal Nominated 3m12s karpenter Pod should schedule on node: ip-192-168-3-68.ap-northeast-2.compute.internal Normal Scheduled 117s default-scheduler Successfully assigned kubeflow-user-example-com/test-0 to ip-192-168-3-68.ap-northeast-2.compute.internal Normal SuccessfulAttachVolume 113s attachdetach-controller AttachVolume.Attach succeeded for volume \u0026#34;pvc-e503cf70-748c-4a9c-aef8-c94172aa2324\u0026#34; Normal Pulling 106s kubelet Pulling image \u0026#34;docker.io/istio/proxyv2:1.16.0\u0026#34; Normal Pulled 99s kubelet Successfully pulled image \u0026#34;docker.io/istio/proxyv2:1.16.0\u0026#34; in 7.047356094s (7.047411717s including waiting) Normal Created 99s kubelet Created container istio-init Normal Started 99s kubelet Started container istio-init Normal Pulling 92s kubelet Pulling image \u0026#34;public.ecr.aws/kubeflow-on-aws/notebook-servers/jupyter-tensorflow:2.12.0-cpu-py310-ubuntu20.04-ec2-v1.0\u0026#34; Warning Evicted 17s kubelet The node was low on resource: ephemeral-storage. Warning ExceededGracePeriod 7s kubelet Container runtime did not kill the pod within specified grace period. 노드 리소스를 확인하면 jupyter notebook 파드의 memory limit 값이 비이상적으로 설정되어 생기는 원인임을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 665m (16%) 2600m (66%) memory 1184Mi (1%) 3167538380800m (5%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 nvidia.com/gpu 1 1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 109s kube-proxy Normal RegisteredNode 2m44s node-controller Node ip-192-168-1-190.ap-northeast-2.compute.internal event: Registered Node ip-192-168-1-190.ap-northeast-2.compute.internal in Controller Normal Starting 2m3s kubelet Starting kubelet. Warning InvalidDiskCapacity 2m3s kubelet invalid capacity 0 on image filesystem Normal NodeHasSufficientMemory 2m3s (x3 over 2m3s) kubelet Node ip-192-168-1-190.ap-northeast-2.compute.internal status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 2m3s (x3 over 2m3s) kubelet Node ip-192-168-1-190.ap-northeast-2.compute.internal status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 2m3s (x3 over 2m3s) kubelet Node ip-192-168-1-190.ap-northeast-2.compute.internal status is now: NodeHasSufficientPID Normal NodeAllocatableEnforced 2m3s kubelet Updated Node Allocatable limit across pods Normal NodeReady 107s kubelet Node ip-192-168-1-190.ap-northeast-2.compute.internal status is now: NodeReady Normal Unconsolidatable 66s karpenter provisioner default has consolidation disabled # 모으는 설정 비활성화 Warning EvictionThresholdMet 25s kubelet Attempting to reclaim ephemeral-storage Normal NodeHasDiskPressure 20s kubelet Node ip-192-168-1-190.ap-northeast-2.compute.internal status is now: NodeHasDiskPressure 파드 limit 값을 수정하면 해결되지만,, kubeflow 플랫폼을 사용할 때마다 limit 값을 수정할 수 는 없기에 깃 이슈를 생성해둔 상태이다. 이슈가 해결되면 업데이트하겠다.\nhttps://github.com/awslabs/kubeflow-manifests/issues/748\n이슈를 뒤져보니,, 비슷한 이슈가 있었다. 개발자 측에서 인지하고 있는 문제로 다음 버전 업데이트에 해결될 것 같다.\nhttps://github.com/awslabs/kubeflow-manifests/issues/748\n","date":"May 26","permalink":"https://HanHoRang31.github.io/post/karpenter-deepdive/","tags":["AEWS","kubeflow","cloud","AWS","eksctl","eks","karpenter","kubeflow","keda"],"title":"[AEWS] karpenter DeeP Dive (Feat. 오버프로비저닝, kubeflow)"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. EKS VPC CNI CNI(Container Network Interface) 란 네트워크 플러그인 인터페이스로 k8s 네트워크 환경을 구성해준다. EKS 에서는 기본 CNI로 VPC를 사용한다. 여기서 VPC는 AWS 네트워크 서비스인 AWS VPC로 VPC CIDR을 이용하여 파드 IP 할당 및 통신을 지원한다.\nEKS 에서 CNI를 VPC로 사용하면 대표적으로 얻을 수 있는 이점은 두 가지이다.\n노드 대역과 파드 대역이 동일하여 다른 노드간 통신시 오버레이가 없어 통신 오버헤드가 감소한다.\n다른 CNI(Caclico)는 노드와 파드 IP 대역이 달라 노드 간 통신에 오버레이를 사용하는 반면 VPC CNI는 대역이 같아 오버레이없이 원본 패킷 그대로 통신한다.\nAWS VPC 의 연동 서비스 (VPC 서브넷, 보안 그룹)를 사용하여 트래픽 제어가 가능하다.\n네트워크 세부 정보 확인하기 VPC CNI에 대해 세부적으로 확인해보겠다. VPC CNI는 aws-cni 파드를 통해 각 노드에 배포된다. 이를 확인하여 네트워크 기본 정보를 확인하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # CNI 버전 정보 확인 kubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d \u0026#34;/\u0026#34; -f 2 --- amazon-k8s-cni-init:v1.12.6-eksbuild.1 amazon-k8s-cni:v1.12.6-eksbuild.1 # VPC MODE 확인 kubectl describe cm -n kube-system kube-proxy-config | grep mode --- mode: \u0026#34;iptables\u0026#34; # 노드 IP 및 PUBLIC IP 확인 aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output table --- ------------------------------------------------------------------------- | DescribeInstances | +------------------------+----------------+------------------+----------+ | InstanceName | PrivateIPAdd | PublicIPAdd | Status | +------------------------+----------------+------------------+----------+ | hanhorang-ng1-Node | 192.168.3.162 | 15.164.220.246 | running | | hanhorang-ng1-Node | 192.168.2.16 | 3.34.30.94 | running | | hanhorang-bastion-EC2 | 192.168.1.100 | 13.125.121.220 | running | | hanhorang-ng1-Node | 192.168.1.5 | 3.36.127.19 | running | +------------------------+----------------+------------------+----------+ # kubectl get pods -n kube-system -o wide --- NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES aws-load-balancer-controller-7857849d69-qc9nr 1/1 Running 0 146m 192.168.1.190 ip-192-168-1-5.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; aws-load-balancer-controller-7857849d69-qjvkk 1/1 Running 0 146m 192.168.3.186 ip-192-168-3-162.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; aws-node-l2twq 1/1 Running 0 170m 192.168.1.5 ip-192-168-1-5.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; aws-node-tj9xl 1/1 Running 0 170m 192.168.3.162 ip-192-168-3-162.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; aws-node-v9m8g 1/1 Running 0 170m 192.168.2.16 ip-192-168-2-16.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; coredns-6777fcd775-2qq92 1/1 Running 0 168m 192.168.2.227 ip-192-168-2-16.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; coredns-6777fcd775-rhqdp 1/1 Running 0 168m 192.168.1.254 ip-192-168-1-5.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; external-dns-6b5bbbf9d-l7ms2 1/1 Running 0 143m 192.168.2.221 ip-192-168-2-16.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-bbgr9 1/1 Running 0 168m 192.168.3.162 ip-192-168-3-162.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-nm8ls 1/1 Running 0 169m 192.168.2.16 ip-192-168-2-16.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-xm2qq 1/1 Running 0 169m 192.168.1.5 ip-192-168-1-5.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;** VPC CNI Mode로 userspace ,iptables , ipvs 모드가 존재하나, 보통 iptable 모드를 사용한다. 이유는 userspace 경우, 느리고 비효율적이여서 거의 사용되지 않은 상태이다. 반면 ipvs 의 경우, L4 기반의 로드밸런서로 매우 빠른 이점이 있지만 호환성 문제로 추가 설정이 필요하고 커뮤니티 규모가 적어 트러블슈팅이 동반되어 기본적으로 사용하지 않는다. 이어서 각 노드에 접속하여 네트워크 정보를 확인해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 # 각 노드 IP 확인 및 변수 설정 kubectl get nodes -o wide --- NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ip-192-168-1-5.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 3h10m v1.24.11-eks-a59e1f0 192.168.1.5 3.36.127.19 Amazon Linux 2 5.10.178-162.673.amzn2.x86_64 containerd://1.6.19 ip-192-168-2-16.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 3h10m v1.24.11-eks-a59e1f0 192.168.2.16 3.34.30.94 Amazon Linux 2 5.10.178-162.673.amzn2.x86_64 containerd://1.6.19 ip-192-168-3-162.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 3h10m v1.24.11-eks-a59e1f0 192.168.3.162 15.164.220.246 Amazon Linux 2 5.10.178-162.673.amzn2.x86_64 containerd://1.6.19 # 위 External IP 확인하여 노드별 변수 설정 N1=3.36.127.19 N2=3.34.30.94 N3=15.164.220.246 # 각 노드에 툴 설치 ssh ec2-user@$N1 sudo yum install links tree jq tcpdump -y ssh ec2-user@$N2 sudo yum install links tree jq tcpdump -y ssh ec2-user@$N3 sudo yum install links tree jq tcpdump -y # 네트워크 CNI 로그 경로 확인 ssh ec2-user@$N1 tree /var/log/aws-routed-eni --- /var/log/aws-routed-eni ├── egress-v4-plugin.log # 이그래스 트래픽 관련 로그 파일 ├── ipamd.log # eni IP 주소 할당, 해제, 그리고 오류 등과 관련된 이벤트와 상태 변경 로그 파일 └── plugin.log # 파드의 네트워크 인터페이스 설정, 트래픽 관리, 오류 처리 등과 관련된 이벤트와 상태 변경 로그 파일** # eni IP 주소 할당, 해제, 그리고 오류 등과 관련된 이벤트와 상태 변경 로그 파일 ssh ec2-user@$N1 cat /var/log/aws-routed-eni/ipamd.log | jq --- { \u0026#34;level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2023-05-08T07:16:00.757Z\u0026#34;, \u0026#34;caller\u0026#34;: \u0026#34;ipamd/ipamd.go:1599\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Adding 192.168.1.190/32 to DS for eni-0d79c0daf04234458\u0026#34; } { \u0026#34;level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2023-05-08T07:16:00.757Z\u0026#34;, \u0026#34;caller\u0026#34;: \u0026#34;ipamd/ipamd.go:1599\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;IP already in DS\u0026#34; } { \u0026#34;level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2023-05-08T07:16:00.757Z\u0026#34;, \u0026#34;caller\u0026#34;: \u0026#34;ipamd/ipamd.go:1475\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Trying to add 192.168.1.145\u0026#34; } { \u0026#34;level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2023-05-08T07:16:00.757Z\u0026#34;, \u0026#34;caller\u0026#34;: \u0026#34;ipamd/ipamd.go:1599\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Adding 192.168.1.145/32 to DS for eni-0d79c0daf04234458\u0026#34; } ... # 파드의 네트워크 인터페이스 설정, 트래픽 관리, 오류 처리 등과 관련된 이벤트와 상태 변경 로그 파일 ssh ec2-user@$N1 cat /var/log/aws-routed-eni/plugin.log | jq --- { \u0026#34;level\u0026#34;: \u0026#34;debug\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2023-05-08T05:59:41.741Z\u0026#34;, \u0026#34;caller\u0026#34;: \u0026#34;driver/driver.go:281\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Successfully disabled IPv6 RA and ICMP redirects on hostVeth eni25c78048487\u0026#34; } { \u0026#34;level\u0026#34;: \u0026#34;debug\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2023-05-08T05:59:41.741Z\u0026#34;, \u0026#34;caller\u0026#34;: \u0026#34;driver/driver.go:297\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Successfully setup container route, containerAddr=192.168.1.190/32, hostVeth=eni25c78048487, rtTable=main\u0026#34; } { \u0026#34;level\u0026#34;: \u0026#34;debug\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2023-05-08T05:59:41.741Z\u0026#34;, \u0026#34;caller\u0026#34;: \u0026#34;driver/driver.go:297\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Successfully setup toContainer rule, containerAddr=192.168.1.190/32, rtTable=main\u0026#34; } ... 직접 라우터 및 네트워크 인터페이스를 확인하여 파드 IP정보를 확인할 수 있다.\n빨간 네모는 노드 IP로 프라이빗 IPv4 주소가 할당되어 있음을 확인할 수 있다. 또한 노란 네모 및 선이 파드 할당이 되면 생기는 IP로 보조 프라이빗 IPv4 주소가 할당됨을 알 수 있다. 노드에 파드가 할당되면 네트워크인터페이스가 eniY@N 의 네트워크 인터페이스가 할당된다. 라우터 정보를 확인했을 때 노드 1의 CIDR이 192.168.1.0/24 로 해당 IP 대역의 트래픽을 받으면 192.168.1.5 로 통신됨을 확인할 수 있다. 통신 흐름 확인하기 이렇게 할당된 파드의 IP가 어떻게 통신하는지 파드간 통신과 외부 통신을 나눠 통신 흐름을 확인해보자. 먼저 클러스터 내부 파드간 통신을 확인하겠다. 테스트 파드를 3개 배포하여 파드 1과 파드2의 IP 통신흐름을 확인할 것이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 테스트 파드 배포 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: netshoot-pod spec: replicas: 3 selector: matchLabels: app: netshoot-pod template: metadata: labels: app: netshoot-pod spec: containers: - name: netshoot-pod image: nicolaka/netshoot command: [\u0026#34;tail\u0026#34;] args: [\u0026#34;-f\u0026#34;, \u0026#34;/dev/null\u0026#34;] terminationGracePeriodSeconds: 0 EOF # 배포 확인 kubectl get pods -A ---- NAMESPACE NAME READY STATUS RESTARTS AGE default netshoot-pod-7757d5dd99-9dr8p 1/1 Running 0 90s default netshoot-pod-7757d5dd99-jw4z4 1/1 Running 0 90s default netshoot-pod-7757d5dd99-tgwj7 1/1 Running 0 90s 내부 통신은 tcpdump 의 패킷 덤프를 통해 통신 과정을 확인할 수 있다. 파드1에 접속하여 파드2에 직접 트래픽을 쏴서 통신 흐름을 확인하자\n패킷에서와 같이 오버레이없이 파드에 할당된 IP로 직접 통신함을 알 수 있다. 이어서 외부 통신을 진행해보자. 외부 통신은 SNAT을 통해 진행하게 되는데 SNAT의 경우 iptable의 룰을 통해 확인할 수 있다.\nIptable 룰에 따라 AWS-SNAT-CHAIN-0, AWS-SNAT-CHAIN-1 에 매칭되어, 목적지가 192.168.0.0/16 아니고 외부 빠져나갈때 SNAT 192.168.1.5로 변경되어 나간다 파드 안에서 외부 네트워크로 통신을 하여 통신 흐름을 확인하자\n통신 패킷을 통해 확인하면 iptable의 룰과 같이 192.168.1.5로 IP가 SNAT되어 통신됨을 알 수 있다. 네트워크 Addon 설치(ALB, External DNS) 이어서 EKS에서 제공하는 네트워크 애드온 중 ALB Controller와 External DNS를 살펴보고 배포하겠다. 각 addon을 간략하게 요약해서 표로 나타내면 다음과 같다.\nAddon 서비스 사용 AWS 서비스 목적 Load Balancer Controller AWS Elastic Load Balancer(ELB) 네트워크 로드밸런서(L4, L7) 사용 External DNS Controller AWS Route53 DNS 도메인 연동 및 사용 표로 확인할 수 있겠지만 네트워크 addon들은 AWS 네트워크 서비스(ELB, Route53)을 연동해서 사용한다. 그렇기 때문에 Addon 설치시 AWS 서비스를 사용하기 위한 보안 정책(IAM) 연동이 필요하다. 이번 글에서는 IAM 정책 연동 방법으로 IAM OIDC를 통해 EKS 서비스어카운트에 IAM 정책을 연동을 하겠다.\nALB Controller 설치 IAM 정책 생성 및 서비스 어카운트 연동하겠다. 다음의 스크립트를 통해 진행하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # ALB controller 정책 설치 curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.7/docs/install/iam_policy.json aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json # 정책 arn 생성 확인 ACCOUNT_ID={AWS 계정 넘버} aws iam get-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --query \u0026#39;Policy.Arn\u0026#39; --- \u0026#34;arn:aws:iam::955963799952:policy/AWSLoadBalancerControllerIAMPolicy\u0026#34; # OIDC 서비스 어카운트 생성 CLUSTER_NAME={EKS 클러스터 이름} eksctl create iamserviceaccount --cluster=$CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller \\ --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve # OIDC 서비스 어카운트 확인 kubectl edit sa/aws-load-balancer-controller -n kube-system --- apiVersion: v1 kind: ServiceAccount metadata: annotations: # annoation 에 role-arn이 추가된 것을 확인! eks.amazonaws.com/role-arn: arn:aws:iam::955963799952:role/eksctl-hanhorang-addon-iamserviceaccount-kub-Role1-1MPK8ATNJVXQH creationTimestamp: \u0026#34;2023-05-08T05:32:22Z\u0026#34; labels: app.kubernetes.io/managed-by: eksctl name: aws-load-balancer-controller namespace: kube-system resourceVersion: \u0026#34;1236\u0026#34; uid: 5fe3e69b-8cfc-426c-a16a-509e1f7c4a86 서비스 어카운트 연동이 확인이 되었으면 ALB 을 설치하겠다. 설치는 Helm을 통해 진행했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # ALB 설치 helm repo add eks https://aws.github.io/eks-charts helm repo update helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$CLUSTER_NAME \\ --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller # ALB 파드 확인 kubectl get pods -A --- NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-load-balancer-controller-7857849d69-qc9nr 1/1 Running 0 24m kube-system aws-load-balancer-controller-7857849d69-qjvkk 1/1 Running 0 24m # ALB 로그 확인 kubectl logs pods/aws-load-balancer-controller-7857849d69-qc9nr -n kube-system --- {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;version\u0026#34;,\u0026#34;GitVersion\u0026#34;:\u0026#34;v2.5.1\u0026#34;,\u0026#34;GitCommit\u0026#34;:\u0026#34;06abaed66e17a411ba064f34e6018b889780ac66\u0026#34;,\u0026#34;BuildDate\u0026#34;:\u0026#34;2023-04-17T22:36:53+0000\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.metrics\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Metrics server is starting to listen\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;:8080\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;setup\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;adding health check for controller\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/mutate-v1-pod\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/mutate-v1-service\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/validate-elbv2-k8s-aws-v1beta1-ingressclassparams\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/mutate-elbv2-k8s-aws-v1beta1-targetgroupbinding\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/validate-elbv2-k8s-aws-v1beta1-targetgroupbinding\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/validate-networking-v1-ingress\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;setup\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;starting podInfo repo\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:47Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook.webhooks\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Starting webhook server\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:47Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Starting server\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;health probe\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;[::]:61779\u0026#34;} ... External DNS 설치 External DNS 설치 과정도 앞 ALB 설치 과정과 동일하다. IAM OIDC 연동을 진행하고, 배포를 진행하겠다.\n먼저, 스크립트를 기반으로 OIDC 정책 연동부터 진행하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # vi iam_external_policy.json 생성 cat \u0026lt;\u0026lt;EOT \u0026gt; iam_external_policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ChangeResourceRecordSets\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:route53:::hostedzone/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } EOT aws iam create-policy --policy-name \u0026#34;AllowExternalDNSUpdates\u0026#34; --policy-document file://iam_external_policy.json.json # 정책 arn 생성 확인 aws iam get-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AllowExternalDNSUpdates --query \u0026#39;Policy.Arn\u0026#39; --- \u0026#34;arn:aws:iam::955963799952:policy/AllowExternalDNSUpdates\u0026#34; # OIDC 서비스 어카운트 생성 eksctl create iamserviceaccount --cluster=$CLUSTER_NAME --namespace=kube-system --name=external-dns \\ --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AllowExternalDNSUpdates --override-existing-serviceaccounts --approve # OIDC 서비스 어카운트 확인 kubectl edit sa/external-dns -n kube-system --- apiVersion: v1 kind: ServiceAccount metadata: annotations: # 마찬가지로 연동 확인이 가능하다. eks.amazonaws.com/role-arn: arn:aws:iam::955963799952:role/eksctl-hanhorang-addon-iamserviceaccount-kub-Role1-499RATDKJVJU kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;ServiceAccount\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app.kubernetes.io/name\u0026#34;:\u0026#34;external-dns\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;external-dns\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;}} creationTimestamp: \u0026#34;2023-05-08T06:02:38Z\u0026#34; labels: app.kubernetes.io/managed-by: eksctl app.kubernetes.io/name: external-dns name: external-dns namespace: kube-system resourceVersion: \u0026#34;13937\u0026#34; uid: 67ce20cc-6545-486c-b525-450be6311d65 external DNS을 연동하기 위해서는 도메인이 필요하다. 필자는 Route53 도메인을 구입했다. 도메인 구매 후 도메인 HOST ID를 변수로 지정하여 external DNS 설치시 파라미터로 사용하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # MyDomain=\u0026lt;자신의 도메인\u0026gt; MyDomain=hanhorang.link # 자신의 Route 53 도메인 ID 조회 및 변수 지정 MyDnzHostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name \u0026#34;${MyDomain}.\u0026#34; --query \u0026#34;HostedZones[0].Id\u0026#34; --output text) # ExternalDNS 배포 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/aews/externaldns.yaml cat externaldns.yaml | yh MyDomain=$MyDomain MyDnzHostedZoneId=$MyDnzHostedZoneId envsubst \u0026lt; externaldns.yaml | kubectl apply -f - # 로그 확인 kubectl get pods -A --- kube-system external-dns-6b5bbbf9d-l7ms2 1/1 Running 0 4 kubectl logs pods/external-dns-6b5bbbf9d-l7ms2 -n kube-system --- time=\u0026#34;2023-05-08T06:02:43Z\u0026#34; level=info msg=\u0026#34;config: {APIServerURL: KubeConfig: RequestTimeout:30s DefaultTargets:[] ContourLoadBalancerService:heptio-contour/contour GlooNamespace:gloo-system SkipperRouteGroupVersion:zalando.org/v1 Sources:[service ingress] Namespace: AnnotationFilter: LabelFilter: FQDNTemplate: CombineFQDNAndAnnotation:false IgnoreHostnameAnnotation:false IgnoreIngressTLSSpec:false IgnoreIngressRulesSpec:false GatewayNamespace: GatewayLabelFilter: Compatibility: PublishInternal:false PublishHostIP:false AlwaysPublishNotReadyAddresses:false ConnectorSourceServer:localhost:8080 Provider:aws GoogleProject: GoogleBatchChangeSize:1000 GoogleBatchChangeInterval:1s GoogleZoneVisibility: DomainFilter:[hanhorang.link] ExcludeDomains:[] RegexDomainFilter: RegexDomainExclusion: ZoneNameFilter:[] ZoneIDFilter:[] TargetNetFilter:[] ExcludeTargetNets:[] AlibabaCloudConfigFile:/etc/kubernetes/alibaba-cloud.json AlibabaCloudZoneType: AWSZoneType:public AWSZoneTagFilter:[] AWSAssumeRole: AWSAssumeRoleExternalID: AWSBatchChangeSize:1000 AWSBatchChangeInterval:1s AWSEvaluateTargetHealth:true AWSAPIRetries:3 AWSPreferCNAME:false AWSZoneCacheDuration:0s AWSSDServiceCleanup:false AzureConfigFile:/etc/kubernetes/azure.json AzureResourceGroup: AzureSubscriptionID: AzureUserAssignedIdentityClientID: BluecatDNSConfiguration: BluecatConfigFile:/etc/kubernetes/bluecat.json BluecatDNSView: BluecatGatewayHost: BluecatRootZone: BluecatDNSServerName: BluecatDNSDeployType:no-deploy BluecatSkipTLSVerify:false CloudflareProxied:false CloudflareDNSRecordsPerPage:100 CoreDNSPrefix:/skydns/ RcodezeroTXTEncrypt:false AkamaiServiceConsumerDomain: AkamaiClientToken: AkamaiClientSecret: AkamaiAccessToken: AkamaiEdgercPath: AkamaiEdgercSection: InfobloxGridHost: InfobloxWapiPort:443 InfobloxWapiUsername:admin InfobloxWapiPassword: InfobloxWapiVersion:2.3.1 InfobloxSSLVerify:true InfobloxView: InfobloxMaxResults:0 InfobloxFQDNRegEx: InfobloxNameRegEx: InfobloxCreatePTR:false InfobloxCacheDuration:0 DynCustomerName: DynUsername: DynPassword: DynMinTTLSeconds:0 OCIConfigFile:/etc/kubernetes/oci.yaml InMemoryZones:[] OVHEndpoint:ovh-eu OVHApiRateLimit:20 PDNSServer:http://localhost:8081 PDNSAPIKey: PDNSTLSEnabled:false TLSCA: TLSClientCert: TLSClientCertKey: Policy:sync Registry:txt TXTOwnerID:/hostedzone/Z08463751O7YNWD79KKIX TXTPrefix: TXTSuffix: Interval:1m0s MinEventSyncInterval:5s Once:false DryRun:false UpdateEvents:false LogFormat:text MetricsAddress::7979 LogLevel:info TXTCacheInterval:0s TXTWildcardReplacement: ExoscaleEndpoint:https://api.exoscale.ch/dns ExoscaleAPIKey: ExoscaleAPISecret: CRDSourceAPIVersion:externaldns.k8s.io/v1alpha1 CRDSourceKind:DNSEndpoint ServiceTypeFilter:[] CFAPIEndpoint: CFUsername: CFPassword: RFC2136Host: RFC2136Port:0 RFC2136Zone: RFC2136Insecure:false RFC2136GSSTSIG:false RFC2136KerberosRealm: RFC2136KerberosUsername: RFC2136KerberosPassword: RFC2136TSIGKeyName: RFC2136TSIGSecret: RFC2136TSIGSecretAlg: RFC2136TAXFR:false RFC2136MinTTL:0s RFC2136BatchChangeSize:50 NS1Endpoint: NS1IgnoreSSL:false NS1MinTTLSeconds:0 TransIPAccountName: TransIPPrivateKeyFile: DigitalOceanAPIPageSize:50 ManagedDNSRecordTypes:[A CNAME] GoDaddyAPIKey: GoDaddySecretKey: GoDaddyTTL:0 GoDaddyOTE:false OCPRouterName: IBMCloudProxied:false IBMCloudConfigFile:/etc/kubernetes/ibmcloud.json TencentCloudConfigFile:/etc/kubernetes/tencent-cloud.json TencentCloudZoneType: PiholeServer: PiholePassword: PiholeTLSInsecureSkipVerify:false PluralCluster: PluralProvider:}\u0026#34; time=\u0026#34;2023-05-08T06:02:43Z\u0026#34; level=info msg=\u0026#34;Instantiating new Kubernetes client\u0026#34; time=\u0026#34;2023-05-08T06:02:43Z\u0026#34; level=info msg=\u0026#34;Using inCluster-config based on serviceaccount-token\u0026#34; time=\u0026#34;2023-05-08T06:02:43Z\u0026#34; level=info msg=\u0026#34;Created Kubernetes client https://10.100.0.1:443\u0026#34; time=\u0026#34;2023-05-08T06:02:45Z\u0026#34; level=info msg=\u0026#34;Applying provider record filter for domains: [hanhorang.link. .hanhorang.link.]\u0026#34; time=\u0026#34;2023-05-08T06:02:45Z\u0026#34; level=info msg=\u0026#34;All records are already up to date\u0026#34; 설치 이후 반드시 로그를 확인하는 것이 중요하다. 필자의 경험에서 도메인 연동이 잘못되었다던가, IAM 정책 미스로 addon 연동이 잘못되면 로그를 통해 상세 확인이 가능하기 떄문이다.\nEKS VPC 운영 EKS Workshop Networking 를 토대로 필자가 실습한 내용들을 공유한다.\nPrefix Delegation을 통한 노드별 할당 IP 확장하기 EKS 노드는 EC2 인스턴스로 운영된다. 문제는 EC2 인스턴스 타입별로 할당되는 IP 개수의 제한이 존재한다. 이유는 인스턴스 타입당 할당가능한 네트워크 인터페이스의 제한이 있기 때문이다. 인스턴스별 IP 개수 제한 관련하여 다음의 명령어를 통해서도 확인할 수 있다.\n1 2 3 4 5 kubectl get nodes -o jsonpath=\u0026#34;{range .items[*]}{.metadata.labels[\u0026#39;beta\\.kubernetes\\.io\\/instance-type\u0026#39;]}{\u0026#39;\\t\u0026#39;}{.status.capacity.pods}{\u0026#39;\\n\u0026#39;}{end}\u0026#34; --- t3.medium 17 t3.medium 17 t3.medium 17 기존에는 IP 할당 개수를 늘리기 위해서 인스턴스 타입을 변경해야 했지만, Amazon VPC CNI 추가 기능 버전 1.9.0 이상부터 Prefix Delegation 를 통해 최대 파드 개수까지 IP 할당 개수를 확장할 수 있다. Prefix Delegation 란 IPv6 기능으로 하위 IP에 접두사를 위임하여 최대 IP를 할당할 수 있게 만들어 주는 기능이다. 이를 VPC CNI에도 사용할 수 있다. IP Mode와 Prefix Mode 를 비교하면 다음과 같이 나타낼 수 있다.\n오른쪽이 Prefix Mode로 Secondary ENI에 접두사가 위임된 것을 확인할 수 있다. 비교하자면 개별 보조 IP 주소를 할당하는 대신 접두사를 할당하여 최대 IP 개수늘 늘릴 수 있다. 좋은 기능이지만 사용하기에 몇 가지 제한이 존재한다. 기능 도입 전 제한 사항을 꼭 확인하자!\nVPC CNI version 1.9.0 이상 Nitro 기반의 인스턴스에서만 가능 /28 접두사를 생성할 수 있는 사용 가능한 IP 주소가 충분하지 않은 경우 바로 VPC CNI 파드를 통해 설정해보겠다. 먼저, 버전 확인과 VPC CNI의 옵션을 살펴보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # VPC CNI 버전 확인 1.9.0 이상 kubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d \u0026#34;/\u0026#34; -f 2 --- amazon-k8s-cni-init:v1.12.6-eksbuild.1 amazon-k8s-cni:v1.12.6-eksbuild.1 # Prefix 관련 옵션 확인 (주석 처리 파라미터 확인) kubectl describe daemonsets.apps aws-node -n kube-system | grep ADDITIONAL_ENI_TAGS: -B1 -A26 Environment: ADDITIONAL_ENI_TAGS: {} ANNOTATE_POD_IP: false AWS_VPC_CNI_NODE_PORT_SUPPORT: true AWS_VPC_ENI_MTU: 9001 AWS_VPC_K8S_CNI_CONFIGURE_RPFILTER: false AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG: false AWS_VPC_K8S_CNI_EXTERNALSNAT: false AWS_VPC_K8S_CNI_LOGLEVEL: DEBUG AWS_VPC_K8S_CNI_LOG_FILE: /host/var/log/aws-routed-eni/ipamd.log AWS_VPC_K8S_CNI_RANDOMIZESNAT: prng AWS_VPC_K8S_CNI_VETHPREFIX: eni AWS_VPC_K8S_PLUGIN_LOG_FILE: /var/log/aws-routed-eni/plugin.log AWS_VPC_K8S_PLUGIN_LOG_LEVEL: DEBUG CLUSTER_ENDPOINT: https://65CBAF476ED2A8986CC4BAFE19F86C44.yl4.ap-northeast-2.eks.amazonaws.com CLUSTER_NAME: hanhorang DISABLE_INTROSPECTION: false DISABLE_METRICS: false DISABLE_NETWORK_RESOURCE_PROVISIONING: false ENABLE_IPv4: true ENABLE_IPv6: false ENABLE_POD_ENI: false ENABLE_PREFIX_DELEGATION: false # PREFIX 관련 옵션 VPC_ID: vpc-0cc614fae36493a12 WARM_ENI_TARGET: 1 # PREFIX 관련 옵션 WARM_PREFIX_TARGET: 1 # PREFIX 관련 옵션 MY_NODE_NAME: (v1:spec.nodeName) MY_POD_NAME: (v1:metadata.name) #MINIMUM_IP_TARGET: # PREFIX 관련 옵션 Prefix 관련 파라미터의 옵션은 다음과 같다.\nENABLE_PREFIX_DELEGATION : Prefix Delegation 활성화 여부 WARM_PREFIX_TARGET : 현재 초과하여 할당할 접두사 수 WARM_IP_TARGET, MINIMUM_IP_TARGET : (WARM_PREFIX_TARGET를 설정하면 오버라이드 됨) 할당할 IP 수와 최소 IP 주소 수 파라미터에서 WARM이라는 표현이 등장하는데, CNI 는 WARM Pool(웜풀) 이라고 하여 더 빠른 파드 시작을 위해 IP 및 접두사를 사전 할당하는 기능이다. 이 웜풀을 통해서 IP와 접두사를 파드에 할당받는다.\nhttps://aws.github.io/aws-eks-best-practices/networking/prefix-mode/\n더 많은 파드가 예약되면 ENI에 프리픽스를 요청하고, 만약에 ENI가 사용량에 도달하면 새 ENI를 할당하려고 시도하여 연결한다. 새 ENI는 최대 ENI 제한에 도달할 때까지 연결된다. 살펴본 파라미터를 통해 접두사 모드를 활성해보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 kubectl set env daemonset aws-node -n kube-system ENABLE_PREFIX_DELEGATION=true --- daemonset.apps/aws-node env updated kubectl describe daemonsets.apps aws-node -n kube-system | grep ADDITIONAL_ENI_TAGS: -B1 -A26 --- Environment: ADDITIONAL_ENI_TAGS: {} ANNOTATE_POD_IP: false AWS_VPC_CNI_NODE_PORT_SUPPORT: true AWS_VPC_ENI_MTU: 9001 AWS_VPC_K8S_CNI_CONFIGURE_RPFILTER: false AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG: false AWS_VPC_K8S_CNI_EXTERNALSNAT: false AWS_VPC_K8S_CNI_LOGLEVEL: DEBUG AWS_VPC_K8S_CNI_LOG_FILE: /host/var/log/aws-routed-eni/ipamd.log AWS_VPC_K8S_CNI_RANDOMIZESNAT: prng AWS_VPC_K8S_CNI_VETHPREFIX: eni AWS_VPC_K8S_PLUGIN_LOG_FILE: /var/log/aws-routed-eni/plugin.log AWS_VPC_K8S_PLUGIN_LOG_LEVEL: DEBUG CLUSTER_ENDPOINT: https://65CBAF476ED2A8986CC4BAFE19F86C44.yl4.ap-northeast-2.eks.amazonaws.com CLUSTER_NAME: hanhorang DISABLE_INTROSPECTION: false DISABLE_METRICS: false DISABLE_NETWORK_RESOURCE_PROVISIONING: false ENABLE_IPv4: true ENABLE_IPv6: false ENABLE_POD_ENI: false ENABLE_PREFIX_DELEGATION: true # 변경되었다! VPC_ID: vpc-0cc614fae36493a12 WARM_ENI_TARGET: 1 WARM_PREFIX_TARGET: 1 MY_NODE_NAME: (v1:spec.nodeName) MY_POD_NAME: (v1:metadata.name) 설정이 확인되면, 테스트용 파드를 150개 배포해보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: netshoot-pod spec: replicas: 150 selector: matchLabels: app: netshoot-pod template: metadata: labels: app: netshoot-pod spec: containers: - name: netshoot-pod image: nicolaka/netshoot command: [\u0026#34;tail\u0026#34;] args: [\u0026#34;-f\u0026#34;, \u0026#34;/dev/null\u0026#34;] terminationGracePeriodSeconds: 0 EOF # 이상하다 파드가 43개밖에 할당되지 않았다. kubectl get deploy --- NAME READY UP-TO-DATE AVAILABLE AGE netshoot-pod 43/150 150 43 19s Pending 된 파드의 이벤트를 확인하니 노드 제한으로 파드 할당이 되지 않았다.\n1 2 3 4 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 88s default-scheduler 0/3 nodes are available: 3 Too many pods. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod. 확인해보니 노드에서 MaX 파드 수치가 존재하기 때문이였다. 앞서 본 17개가 해당 인스턴스의 맥스 파드 수였다. 이 MAX 파드 제한을 늘리기 위해서는 워커 노드의 kubelet의 옵션을 수정해야 한다. 방법은 두 가지로 1. EKS 구축시 MAX POD 설정 및 2. 실행 중인 노드 kubelet을 수정하여 MAX POD를 설정할 수 있다.\nEksctl 파라미터를 통한 MAX POD 설정 1 2 3 4 5 6 7 eksctl create nodegroup \\ --cluster hanhorang \\ --region ap-northeast-2 \\ --name ng-1 \\ --node-type t3.medium \\ --managed \\ --max-pods-per-node 100 # 해당 파라미터 실행중인 노드 kubelet 수정 ( kimalram 님 블로그 글을 참고하였습니다! ) 실행 중인 노드에 접속하여 kubelet 옵션의 MAX 파드를 수정할 것이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 # 노드 IP 확인 kubectl get nodes -o wide --- ssNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ip-192-168-1-209.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 3h16m v1.24.11-eks-a59e1f0 192.168.1.209 52.78.121.165 Amazon Linux 2 5.10.178-162.673.amzn2.x86_64 containerd://1.6.19 ip-192-168-2-134.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 3h17m v1.24.11-eks-a59e1f0 192.168.2.134 3.36.17.42 Amazon Linux 2 5.10.178-162.673.amzn2.x86_64 containerd://1.6.19 ip-192-168-3-250.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 3h17m v1.24.11-eks-a59e1f0 192.168.3.250 54.180.101.37 Amazon Linux 2 5.10.178-162.673.amzn2.x86_64 containerd://1.6.19 # 노드 접속 ssh ec2-user@52.78.121.165 --- The authenticity of host \u0026#39;52.78.121.165 (52.78.121.165)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:gr23rumKlP2+B0Dn0wYN+UPxaCgoyrL/uyhjBZxPCug. ECDSA key fingerprint is MD5:7c:d8:29:c9:5c:09:66:b6:7c:64:62:e6:85:1d:46:ac. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;52.78.121.165\u0026#39; (ECDSA) to the list of known hosts. Last login: Mon May 1 20:27:30 2023 from 205.251.233.237 __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ 3 package(s) needed for security, out of 12 available Run \u0026#34;sudo yum update\u0026#34; to apply all updates. # 노드 eks 설정 부분 경로 접속 cd /etc/eks # kubelet 재실행 sudo /etc/eks/bootstrap.sh hanhorang --use-max-pods false --kubelet-extra-args \u0026#39;--max-pods=110\u0026#39; --- 2023-05-09T11:31:57+0000 [eks-bootstrap] INFO: starting... 2023-05-09T11:31:57+0000 [eks-bootstrap] INFO: --use-max-pods=\u0026#39;false\u0026#39; 2023-05-09T11:31:57+0000 [eks-bootstrap] INFO: --kubelet-extra-args=\u0026#39;--max-pods=110\u0026#39; 2023-05-09T11:31:57+0000 [eks-bootstrap] INFO: Using kubelet version 1.24.11 2023-05-09T11:31:57+0000 [eks-bootstrap] INFO: Using containerd as the container runtime 2023-05-09T11:31:58+0000 [eks-bootstrap] INFO: --cluster-ca or --api-server-endpoint is not defined, describing cluster... 2023-05-09T11:31:59+0000 [eks-bootstrap] INFO: Using IP family: ipv4 [Service] Slice=runtime.slice ‘/etc/eks/containerd/kubelet-containerd.service’ -\u0026gt; ‘/etc/systemd/system/kubelet.service’ 2023-05-09T11:32:01+0000 [eks-bootstrap] INFO: complete! # kubelet 로그 확인 ( 수정이 안됨) sudo ps -ef | grep kubelet -- root 2860 1 1 08:11 ? 00:02:29 /usr/bin/kubelet --config /etc/kubernetes/kubelet/kubelet-config.json --kubeconfig /var/lib/kubelet/kubeconfig --container-runtime-endpoint unix:///run/containerd/containerd.sock --image-credential-provider-config /etc/eks/image-credential-provider/config.json --image-credential-provider-bin-dir /etc/eks/image-credential-provider --node-ip=192.168.1.209 --pod-infra-container-image=602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/eks/pause:3.5 --v=2 --cloud-provider=aws --container-runtime=remote --node-labels=eks.amazonaws.com/sourceLaunchTemplateVersion=1,alpha.eksctl.io/cluster-name=hanhorang,alpha.eksctl.io/nodegroup-name=ng1,eks.amazonaws.com/nodegroup-image=ami-0da378ed846e950a4,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup=ng1,eks.amazonaws.com/sourceLaunchTemplateId=lt-0a84643ab8f551110 --max-pods=17 ec2-user 6100 4064 0 11:34 pts/0 00:00:00 grep --color=auto kubelet # kubelet 재시작 sudo systemctl restart kubelet kubelet 은 Static 파드로 동작하므로 수동으로 재시작을 해줘야 기능이 적용된다. 맥스 파드가 적용되었으면 전에 배포한 150개 파드 동작을 확인할 수 있다.\n1 2 3 4 kubectl get deploy --- NAME READY UP-TO-DATE AVAILABLE AGE netshoot-pod 150/150 150 150 38m CUSTOM Network를 통한 IP 확장하기 서브넷 대역에 IP가 부족했을 때 추가 서브넷 CIDR을 부여하여 IP 할당 개수를 추가로 확장할 수 있다.\n공식 문서에 따르면 Custom CIDR 제약 사항도 존재한다. 적용시 반드시 참고하자\n사용자 지정 네트워킹을 사용 설정하면 기본 네트워크 인터페이스에 할당된 IP 주소가 pods에 할당되지 않는다. 보조 네트워크 인터페이스의 IP 주소만 pods에 할당된다. 클러스터에서 IPv6 패밀리를 사용하는 경우 사용자 지정 네트워킹을 사용할 수 없다. 사용자 지정 네트워킹을 사용하여 IPv4 주소 소모를 완화하려는 경우 대신 IPv6 패밀리를 사용하여 클러스터를 생성할 수 있다. 그러면 서브넷 CIDR 을 추가하겠다. 추가 과정은 다음의 4가지로 진행된다.\nVPC 서브넷 생성 aws-node 파드 파라미터 수정(VPC CNI) ENIConfig (CRD) 배포 생성 및 EKS 노드에 연결 노드 그룹 재배포 VPC 할당 및 서브넷 생성은 AWS 콘솔에서 진행하였다. AWS 콘솔 [VPC] 에서 CIDR 및 서브넷 조작이 가능하다. 다음과 같이 10.64.0.0/16 의 VPC CIDR 과 대역에 맞는 서브넷을 생성하였다.\n생성한 서브넷 ID는 다음의 명령어로 조회가 가능하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #aws ec2 describe-subnets --filters \u0026#34;Name=vpc-id,Values=$vpc_id\u0026#34; --query \u0026#39;Subnets[*].{SubnetId: SubnetId, AvailabilityZone: AvailabilityZone, CidrBlock: CidrBlock}\u0026#39; --output table aws ec2 describe-subnets --filters \u0026#34;Name=vpc-id,Values=vpc-0fc5c162640e67e66\u0026#34; --query \u0026#39;Subnets[*].{SubnetId: SubnetId, AvailabilityZone: AvailabilityZone, CidrBlock: CidrBlock}\u0026#39; --output table --------------------------------------------------------------------- | DescribeSubnets | +------------------+-------------------+----------------------------+ | AvailabilityZone | CidrBlock | SubnetId | +------------------+-------------------+----------------------------+ | ap-northeast-2a | 100.64.1.0/24 | subnet-06daaaa9fa3990545 | | ap-northeast-2c | 192.168.3.0/24 | subnet-0020334582f221a4e | | ap-northeast-2b | 100.64.2.0/24 | subnet-0a498cbb63e30d085 | | ap-northeast-2c | 100.64.3.0/24 | subnet-03de86c554113fec7 | | ap-northeast-2b | 192.168.2.0/24 | subnet-0808f8970f303632f | | ap-northeast-2a | 192.168.11.0/24 | subnet-0bc63c89a9965b672 | | ap-northeast-2c | 192.168.13.0/24 | subnet-0f229c12c2c595c5b | | ap-northeast-2b | 192.168.12.0/24 | subnet-06229a20d32b499de | | ap-northeast-2a | 192.168.1.0/24 | subnet-0de74f64866c7a8e5 | +------------------+-------------------+----------------------------+ 이어서 VPC CNI 파드(aws-node)에 custom network 를 사용하겠다는 파라미터를 추가하자.\n1 2 3 4 5 kubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true kubectl describe daemonsets.apps aws-node -n kube-system | grep CUSTOM ---- AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG: true # 설정 확인 추가 작업으로 서브넷 배포 설정에 대한 eniconfig 파일을 작성하여 배포하도록 하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # eniconfigs.yaml apiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: ap-noratheast-2a spec: securityGroups: - sg-0db212244a3e72ae1 subnet: subnet-06daaaa9fa3990545 --- apiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: ap-noratheast-2b spec: securityGroups: - sg-0db212244a3e72ae1 subnet: subnet-0a498cbb63e30d085 --- apiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: ap-noratheast-2c spec: securityGroups: - sg-0db212244a3e72ae1 subnet: subnet-03de86c554113fec7 추가할 서브넷만 작성하도록 하자. 밑의 노드에 annotation을 추가할 것이기 때문에 AND 로 서브넷이 추가된다.\n보안 그룹은 배포된 워크 노드의 보안 그룹으로 작성하였다.\nmetadata.name 에 되도록 AZ 이름을 입력하도록 하자, AZ 이름이 아니면 노드에 적용시키기 위해서는 아래와 같이 추가 주석이 필요하다.\n1 2 3 kubectl annotate node ip-192-168-1-84.ap-northeast-2.compute.internal k8s.amazonaws.com/eniConfig=ap-noratheast-2a-custom kubectl annotate node ip-192-168-2-219.ap-northeast-2.compute.internal k8s.amazonaws.com/eniConfig=ap-noratheast-2b-custom kubectl annotate node ip-192-168-3-140.ap-northeast-2.compute.internal k8s.amazonaws.com/eniConfig=ap-noratheast-2c-custom 배포 및 노드에 연결하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 kubectl apply -f eniconfigs.yaml --- eniconfig.crd.k8s.amazonaws.com/ap-northeast-2a created eniconfig.crd.k8s.amazonaws.com/ap-northeast-2b created eniconfig.crd.k8s.amazonaws.com/ap-northeast-2c created kubectl get eniconfig --- NAME AGE ap-northeast-2a 16s ap-northeast-2b 16s ap-northeast-2c 16s # VPC 서브넷 업데이트 kubectl set env daemonset aws-node -n kube-system ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone 업데이트된 내용이 노드에는 바로 적용되지 않는다. 새로 추가된 노드에만 적용된다. 새로 노드 그룹을 추가하고 확인하도록 하자.\n1 2 3 4 5 6 eksctl create nodegroup \\ --cluster hanhorang \\ --region ap-northeast-2 \\ --name ng-2 \\ --node-type t3.medium \\ --managed AWS 콘솔을 통해 확인이 가능하다.\nSG for Pod 설정 VPC 보안 그룹을 파드에도 설정할 수 있는 기능이다. 파드에 보안 그룹을 적용하면 파드 접근에 세부적인 보안 관리가 가능해진다.\nhttps://www.eksworkshop.com/docs/networking/security-groups-for-pods/\nSG for Pod를 설정하기 전 몇가지 제한 사항이 존재한다.\n윈도우 노드에서 사용할 수 없다.\n인스턴스 유형 중 t 타입의 인스턴스에서 사용할 수 없다.\nIPv6 패밀리용으로 구성된 클러스터에서 사용할 수 없지만 fargate에서는 사용이 가능하다.\n정책 제한이 있을시 사용자에 SG for Pod를 위한 섹션을 추가 해야 한다.\n1 2 3 4 5 6 7 8 9 10 ... subjects: - kind: Group apiGroup: rbac.authorization.k8s.io name: system:authenticated - apiGroup: rbac.authorization.k8s.io kind: User name: eks:vpc-resource-controller - kind: ServiceAccount name: eks-vpc-resource-controller VPC CNI 버전이 1.7.7 이상이여야 한다.\n이외에도 VPC CNI 버전 별 플래그 마다 제한사항이 존재한다. 세부 내용은 공식 문서를 참고하자.\n공식 문서를 참고하여 SG for Pod 설정을 진행하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 버전 확인 kubectl describe daemonset aws-node --namespace kube-system | grep amazon-k8s-cni: | cut -d : -f 3 --- v1.12.6-eksbuild.1 # EKS VPC ResourceController Role 추가 cluster_role=$(aws eks describe-cluster --name hanhorang --query cluster.roleArn --output text | cut -d / -f 2) # 클러스터에 정책 연결 aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonEKSVPCResourceController --role-name $cluster_role # VPC CNI 파라미터 수정(Pod eni 관리 파라미터 허용) kubectl set env daemonset aws-node -n kube-system ENABLE_POD_ENI=true # 파드 eni 가능 노드 확인 # 위 명령어로 업데이트에 몇 초의 시간이 소요됩니다. kubectl get nodes -o wide -l vpc.amazonaws.com/has-trunk-attached=true --- NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ip-192-168-1-108.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 20m v1.24.11-eks-a59e1f0 192.168.1.108 3.38.214.241 Amazon Linux 2 5.10.178-162.673.amzn2.aarch64 containerd://1.6.19 ip-192-168-2-45.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 20m v1.24.11-eks-a59e1f0 192.168.2.45 3.38.188.199 Amazon Linux 2 5.10.178-162.673.amzn2.aarch64 containerd://1.6.19 ip-192-168-3-18.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 20m v1.24.11-eks-a59e1f0 192.168.3.18 13.125.254.20 Amazon Linux 2 5.10.178-162.673.amzn2.aarch64 containerd://1.6.19 # 파드에서 VPC 외부 주소로 아웃바운드되는 트래픽은 기본 ENI로 가며, 노드 보안 그룹에 대한 규칙이 적용된다, 파드의 eni만 적용시키고 싶다면 아래 파라미터를 추가하자. kubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_EXTERNALSNAT=true # 옵션 1. liveness \u0026amp; readyprobe 사용 경우, kubelet이 TCP를 사용하여 파드 eni 와 연결할 수 있도록 TCP 초기 demux 중지 kubectl patch daemonset aws-node -n kube-system \\ -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;template\u0026#34;: {\u0026#34;spec\u0026#34;: {\u0026#34;initContainers\u0026#34;: [{\u0026#34;env\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;DISABLE_TCP_EARLY_DEMUX\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;true\u0026#34;}],\u0026#34;name\u0026#34;:\u0026#34;aws-vpc-cni-init\u0026#34;}]}}}}\u0026#39; # 옵션 2. externalTrafficPolicy: local, NodeLocal DNSCache, 자체 보안 그룹이 있는 네트워크 정책이 있는 경우 아래 명령어 추가 # 해당 설정을 기존 Pods에 적용하려면, 노드를 재시작해야함 kubectl set env daemonset aws-node -n kube-system POD_SECURITY_GROUP_ENFORCING_MODE=standard SG for Pod 설정 이후 파드에 직접 보안 그룹을 설정하도록 하겠다. 활용 예제는 EKS Workshop 을 참고하였다.\nhttps://www.eksworkshop.com/docs/introduction/getting-started/about\nComponent Description UI 프론트엔드로 사용자 인터페이스를 제공하며 다른 서비스에 대한 API을 연결한다. Catalog 상품 목록과 상세 정보에 대한 API Cart 고객 쇼핑카트에 대한 API Checkout 체크아웃 프로세스를 조정하는 API Orders 고객 주문을 받고 처리하는 API Static assets 상품 카탈로그와 관련된 이미지와 같은 정적 파일 활용 예제는 마이크로서비스 아키텍처의 웹 스토어 어플리케이션으로 아래 Catalog의 DB를 RDS로 변경하고 Catalog Pod에 보안 그룹을 적용하여 RDS 접근 권한을 부여한다. 이렇게 하면 Catalog에서만 RDS에 접근이 가능해진다.\n파일 배포는 다음과 같이 진행하자.\n1 2 3 4 5 6 7 8 git clone https://github.com/aws-samples/eks-workshop-v2.git cd eks-workshop-v2/environment/workspace/manifests # public ecr 로그인 aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws kubectl apply -k . 🧐 배포 트러블슈팅\nEKS 노드에서 이미지 pull 이 안되어 파드 실행이 안되는 문제가 발생했다.\n1 2 3 4 5 6 7 8 9 10 11 12 kubectl describe pods/catalog-mysql-0 -n catalog --- ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 10m default-scheduler Successfully assigned catalog/catalog-mysql-0 to ip-192-168-2-197.ap-northeast-2.compute.internal Normal Pulling 9m19s (x4 over 10m) kubelet Pulling image \u0026#34;public.ecr.aws/docker/library/mysql:5.7\u0026#34; Warning Failed 9m18s (x4 over 10m) kubelet Failed to pull image \u0026#34;public.ecr.aws/docker/library/mysql:5.7\u0026#34;: rpc error: code = NotFound desc = failed to pull and unpack image \u0026#34;public.ecr.aws/docker/library/mysql:5.7\u0026#34;: no match for platform in manifest: not found Warning Failed 9m18s (x4 over 10m) kubelet Error: ErrImagePull Warning Failed 9m5s (x6 over 10m) kubelet Error: ImagePullBackOff Normal BackOff 56s (x42 over 10m) kubelet Back-off pulling image \u0026#34;public.ecr.aws/docker/library/mysql:5.7\u0026#34; 이상한 점은 베스천 서버에서는 이미지 풀이 되는 반면, EKS 노드에서는 이미지 풀에 대해 not found가 발생한다.\n확인해보니 해당 이미지의 경우(ECR) 에서 linux/arm64/v8 에 대한 이미지 레이어 대한 정보가 없어서 발생한 문제였다. linux/arm64/v8 레이어는 AWS 인스턴스 유형 (A1, M6g,C6g,R6g)에서 사용되는 반면 linux/amd64 는 (x86-64) 아키텍처로 AWS 인스턴스 유형(t2,t3 m4, m5) 기반의 프로세서에서 동작한다.\n해결 방법으로 이미지를 직접 빌드 및 Push 할 수 있지만, 인스턴스 유형을 m5.large 로 변경했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 kubectl get pods -A --- NAMESPACE NAME READY STATUS RESTARTS AGE assets assets-97576f445-fbf8m 1/1 Running 0 2m5s carts carts-75d5cc858f-xh474 1/1 Running 0 2m5s carts carts-dynamodb-69b57dddc9-5qcmb 1/1 Running 0 2m5s catalog catalog-6d688b9f9c-9f8sg 1/1 Running 2 (103s ago) 2m5s catalog catalog-mysql-0 1/1 Running 0 2m4s checkout checkout-79d67d6765-r442r 1/1 Running 0 2m5s checkout checkout-redis-cb98f6ff7-qmsk5 1/1 Running 0 2m5s kube-system aws-node-h8rnr 1/1 Running 0 7m15s kube-system aws-node-k7nzq 1/1 Running 0 7m47s kube-system aws-node-mbc76 1/1 Running 0 7m31s kube-system coredns-dc4979556-hfztx 1/1 Running 0 14m kube-system coredns-dc4979556-s8qj7 1/1 Running 0 14m kube-system kube-proxy-h2r6m 1/1 Running 0 9m8s kube-system kube-proxy-jjjmx 1/1 Running 0 9m8s kube-system kube-proxy-np7fj 1/1 Running 0 9m8s orders orders-7fcc4fb7d8-kvb2b 1/1 Running 1 (91s ago) 2m5s orders orders-mysql-5d99464c58-mp6xm 1/1 Running 0 2m5s rabbitmq rabbitmq-0 1/1 Running 0 2m4s ui ui-5b9cf4db94-54v8g 1/1 Running 0 2m4s 이어서 Catalog 의 DB를 mysql에서 AWS RDS 로 변경하기 위한 작업을 진행하겠다. 이를 위해 보안 그룹 생성 및 RDS 생성을 다음과 같이 진행하자.\n보안 그룹 생성(VPC는 EKS VPC로 설정)\nRDS 생성 과정 중 보안 그룹 설정을 앞서 생성한 보안 그룹으로 만들고, VPC를 EKS 가 배포된 VPC로 설정하자. 추가로 필자는 비용 문제로 프리티어로 설정했고 비밀번호를 12341234 로 설정했다.\n생성한 DB 인스턴스의 엔드포인트를 환경 변수로 입력하여 catalog를 재배포 하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 export CATALOG_RDS_PASSWORD=12341234 export CATALOG_RDS_ENDPOINT=catalog-db2.cnrosybtsnww.ap-northeast-2.rds.amazonaws.com:3306 kubectl apply -k /workspace/modules/networking/securitygroups-for-pods/rds -- # 생성 이후 DB endpoint 확인 kubectl get -n catalog cm catalog -o yaml --- apiVersion: v1 data: DB_ENDPOINT: database-2.cnrosybtsnww.ap-northeast-2.rds.amazonaws.com:3306 DB_NAME: catalog DB_READ_ENDPOINT: database-2.cnrosybtsnww.ap-northeast-2.rds.amazonaws.com:3306 kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;DB_ENDPOINT\u0026#34;:\u0026#34;database-2.cnrosybtsnww.ap-northeast-2.rds.amazonaws.com:3306\u0026#34;,\u0026#34;DB_NAME\u0026#34;:\u0026#34;catalog\u0026#34;,\u0026#34;DB_READ_ENDPOINT\u0026#34;:\u0026#34;database-2.cnrosybtsnww.ap-northeast-2.rds.amazonaws.com:3306\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;catalog\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;catalog\u0026#34;}} creationTimestamp: \u0026#34;2023-05-10T19:48:42Z\u0026#34; name: catalog namespace: catalog resourceVersion: \u0026#34;6645\u0026#34; # 기존 catalog 파드 삭제 kubectl delete pod -n catalog -l app.kubernetes.io/component=service # catalog 로그 확인 kubectl -n catalog logs deployment/catalog --- 2023/05/10 20:13:25 Running database migration... 2023/05/10 20:13:30 Error: Failed to prep migration dial tcp 192.168.12.123:3306: i/o timeout 2023/05/10 20:13:30 Error: Failed to run migration dial tcp 192.168.12.123:3306: i/o timeout 2023/05/10 20:13:30 dial tcp 192.168.12.123:3306: i/o timeout 결과 처럼 i/o timeout 이 발생하는데 보안 그룹을 설정하지 않았기 때문이다.\n보안 정책 생성을 통해 앞서 생성한 보안 그룹(pods-connect-rds)을 catalog 파드에 적용시키자.\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: vpcresources.k8s.aws/v1beta1 kind: SecurityGroupPolicy metadata: name: catalog-rds-access namespace: catalog spec: podSelector: matchLabels: app.kubernetes.io/component: mysql securityGroups: groupIds: - sg-07739bd847069f56e # catalog-sg 보안 그룹 적용 후 파드를 재배포하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 export CATALOG_SG_ID=sg-08bb6f8b77a20e693 kubectl apply -k . --- WARNING: This Kustomization is relying on a bug that loads values from the environment when they are omitted from an env file. This behaviour will be removed in the next major release of Kustomize. WARNING: This Kustomization is relying on a bug that loads values from the environment when they are omitted from an env file. This behaviour will be removed in the next major release of Kustomize. WARNING: This Kustomization is relying on a bug that loads values from the environment when they are omitted from an env file. This behaviour will be removed in the next major release of Kustomize. namespace/catalog unchanged serviceaccount/catalog unchanged configmap/catalog unchanged configmap/catalog-env-7gtc4hbmd2 unchanged configmap/catalog-sg-env-5969b7c958 created secret/catalog-db unchanged service/catalog unchanged service/catalog-mysql unchanged service/ui-nlb unchanged deployment.apps/catalog unchanged statefulset.apps/catalog-mysql unchanged securitygrouppolicy.vpcresources.k8s.aws/catalog-rds-access created # 기존 catalog 파드 삭제 kubectl delete pod -n catalog -l app.kubernetes.io/component=service # 보안 그룹 변경 확인 kubectl get events -n catalog | grep SecurityGroupRequested -- 5m22s Normal SecurityGroupRequested pod/catalog-6d688b9f9c-gjhvt Pod will get the following Security Groups [sg-08bb6f8b77a20e693] 20s Normal SecurityGroupRequested pod/catalog-6d688b9f9c-vnlgb Pod will get the following Security Groups [sg-08bb6f8b77a20e693] 10s Normal SecurityGroupRequested pod/catalog-mysql-0 Pod will get the following Security Groups [sg-08bb6f8b77a20e693] 이후 파드에 접속하여 rds에 접속하면 정상적으로 접근이 된다.(밑 사진의 빨간네모) 반면에 노드에서 rds 접근 시도시 timeout이 발생한다. (밑 사진의 파란네모)\n워크샵에 있는 내용을 따라했지만, catalog 파드에서 연결이 안되어 catalog-mysal 에서 RDS 연결 테스트를 진행하였다. 실습 진행에 참고하자!\n마치며 이번 글에서는 못 다뤘지만, VPC 관련하여 다룰 주제가 많다! 틈틈히 정리하여 VPC Deep Dive 2탄에서 해당 내용들을 다룰 수 있도록 하겠다.\n","date":"May 12","permalink":"https://HanHoRang31.github.io/post/aews-eks-vpc-1/","tags":["KANS","eks","cloud","AWS","kubernetes","vpc"],"title":"[AEWS] EKS VPC CNI Deep Dive"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. 지난 블로그 글에 이어서 kubeflow 를 스터디한 내용을 공유하고자 한다. 오늘 주제는 kubeflow 의 인프라 요소로 왜 쿠버네티스에서 머신러닝을 쓰는 것이 좋은 가와 kubeflow 가 AWS 클라우드(EKS)에 올라갔을 때 어떤 이점이 있는 지 확인하겠다.\nML on kubernetes 먼저, 쿠버네티스는 머신러닝 워크플로우를 지원하는 데 매우 유용한 플랫폼으로 이유는 다음과 같이 확인할 수가 있다. (참고 : ChatGPT)\n확장성: 쿠버네티스는 애플리케이션을 클러스터의 다양한 노드에 자동으로 분산시키는 능력을 가지고 있다. 이는 머신러닝 모델을 학습하거나 예측을 생성할 때 필요한 컴퓨팅 자원을 자동으로 확장하고 축소할 수 있다는 것을 의미한다. 포터빌리티와 다중 클라우드 지원: 쿠버네티스는 여러 클라우드 제공 업체에 걸쳐 동일한 방식으로 애플리케이션을 배포하고 관리하는 데 도움이 된다. 이는 머신러닝 워크플로우를 어디서든 쉽게 이동하고 배포할 수 있음을 의미한다. 자동화와 오케스트레이션: 쿠버네티스는 컨테이너화 된 애플리케이션의 배포, 확장 및 관리를 자동화한다. 이는 머신러닝 파이프라인의 복잡한 워크플로우를 쉽게 관리하고 오케스트레이션할 수 있음을 의미한다. 자원 관리: 쿠버네티스는 CPU, 메모리 등의 자원을 효과적으로 관리하여 각 애플리케이션에 필요한 자원을 제공한다. 이는 머신러닝 워크플로우에서 중요한 역할이다. 커뮤니티와 에코시스템: 쿠버네티스는 강력한 커뮤니티와 에코시스템을 가지고 있다. 여기에는 머신러닝에 특화된 도구와 프레임워크를 쿠버네티스에 쉽게 통합할 수 있는 Kubeflow와 같은 프로젝트가 포함된다. 여기서 가장 중요하게 볼 점은 4번인 것 같다. 머신러닝의 모델 학습 과정에서 분산 처리로 학습을 진행하면 속도가 선형적으로 빨라지기 때문이다.\nhttps://www.youtube.com/watch?v=qctwfYZKK8M\u0026amp;t=528s\n쿠버네티스가 머신러닝 플랫폼에 유용한 플랫폼인 것을 확인했지만, 어떻게 쿠버네티스에서 머신러닝 플랫폼을 머신러닝 과학자나 분석가 분들에게 제공할 것에 대한 의문이 남는다. 여기에 대한 해결점이 지난 시간에 구축한 kubeflow 인데 ML 플랫폼을 한 데 모아 묶어 설치하고 인터넷 링크를 통해 쉽게 제공할 수 있기 때문이다.\nhttps://www.youtube.com/watch?v=6GYuRy84M1o\u0026amp;t=67s\nKubeflow on AWS kubeflow on AWS 라는 말은 kubeflow를 EKS에서 배포했을 때를 얘기한다. kubeflow on AWS 로 올리면 AWS 서비스와의 통합을 통해 운영 오버헤드를 줄이면서 안전성, 보안, 이식성, 확장성이 우수한 ML 시스템을 구축할 수 있다.\nAWS 서비스 통합을 구체적으로 예를 들자면 다음과 같다.\n사용하기 쉬운 파이프라인 아티팩트 스토어를 위한 Amazon Simple Storage Service(Amazon S3) 높은 확장성의 파이프라인 및 메타데이터 스토어를 위한 Amazon Relational Database Service(Amazon RDS) 훈련 성능 향상 목적의 간단하고 확장 가능한 서버리스 파일 스토리지 솔루션을 위한 Amazon Elastic File System/Amazon FSx for Lustre 애플리케이션 액세스에 필요한 보안 암호 보호를 위한 AWS Secrets Manager 영구 로그 관리를 위한 AWS CloudWatch 고도로 최적화된 Jupyter 노트북 서버 이미지를 위한 AWS Deep Learning Containers HTTPS를 경유하는 안전한 외부 트래픽 관리를 위한 AWS Application Load Balancer TLS를 통한 사용자 인증을 위한 AWS Cognito 자세한 서비스 통합 방법 관련하여 Kubeflow 공식문서를 참고하도록 하자.\n이번 장에서는 지난 시간에 구축한 kubeflow에 AWS 로드밸런서 서비스인 AWS Application Load Balancer 와 파일 스토리지인 EFS를 추가로 구축하는 방법을 공유하겠다.\nAWS ALB 및 External DNS 연결 kubeflow에 AWS ALB을 연결하여 클러스터 외부 인터넷 망에서 HTTPS로 접근하도록 설정하겠다. 이에 대한 사전 작업으로 개인 도메인과 인증서가 필요하다. 필자의 경우 AWS Route53에서 도메인을 구매하였고, AWS ACM을 통해 인증서를 발급받았다. 사전 작업에 대한 내용은 kubeflow on AWS 공식문서를 참고하자.\n이어서 EKS 클러스터에 ALB controller 설치가 필요하다. 다음의 스크립트를 통해 진행하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # ALB controller 정책 설치 curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.7/docs/install/iam_policy.json aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json # 정책 arn 생성 확인 ACCOUNT_ID={AWS 계정 넘버} aws iam get-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --query \u0026#39;Policy.Arn\u0026#39; --- \u0026#34;arn:aws:iam::955963799952:policy/AWSLoadBalancerControllerIAMPolicy\u0026#34; # OIDC 서비스 어카운트 생성 CLUSTER_NAME={EKS 클러스터 이름} eksctl create iamserviceaccount --cluster=$CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller \\ --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve # OIDC 서비스 어카운트 확인 kubectl edit sa/aws-load-balancer-controller -n kube-system --- apiVersion: v1 kind: ServiceAccount metadata: annotations: # annotation 확인 eks.amazonaws.com/role-arn: arn:aws:iam::955963799952:role/eksctl-my-eks-kubeflow-addon-iamserviceaccou-Role1-18M1QX0LI36SU creationTimestamp: \u0026#34;2023-05-12T13:22:02Z\u0026#34; labels: app.kubernetes.io/managed-by: eksctl name: aws-load-balancer-controller namespace: kube-system resourceVersion: \u0026#34;42768\u0026#34; uid: 00393ee1-e469-4bd3-bd4e-6a10303a3f76 ~ 서비스 어카운트 연동이 확인이 되었으면 ALB 을 설치하겠다. 설치는 Helm을 통해 진행했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 클러스터 이름 설정 CLUSTER_NAME=my-eks-kubeflow printf \u0026#39;clusterName=\u0026#39;$CLUSTER_NAME\u0026#39;\u0026#39; \u0026gt; awsconfigs/common/aws-alb-ingress-controller/base/params.env # Install Load Balancer Controoler kustomize build awsconfigs/common/aws-alb-ingress-controller/base | kubectl apply -f - kubectl wait --for condition=established crd/ingressclassparams.elbv2.k8s.aws kustomize build awsconfigs/common/aws-alb-ingress-controller/base | kubectl apply -f - # ALB 파드 확인 kubectl get pods -A --- NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-load-balancer-controller-7857849d69-qc9nr 1/1 Running 0 24m # ALB 로그 확인 kubectl logs pods/aws-load-balancer-controller-7857849d69-qc9nr -n kube-system --- {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;version\u0026#34;,\u0026#34;GitVersion\u0026#34;:\u0026#34;v2.5.1\u0026#34;,\u0026#34;GitCommit\u0026#34;:\u0026#34;06abaed66e17a411ba064f34e6018b889780ac66\u0026#34;,\u0026#34;BuildDate\u0026#34;:\u0026#34;2023-04-17T22:36:53+0000\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.metrics\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Metrics server is starting to listen\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;:8080\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;setup\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;adding health check for controller\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/mutate-v1-pod\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/mutate-v1-service\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/validate-elbv2-k8s-aws-v1beta1-ingressclassparams\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/mutate-elbv2-k8s-aws-v1beta1-targetgroupbinding\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/validate-elbv2-k8s-aws-v1beta1-targetgroupbinding\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Registering webhook\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/validate-networking-v1-ingress\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:45Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;setup\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;starting podInfo repo\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:47Z\u0026#34;,\u0026#34;logger\u0026#34;:\u0026#34;controller-runtime.webhook.webhooks\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Starting webhook server\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2023-05-08T05:59:47Z\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Starting server\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;health probe\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;[::]:61779\u0026#34;} ... 이어서 External DNS 를 설치하자. 다음의 스크립트를 기반으로 OIDC 정책 연동부터 진행하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # vi iam_external_policy.json 생성 cat \u0026lt;\u0026lt;EOT \u0026gt; iam_external_policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ChangeResourceRecordSets\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:route53:::hostedzone/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } EOT aws iam create-policy --policy-name \u0026#34;AllowExternalDNSUpdates\u0026#34; --policy-document file://iam_external_policy.json.json # 정책 arn 생성 확인 aws iam get-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AllowExternalDNSUpdates --query \u0026#39;Policy.Arn\u0026#39; --- \u0026#34;arn:aws:iam::955963799952:policy/AllowExternalDNSUpdates\u0026#34; # OIDC 서비스 어카운트 생성 eksctl create iamserviceaccount --cluster=$CLUSTER_NAME --namespace=kube-system --name=external-dns \\ --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AllowExternalDNSUpdates --override-existing-serviceaccounts --approve # OIDC 서비스 어카운트 확인 kubectl edit sa/external-dns -n kube-system --- apiVersion: v1 kind: ServiceAccount metadata: annotations: # 마찬가지로 연동 확인이 가능하다. eks.amazonaws.com/role-arn: arn:aws:iam::955963799952:role/eksctl-hanhorang-addon-iamserviceaccount-kub-Role1-499RATDKJVJU kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;ServiceAccount\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app.kubernetes.io/name\u0026#34;:\u0026#34;external-dns\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;external-dns\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;}} creationTimestamp: \u0026#34;2023-05-08T06:02:38Z\u0026#34; labels: app.kubernetes.io/managed-by: eksctl app.kubernetes.io/name: external-dns name: external-dns namespace: kube-system resourceVersion: \u0026#34;13937\u0026#34; uid: 67ce20cc-6545-486c-b525-450be6311d65 external DNS을 연동하기 위해서는 도메인이 필요하다. 앞서 Route53에서 생성한 도메인을 연동하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # MyDomain=\u0026lt;자신의 도메인\u0026gt; MyDomain=hanhorang.link # 자신의 Route 53 도메인 ID 조회 및 변수 지정 MyDnzHostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name \u0026#34;${MyDomain}.\u0026#34; --query \u0026#34;HostedZones[0].Id\u0026#34; --output text) # ExternalDNS 배포 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/aews/externaldns.yaml cat externaldns.yaml | yh MyDomain=$MyDomain MyDnzHostedZoneId=$MyDnzHostedZoneId envsubst \u0026lt; externaldns.yaml | kubectl apply - # 로그 확인 kubectl get pods -A --- kube-system external-dns-6b5bbbf9d-l7ms2 1/1 Running 0 4 kubectl logs pods/external-dns-6b5bbbf9d-l7ms2 -n kube-system --- time=\u0026#34;2023-05-08T06:02:43Z\u0026#34; level=info msg=\u0026#34;config: {APIServerURL: KubeConfig: RequestTimeout:30s DefaultTargets:[] ContourLoadBalancerService:heptio-contour/contour GlooNamespace:gloo-system SkipperRouteGroupVersion:zalando.org/v1 Sources:[service ingress] Namespace: AnnotationFilter: LabelFilter: FQDNTemplate: CombineFQDNAndAnnotation:false IgnoreHostnameAnnotation:false IgnoreIngressTLSSpec:false IgnoreIngressRulesSpec:false GatewayNamespace: GatewayLabelFilter: Compatibility: PublishInternal:false PublishHostIP:false AlwaysPublishNotReadyAddresses:false ConnectorSourceServer:localhost:8080 Provider:aws GoogleProject: GoogleBatchChangeSize:1000 GoogleBatchChangeInterval:1s GoogleZoneVisibility: DomainFilter:[hanhorang.link] ExcludeDomains:[] RegexDomainFilter: RegexDomainExclusion: ZoneNameFilter:[] ZoneIDFilter:[] TargetNetFilter:[] ExcludeTargetNets:[] AlibabaCloudConfigFile:/etc/kubernetes/alibaba-cloud.json AlibabaCloudZoneType: AWSZoneType:public AWSZoneTagFilter:[] AWSAssumeRole: AWSAssumeRoleExternalID: AWSBatchChangeSize:1000 AWSBatchChangeInterval:1s AWSEvaluateTargetHealth:true AWSAPIRetries:3 AWSPreferCNAME:false AWSZoneCacheDuration:0s AWSSDServiceCleanup:false AzureConfigFile:/etc/kubernetes/azure.json AzureResourceGroup: AzureSubscriptionID: AzureUserAssignedIdentityClientID: BluecatDNSConfiguration: BluecatConfigFile:/etc/kubernetes/bluecat.json BluecatDNSView: BluecatGatewayHost: BluecatRootZone: BluecatDNSServerName: BluecatDNSDeployType:no-deploy BluecatSkipTLSVerify:false CloudflareProxied:false CloudflareDNSRecordsPerPage:100 CoreDNSPrefix:/skydns/ RcodezeroTXTEncrypt:false AkamaiServiceConsumerDomain: AkamaiClientToken: AkamaiClientSecret: AkamaiAccessToken: AkamaiEdgercPath: AkamaiEdgercSection: InfobloxGridHost: InfobloxWapiPort:443 InfobloxWapiUsername:admin InfobloxWapiPassword: InfobloxWapiVersion:2.3.1 InfobloxSSLVerify:true InfobloxView: InfobloxMaxResults:0 InfobloxFQDNRegEx: InfobloxNameRegEx: InfobloxCreatePTR:false InfobloxCacheDuration:0 DynCustomerName: DynUsername: DynPassword: DynMinTTLSeconds:0 OCIConfigFile:/etc/kubernetes/oci.yaml InMemoryZones:[] OVHEndpoint:ovh-eu OVHApiRateLimit:20 PDNSServer:http://localhost:8081 PDNSAPIKey: PDNSTLSEnabled:false TLSCA: TLSClientCert: TLSClientCertKey: Policy:sync Registry:txt TXTOwnerID:/hostedzone/Z08463751O7YNWD79KKIX TXTPrefix: TXTSuffix: Interval:1m0s MinEventSyncInterval:5s Once:false DryRun:false UpdateEvents:false LogFormat:text MetricsAddress::7979 LogLevel:info TXTCacheInterval:0s TXTWildcardReplacement: ExoscaleEndpoint:https://api.exoscale.ch/dns ExoscaleAPIKey: ExoscaleAPISecret: CRDSourceAPIVersion:externaldns.k8s.io/v1alpha1 CRDSourceKind:DNSEndpoint ServiceTypeFilter:[] CFAPIEndpoint: CFUsername: CFPassword: RFC2136Host: RFC2136Port:0 RFC2136Zone: RFC2136Insecure:false RFC2136GSSTSIG:false RFC2136KerberosRealm: RFC2136KerberosUsername: RFC2136KerberosPassword: RFC2136TSIGKeyName: RFC2136TSIGSecret: RFC2136TSIGSecretAlg: RFC2136TAXFR:false RFC2136MinTTL:0s RFC2136BatchChangeSize:50 NS1Endpoint: NS1IgnoreSSL:false NS1MinTTLSeconds:0 TransIPAccountName: TransIPPrivateKeyFile: DigitalOceanAPIPageSize:50 ManagedDNSRecordTypes:[A CNAME] GoDaddyAPIKey: GoDaddySecretKey: GoDaddyTTL:0 GoDaddyOTE:false OCPRouterName: IBMCloudProxied:false IBMCloudConfigFile:/etc/kubernetes/ibmcloud.json TencentCloudConfigFile:/etc/kubernetes/tencent-cloud.json TencentCloudZoneType: PiholeServer: PiholePassword: PiholeTLSInsecureSkipVerify:false PluralCluster: PluralProvider:}\u0026#34; time=\u0026#34;2023-05-08T06:02:43Z\u0026#34; level=info msg=\u0026#34;Instantiating new Kubernetes client\u0026#34; time=\u0026#34;2023-05-08T06:02:43Z\u0026#34; level=info msg=\u0026#34;Using inCluster-config based on serviceaccount-token\u0026#34; time=\u0026#34;2023-05-08T06:02:43Z\u0026#34; level=info msg=\u0026#34;Created Kubernetes client https://10.100.0.1:443\u0026#34; time=\u0026#34;2023-05-08T06:02:45Z\u0026#34; level=info msg=\u0026#34;Applying provider record filter for domains: [hanhorang.link. .hanhorang.link.]\u0026#34; time=\u0026#34;2023-05-08T06:02:45Z\u0026#34; level=info msg=\u0026#34;All records are already up to date\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 export certArn= ACM 인증서 arn # 경로 kubeflow-manifests printf \u0026#39;certArn=\u0026#39;$certArn\u0026#39;\u0026#39; \u0026gt; awsconfigs/common/istio-ingress/overlays/https/params.env # istio certArn 설정 업데이트 kustomize build awsconfigs/common/istio-ingress/overlays/https | kubectl apply -f - # 정상 등록 확인 kubectl get ingress -n istio-system istio-ingress --- NAME CLASS HOSTS ADDRESS PORTS AGE istio-ingress \u0026lt;none\u0026gt; * k8s-istiosys-istioing-663e16c023-2114118481.ap-northeast-2.elb.amazonaws.com 80 43s 이어서 도메인 등록이 필요하다. 다음의 경로 tests/e2e/utils/load_balancer/config.yaml 에서 파일을 수정하자.\n1 2 3 4 5 6 7 8 9 10 11 12 cluster: name: my-eks-kubeflow region: ap-northeast-2 kubeflow: alb: scheme: internet-facing route53: rootDomain: hostedZoneId: Z08463751O7YNWD79KKIX name: hanhorang.link subDomain: name: platform.hanhorang.link 적용 명령어는 다음과 같다.\n1 2 3 4 5 6 7 8 # 경로 kubeflow-manifests # 설치 스크립트 패키지 설치 cd tests/e2e pip3 install -r requirements.txt # 경로 확인! # 반드시 kubeflow-manifests/tests/e2e 에서 진행하자. (파이썬 e2e 모듈 인식) PYTHONPATH=.. python3 utils/load_balancer/setup_load_balancer.py 업데이트가 진행되었으면tests/e2e/utils/load_balancer/config.yaml 파일을 다시 확인하자. 등록한 도메인과 certARN을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cluster: name: my-eks-kubeflow region: ap-northeast-2 kubeflow: alb: dns: k8s-istiosys-istioing-663e16c023-2114118481.ap-northeast-2.elb.amazonaws.com scheme: internet-facing serviceAccount: name: aws-load-balancer-controller namespace: kube-system policyArn: arn:aws:iam::955963799952:policy/alb_ingress_controller_my-eks-kubeflow3tdc061n5e route53: rootDomain: certARN: arn:aws:acm:ap-northeast-2:955963799952:certificate/274b725c-c1ad-4528-a593-e0030aaa62f9 hostedZoneId: Z08463751O7YNWD79KKIX name: hanhorang.link subDomain: certARN: arn:aws:acm:ap-northeast-2:955963799952:certificate/09d02797-709e-4c9b-9b95-8eef6fee7e3f hostedZoneId: Z08293761OELZZQTGI41U name: platform.hanhorang.link 약 5분~10분이후 다음의 서브도메인 (kubeflow.platform.hanhorang.link) 에 접속하면 정상적으로 연결된 것을 확인할 수 있다.\nEFS 연결 EFS(Amazon Elastic File System)는 AWS에서 제공하는 파일 스토리지이다. 파일 스토리지인 만큼 여러 노드에서 접근이 가능하여 머신러닝같은 워크 플로우에 자주 추천하는 서비스이다. kubeflow에서도 EFS를 활용할 수 있는데 활용하여 얻는 이점은 다음과 같다. (참고 ChatGPT)\n분산 학습: EFS를 사용하면 여러 노드가 동일한 파일 시스템에 접근할 수 있다. 모든 노드가 동일한 데이터에 접근하고, 중간 학습 결과를 공유하면서 동시에 작업을 수행할 수 있어 학습 속도가 빨라진다.. 데이터 공유와 재사용: EFS는 클러스터의 모든 노드에서 동시에 접근할 수 있는 중앙화된 저장 공간을 제공한다. 이는 데이터를 쉽게 공유하고 재사용할 수 있게 하므로, 데이터 관리를 단순화하고 머신러닝 워크플로우를 효율적으로 만든다. 확장성: EFS는 자동으로 확장되고 축소되므로, 데이터 저장량에 대해 걱정할 필요가 없다. 또한, 데이터는 여러 가용 영역에 걸쳐 복제되므로, 내구성과 가용성도 보장된다. 지속적인 저장소: EFS는 지속적인 스토리지를 제공한다. 즉, 파드가 종료되거나 노드에 문제가 생겨도 데이터는 안전하게 보호된다. 이는 머신러닝 훈련에서 중요한데, 훈련 중인 모델의 체크포인트를 저장하고 필요할 때 언제든지 복구할 수 있기 때문이다. 아래는 EFS 연결시 각 워커 노드에서 작동하는 아키텍처이다. EFS 특성에 맞게 각 노드가 스토리지를 공유하여 사용하는 것을 알 수 있다.\nEFS 연결을 위해서는 EFS 프로비저닝이 필요하다. EFS 프로비저닝 방법은 정적, 동적 두가지로 이번 글에서는 동적으로 생성하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # IAM 정책 생성 curl -s -O https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/docsiam-policy-example.json aws iam create-policy --policy-name AmazonEKS_EFS_CSI_Driver_Policy --policy-document file://iam-policy-example.json # ISRA 설정 : 고객관리형 정책 AmazonEKS_EFS_CSI_Driver_Policy 사용 eksctl create iamserviceaccount \\ --name efs-csi-controller-sa \\ --namespace kube-system \\ --cluster ${CLUSTER_NAME} \\ --attach-policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AmazonEKS_EFS_CSI_Driver_Policy \\ --approve # 적용 확인, annotation kubectl get sa -n kube-system efs-csi-controller-sa -o yaml | head -5 --- apiVersion: v1 kind: ServiceAccount metadata: annotations: # 적용 확인 eks.amazonaws.com/role-arn: arn:aws:iam::955963799952:role/eksctl-my-eks-kubeflow-addon-iamserviceaccou-Role1-1TAOJ41 # efs csi driver 설치 helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/ helm repo update helm upgrade -i aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \\ --namespace kube-system \\ --set image.repository=602401143452.dkr.ecr.ap-northeast-2.amazonaws.com/eks/aws-efs-csi-driver \\ --set controller.serviceAccount.create=false \\ --set controller.serviceAccount.name=efs-csi-controller-sa EFS 프로비저닝 전 EFS 스토리지 생성이 필요하다. 스토리지 생성 단계 전 EKS CIDR 보안 그룹 추가(NFS 트래픽 허용)가 필요하다. 아래 스크립트를 통해 보안 그룹을 생성하고 EFS 파일 시스템을 생성하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # 보안 그룹 생성을 위한 네트워크 변수 설정 vpc_id=$(aws eks describe-cluster \\ --name my-eks-kubeflow \\ --query \u0026#34;cluster.resourcesVpcConfig.vpcId\u0026#34; \\ --output text) cidr_range=$(aws ec2 describe-vpcs \\ --vpc-ids $vpc_id \\ --query \u0026#34;Vpcs[].CidrBlock\u0026#34; \\ --output text \\ --region ap-northeast-2) # 보안 그룹 생성 security_group_id=$(aws ec2 create-security-group \\ --group-name MyEfsSecurityGroup \\ --description \u0026#34;My EFS security group\u0026#34; \\ --vpc-id $vpc_id \\ --output text) # NFS 트래픽 허용 aws ec2 authorize-security-group-ingress \\ --group-id $security_group_id \\ --protocol tcp \\ --port 2049 \\ --cidr $cidr_range # EFS 파일 시스템 생성 file_system_id=$(aws efs create-file-system \\ --region ap-northeast-2 \\ --performance-mode generalPurpose \\ --query \u0026#39;FileSystemId\u0026#39; \\ --output text) # EFS 확인 echo $file_system_id -- fs-01eea8c4de75fcd80 # 스토리지 클래스 생성 EfsFsId=$(aws efs describe-file-systems --query \u0026#34;FileSystems[*].FileSystemId\u0026#34; --output text) curl -s -O https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/storageclass.yaml sed -i \u0026#34;s/fs-92107410/$EfsFsId/g\u0026#34; storageclass.yaml kubectl apply -f storageclass.yaml kubectl get sc efs-sc -- NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ebs-sc (default) ebs.csi.aws.com Delete WaitForFirstConsumer false 6h48m efs-sc efs.csi.aws.com Delete Immediate false 25s gp2 kubernetes.io/aws-ebs Delete WaitForFirstConsumer false 7h37m kubeflow 대시보드에 접속하여 EFS 스토리지를 생성해보자. 기본 아이디와 비밀번호는 user@example.com , 12341234 이다.\n로그인 이후 왼쪽 메뉴 [Volumes] → [New Volume] 클릭 후 Storage Class 확인시 efs-sc 를 확인할 수 있다.\n생성된 볼륨은 콘솔 및 터미널에서 확인이 가능하다.\n생성한 볼륨은 jupyter notebook 생성시 데이터 볼륨 설정해서 볼륨 지정이 가능하다\n이후 EFS 볼륨을 사용하는 기계 학습 훈련의 예제는 GitHub에 따라 진행하도록 하자.\n마치며 Kubeflow on AWS 관련하여 ALB 와 EFS를 연동하여 테스트를 해보았다. 다음 시간에는 kubeflow를 통해 파이프라인을 통한 모델 생성 및 이를 기반한 API deployment 등등 ML 쪽 예제를 다룰 예정이다. 또한 못 다룬 이야기지만 kubeflow 학습시 노드 확장성으로 (노드 셀렉터, 어피니티)를 통해 노드가 자동 확장된다는데 이 점에 대해서도 확인할 생각이다.\n","date":"May 11","permalink":"https://HanHoRang31.github.io/post/kubeflow-on-aws/","tags":["KANS","kubeflow","cloud","AWS","eksctl","eks"],"title":"[AEWS] Kubeflow on AWS"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. EKS 구축 및 관리 툴인 eksctl은 다양한 구성 옵션을 제공한다. 공식 문서에 정리가 잘 되어 있으며 이번 블로그 글에서 필자 기준의 흥미로운 옵션을 몇 가지 선택하여 테스트한 내용을 공유하고자 한다.\nEKS addon 확장을 위한 AWS IAM 정책 생성 EKS 노드로 Spot 인스턴스 사용하기 Spot 인스턴스를 기반으로한 kubeflow 인프라 구성하기 EKS addon 확장을 위한 AWS IAM 정책 생성 EKS addon는 Amazon EKS에서 제공하는 쿠버네티스 클러스터 구성 요소로, 클러스터의 관리, 네트워킹, 로드 밸런싱 등을 담당하는 확장 기능이다. 이러한 addon 을 사용하면 확장 기능의 버전 관리와 업데이트가 쉬워진다. 기본적으로 EKS 설치시 네트워크단의 addon이 설치된다. 설치되는 addon은 다음과 같다.\n애드온 이름 설명 CoreDNS 클러스터 내의 DNS 쿼리를 처리하는데 사용된다. Kube-proxy 쿠버네티스 서비스와 관련된 네트워크 요청을 처리한다. VPC CNI 쿠버네티스 클러스터 내의 파드 간 네트워킹을 관리하는 Amazon VPC CNI 플러그인이다. 로드밸런싱, 네트워크 등의 추가 addon은 eks 설치 이후 설치가 가능하다. 중요한 점은 addon 설치을 위해서는 필요 IAM 정책이 필요하다. EKS addon 의 확장 기능은 AWS 서비스를 사용하며 이를 위해 AWS 서비스 사용을 위한 IAM 정책이 필요하기 때문이다. eksctl는 addon 설치를 제공하지 않지만, IAM 정책 생성은 제공한다. eksctl를 통해서 노드에 IAM 정책이 부여되는데 부여할 수 있는 리스트는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 nodeGroups: - name: ng-1 instanceType: m5.xlarge desiredCapacity: 1 iam: # addon 정책 부여 withAddonPolicies: imageBuilder: true # 이미지 빌더: 사용자 정의 컨테이너 이미지를 빌드하고 관리하는 도구 autoScaler: true # 오토 스케일러: 클러스터 내에서 자동으로 노드 및 파드 크기를 조정 externalDNS: true # 외부 DNS: 쿠버네티스 서비스와 인그레스 리소스에 대한 외부 DNS 레코드를 관리 certManager: true # 인증서 관리자: 쿠버네티스 클러스터 내에서 TLS 인증서를 자동으로 발급 및 관리 appMesh: true # 앱 메시: 마이크로서비스 간 통신을 관리하고 모니터링하는 서비스 메시 appMeshPreview: true # 앱 메시 프리뷰: 앱 메시의 베타 기능을 미리 사용할 수 있게 해주는 프리뷰 버전 ebs: true # Amazon EBS CSI 드라이버: 쿠버네티스 클러스터에서 Amazon EBS 볼륨을 사용할 수 있게 함 fsx: true # Amazon FSx CSI 드라이버: 쿠버네티스 클러스터에서 Amazon FSx 파일 시스템을 사용할 수 있게 함 efs: true # Amazon EFS CSI 드라이버: 쿠버네티스 클러스터에서 Amazon EFS 파일 시스템을 사용할 수 있게 함 awsLoadBalancerController: true # AWS 로드 밸런서 컨트롤러: AWS 로드 밸런서를 쿠버네티스 서비스와 통합 xRay: true # AWS X-Ray: 분산 애플리케이션의 성능 문제를 분석하고 디버깅하는 서비스 cloudWatch: true # Amazon CloudWatch: AWS 리소스 및 애플리케이션의 모니터링 및 관측을 제공 eksctl 를 통해 설치된 정책은 AWS 콘솔에서 확인이 가능하다.\nEKS 노드로 Spot 인스턴스 사용하기 Spot 인스턴스는 AWS의 미사용 컴퓨팅 용량을 할인된 가격으로 제공하는 Amazon EC2 인스턴스 유형이다. Spot 인스턴스는 온디맨드 인스턴스보다 비용이 최대 90% 까지 저렴하게 사용할 수 있지만, 가용성이 떨어질 경우 AWS에 의해 중단될 수 있다. 이러한 이유로 가변 워크로드 처리나 시간에 민감하지 않는 워크로드(데이터 분석, 배치) 작업에 사용된다.\n복잡할 것 같지만, Spot 인스턴스의 비용 절감이 가지고 오는 장점이 어마무시하다. 비용 절감의 원리는 다음과 같다.\nhttps://www.youtube.com/watch?v=ugDrxMqSj-E\u0026amp;t=426s\n그림과 같이 사용자가 원하는 인스턴스의 가격을 정해두면 AWS 에서 사용하지 않는 인스턴스를 한정하여 인스턴스를 제공하는 식이다. 사용자 제시 가격에 따라 최대 90퍼까지 절감이 가능하나, 사용하지 않는 인스턴스가 가변적으로 변하기에 보통 95%로 중단된다고 한다.\nAWS 에서 spot 인스턴스를 사용할 수 있는 서비스들이 다양하다. 이번 글에서는 EKS에서 SPOT 인스턴스를 사용하는 경우를 다루겠다.\neksctl 에서 워크 노드에 대해 spot 인스턴스 설정이 가능하다. 다만, 워크 노드 옵션이 관리형 노드 그룹, 비관리형 노드 그룹에 따라 구성 옵션이 다른데 구성 파일을 확인하면 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # spot-ng.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: my-eks region: ap-northeast-2 nodeGroups: # 비관리형 노드 그룹 - name: spot-1 minSize: 0 maxSize: 2 instancesDistribution: maxPrice: 0.017 instanceTypes: [\u0026#34;t3.small\u0026#34;, \u0026#34;t3.medium\u0026#34;] # At least one instance type should be specified onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 50 spotInstancePools: 2 managedNodeGroups: # 관리형 노드 그룹 - name: spot-m1 instanceTypes: [\u0026#34;c3.large\u0026#34;,\u0026#34;c4.large\u0026#34;,\u0026#34;c5.large\u0026#34;,\u0026#34;c5d.large\u0026#34;,\u0026#34;c5n.large\u0026#34;,\u0026#34;c5a.large\u0026#34;] spot: true desiredCapacity: 1 # 인스턴스를 설정하지 않으면 m5.large로 설정된다. - name: spot-m2 spot: true desiredCapacity: 1 관리형 노드 그룹은 spot: ture 을 통해 비관리형 노드 그룹은 instancesDistribution 을 통해 가능하다. 옵션 설정 부분이 많이 차이나는데 관리형 노드 그룹은 AWS가 알아서 설정해주는 반면 비관리형 노드 그룹은 사용자가 자세하게 비용 및 정책을 설정해야 하기 때문이다. 비관리형 노드 그룹을 통해 구성하는 경우가 많으며 spot인스턴스 사용 예에 대해 이해가 필요하다. 아래는 공식 문서에서 제공하는 예를 이해하기 위해 작성하였다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # 50% 스팟 인스턴스와 50% 온디맨드 인스턴스를 사용하는 노드 그룹 nodeGroups: - name: ng-1 minSize: 2 maxSize: 5 instancesDistribution: maxPrice: 0.017 instanceTypes: [\u0026#34;t3.small\u0026#34;, \u0026#34;t3.medium\u0026#34;] # At least one instance type should be specified onDemandBaseCapacity: 0 # 항상 사용 가능한 최소 온디맨드 인스턴스 수 onDemandPercentageAboveBaseCapacity: 50 # 초과하는 인스턴스에 대해 온디맨드 인스턴스를 사용할 비율을 설정(백분율) spotInstancePools: 2 # 인스턴스 유형과 가용 영역 풀 설정 제한 # GPU 인스턴스도 Spot 인스턴스로 사용가능하다. nodeGroups: - name: ng-gpu instanceType: mixed desiredCapacity: 1 instancesDistribution: instanceTypes: - p2.xlarge - p2.8xlarge - p2.16xlarge maxPrice: 0.50 # capacity-optimized 용량 최적화 전략으로 할당 nodeGroups: - name: ng-capacity-optimized minSize: 2 maxSize: 5 instancesDistribution: maxPrice: 0.017 instanceTypes: [\u0026#34;t3.small\u0026#34;, \u0026#34;t3.medium\u0026#34;] # At least one instance type should be specified onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 50 spotAllocationStrategy: \u0026#34;capacity-optimized\u0026#34; # capacity-optimized 용량 최적화 전략으로 할당(우선순위로 인스턴스 타입에서 첫 번째 인스턴스가 우선순위로 선택된다.) nodeGroups: - name: ng-capacity-optimized-prioritized minSize: 2 maxSize: 5 instancesDistribution: maxPrice: 0.017 instanceTypes: [\u0026#34;t3a.small\u0026#34;, \u0026#34;t3.small\u0026#34;] # At least two instance types should be specified onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 0 spotAllocationStrategy: \u0026#34;capacity-optimized-prioritized\u0026#34; 노드 그룹을 배포하면 AWS 콘솔에서 다음과 같이 확인이 가능하다.\n1 eksctl create ng --cluster my-eks -f spot-ng.yaml 배포한지 30분이 지났지만 하나의 인스턴스가 실행되었다가 중단되었고 또 다른 인스턴스가 실행되는 것을 확인할 수 있다.\n[절감액 요약] 에서 필자가 설정한 인스턴스로 얼마가 절감되었는지 확인할 수 있다.\n오오 72퍼센트나..! 유용하게 사용하자!\nSpot 인스턴스를 기반으로한 kubeflow 인프라 구성하기 Spot 인스턴스가 가져오는 비용 절감을 통해 쿠버네티스 머신러닝 플랫폼인 kubeflow 인프라를 구성하겠다. 굳이 머신러닝 플랫폼을 정한 이유는 GPU 인스턴스 사용 비용을 최대한으로 절감하고 자원 사용을 최적화시켜줄 수 있어서 선택하였다. eksctl 공식 예에서도 kubeflow에 대한 인프라 구성을 예로 제공하고 있다. 해당 예를 가지고 리전 및 인스턴스를 변경하여 kubeflow 인프라를 구성해보겠다. 아키텍처는 다음과 같다.\nGPU 인스턴스(px로 시작)만 Spot 인스턴스로 할당하였다. 최소 0개부터 시작하여 필요할 때만 사용할 수 있도록 하여 비용을 절감할 수 있도록 설정하였다. GPU 인스턴스의 비용을 확인하면 상당히 비싼것을 확인할 수 있는데 서울 리전 기준 p2.xlarge1.465 USD, p3.2xlarge4.234 USD 이다. 약 절반 기준의 비용을 산정해서 Spot 인스턴스의 비용을 설정하였다. 가용 영역을 ap-northeast-2a 에만 설정한 이유는 네트워크 지연 최소화 때문이다. 머신러닝에서 네트워크 지연을 최소화하기위함이며 머신러닝 워크로드 특성상 고가용성을 고려하지 않았다. 아키텍처로 베스천 서버와 EKS 클러스터를 구축할 것이다. 베스천 서버는 cloudformation 을 통한 EC2 서버로 생성하고, EKS 클러스터는 eksctl 구축하겠다. 베스천 서버의 cloudformation 코드는 필자의 깃허브 repo 를 참고하여 배포하자. 중요한 점은 ami를 ubuntu 지정하였는데 kubeflow 설치를 위해서는 설치 환경이 ubuntu이여만 한다.\nhttps://awslabs.github.io/kubeflow-manifests/docs/deployment/prerequisites/\n이를 위해 베스천 서버를 ubuntu로 구성하였다.\n다음은 eksctl 를 통해 EKS 클러스터를 구축하겠다. 구성 yaml 파일은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 # Cost-Optimized EKS cluster for Kubeflow with spot GPU instances and node scale down to zero # Built in efforts to reducing training costs of ML workloads. # Supporting tutorial can be found at the following link: # https://blog.gofynd.com/how-we-reduced-our-ml-training-costs-by-78-a33805cb00cf # This spec creates a cluster on EKS with the following active nodes # - 2x m5a.2xlarge - Accomodates all pods of Kubeflow # It also creates the following nodegroups with 0 nodes running unless a pod comes along and requests for the node to get spun up # - m5a.2xlarge -- Max Allowed 10 worker nodes # - p2.xlarge -- Max Allowed 10 worker nodes # - p3.2xlarge -- Max Allowed 10 worker nodes # - p3.8xlarge -- Max Allowed 04 worker nodes # - p3dn.24xlarge -- Max Allowed 01 worker nodes apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: # Name of your cluster, change to whatever you find fit. # If changed, make sure to change all nodegroup tags from # \u0026#39;k8s.io/cluster-autoscaler/my-eks-kubeflow: \u0026#34;owned\u0026#34;\u0026#39; --\u0026gt; \u0026#39;k8s.io/cluster-autoscaler/your-new-name: \u0026#34;owned\u0026#34;\u0026#39; name: my-eks-kubeflow # choose your region wisely, this will significantly impact the cost incurred region: ap-northeast-2 # 1.14 Kubernetes version since Kubeflow 1.0 officially supports the same version: \u0026#39;1.25\u0026#39; tags: # Add more cloud tags if needed for billing environment: staging # Add all possible AZs to ensure nodes can be spun up in any AZ later on. # THIS CAN\u0026#39;T BE CHANGED LATER. YOU WILL HAVE TO CREATE A NEW CLUSTER TO ADD NEW AZ SUPPORT. # This list applies to the whole cluster and isn\u0026#39;t specific to nodegroups vpc: id: vpc-04686564a10b92c9c cidr: 192.168.0.0/16 securityGroup: sg-0ea8529af823353e9 nat: gateway: HighlyAvailable subnets: public: public-2a: id: subnet-03eeb6d32aa5397bf cidr: 192.168.1.0/24 public-2c: id: subnet-023bc1a3fce0cde07 cidr: 192.168.2.0/24 private: private-2a: id: subnet-02c160be5273d5171 cidr: 192.168.3.0/24 private-2c: id: subnet-018a370a44f973ac4 cidr: 192.168.4.0/24 iam: withOIDC: true nodeGroups: - name: ng-1 desiredCapacity: 4 minSize: 0 maxSize: 10 # Set one nodegroup with 100GB volumes for Kubeflow to get deployed. # Kubeflow requirement states 1-2 Nodes with 100GB volume attached to the node. volumeSize: 100 volumeType: gp2 instanceType: c5n.xlarge privateNetworking: true ssh: publicKeyName: eks-terraform-key availabilityZones: - ap-northeast-2a labels: node-class: \u0026#34;worker-node\u0026#34; tags: # EC2 tags required for cluster-autoscaler auto-discovery k8s.io/cluster-autoscaler/node-template/label/lifecycle: OnDemand k8s.io/cluster-autoscaler/node-template/label/aws.amazon.com/spot: \u0026#34;false\u0026#34; k8s.io/cluster-autoscaler/node-template/label/gpu-count: \u0026#34;0\u0026#34; k8s.io/cluster-autoscaler/enabled: \u0026#34;true\u0026#34; k8s.io/cluster-autoscaler/my-eks-kubeflow: \u0026#34;owned\u0026#34; iam: withAddonPolicies: awsLoadBalancerController: true autoScaler: true cloudWatch: true efs: true ebs: true externalDNS: true - name: 1-gpu-spot-p2-xlarge minSize: 0 maxSize: 10 instancesDistribution: # set your own max price. AWS spot instance prices no longer cross OnDemand price. # Comment out the field to default to OnDemand as max price. maxPrice: 0.7 instanceTypes: [\u0026#34;p2.xlarge\u0026#34;] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 0 spotAllocationStrategy: capacity-optimized labels: lifecycle: Ec2Spot aws.amazon.com/spot: \u0026#34;true\u0026#34; gpu-count: \u0026#34;1\u0026#34; # Stick to one AZ for all GPU nodes. # In case of termination, this will prevent volumes from being unavailable # if the new instance got spun up in another AZ. privateNetworking: true ssh: publicKeyName: eks-terraform-key availabilityZones: - ap-northeast-2a taints: - key: spotInstance value: \u0026#34;true\u0026#34; effect: PreferNoSchedule tags: k8s.io/cluster-autoscaler/node-template/label/lifecycle: Ec2Spot k8s.io/cluster-autoscaler/node-template/label/aws.amazon.com/spot: \u0026#34;true\u0026#34; k8s.io/cluster-autoscaler/node-template/label/gpu-count: \u0026#34;1\u0026#34; k8s.io/cluster-autoscaler/node-template/taint/spotInstance: \u0026#34;true:PreferNoSchedule\u0026#34; k8s.io/cluster-autoscaler/enabled: \u0026#34;true\u0026#34; k8s.io/cluster-autoscaler/my-eks-kubeflow: \u0026#34;owned\u0026#34; iam: withAddonPolicies: autoScaler: true cloudWatch: true awsLoadBalancerController: true efs: true ebs: true externalDNS: true - name: 1-gpu-spot-p3-2xlarge minSize: 0 maxSize: 10 instancesDistribution: # set your own max price. AWS spot instance prices no longer cross OnDemand price. # Comment out the field to default to OnDemand as max price. maxPrice: 2.0 instanceTypes: [\u0026#34;p3.2xlarge\u0026#34;] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 0 spotAllocationStrategy: capacity-optimized labels: lifecycle: Ec2Spot aws.amazon.com/spot: \u0026#34;true\u0026#34; gpu-count: \u0026#34;1\u0026#34; # Stick to one AZ for all GPU nodes. # In case of termination, this will prevent volumes from being unavailable # if the new instance got spun up in another AZ. privateNetworking: true ssh: publicKeyName: eks-terraform-key availabilityZones: - ap-northeast-2a taints: - key: spotInstance value: \u0026#34;true\u0026#34; effect: PreferNoSchedule tags: k8s.io/cluster-autoscaler/node-template/label/lifecycle: Ec2Spot k8s.io/cluster-autoscaler/node-template/label/aws.amazon.com/spot: \u0026#34;true\u0026#34; k8s.io/cluster-autoscaler/node-template/label/gpu-count: \u0026#34;1\u0026#34; k8s.io/cluster-autoscaler/node-template/taint/spotInstance: \u0026#34;true:PreferNoSchedule\u0026#34; k8s.io/cluster-autoscaler/enabled: \u0026#34;true\u0026#34; k8s.io/cluster-autoscaler/my-eks-kubeflow: \u0026#34;owned\u0026#34; iam: withAddonPolicies: autoScaler: true cloudWatch: true awsLoadBalancerController: true efs: true ebs: true externalDNS: true - name: 4-gpu-spot-p3-8xlarge minSize: 0 maxSize: 4 instancesDistribution: # set your own max price. AWS spot instance prices no longer cross OnDemand price. # Comment out the field to default to OnDemand as max price. maxPrice: 4.4 instanceTypes: [\u0026#34;p3.8xlarge\u0026#34;] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 0 spotAllocationStrategy: capacity-optimized labels: lifecycle: Ec2Spot aws.amazon.com/spot: \u0026#34;true\u0026#34; gpu-count: \u0026#34;4\u0026#34; # Stick to one AZ for all GPU nodes. # In case of termination, this will prevent volumes from being unavailable # if the new instance got spun up in another AZ. privateNetworking: true ssh: publicKeyName: eks-terraform-key availabilityZones: - ap-northeast-2a taints: - key: spotInstance value: \u0026#34;true\u0026#34; effect: PreferNoSchedule tags: k8s.io/cluster-autoscaler/node-template/label/lifecycle: Ec2Spot k8s.io/cluster-autoscaler/node-template/label/aws.amazon.com/spot: \u0026#34;true\u0026#34; k8s.io/cluster-autoscaler/node-template/label/gpu-count: \u0026#34;4\u0026#34; k8s.io/cluster-autoscaler/node-template/taint/spotInstance: \u0026#34;true:PreferNoSchedule\u0026#34; k8s.io/cluster-autoscaler/enabled: \u0026#34;true\u0026#34; k8s.io/cluster-autoscaler/my-eks-kubeflow: \u0026#34;owned\u0026#34; iam: withAddonPolicies: autoScaler: true cloudWatch: true awsLoadBalancerController: true efs: true ebs: true externalDNS: true addons: - name: vpc-cni # no version is specified so it deploys the default version version: v1.12.6-eksbuild.1 attachPolicyARNs: - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy - name: kube-proxy version: latest # auto discovers the latest available - name: coredns version: latest # v1.9.3-eksbuild.2 withAddonPolicies 정책에서 efs: true 가 추가된 것을 확인할 수 있는데 머신러닝의 데이터 셋을 공유 스토리지로 활용하여 모델 훈련 및 추론에 대한 더 나은 성능을 얻을 수 있기 때문에 추가하였다. eksctl를 통해 EKS 클러스터를 구축하자.\n1 eksctl create cluster -f kubeflow-infra.yaml 약 20분 정도 소요된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 root@hanhorang:/home/ubuntu/blog-share/aews-eksctl/example# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-6xqcv 1/1 Running 0 94s kube-system aws-node-dtm6v 1/1 Running 0 94s kube-system aws-node-kn5fj 1/1 Running 0 94s kube-system aws-node-s7grj 1/1 Running 0 94s kube-system coredns-595d647554-f7576 1/1 Running 0 27s kube-system coredns-595d647554-jzvlg 1/1 Running 0 27s kube-system kube-proxy-r9csf 1/1 Running 0 94s kube-system kube-proxy-thglh 1/1 Running 0 94s kube-system kube-proxy-txzr4 1/1 Running 0 94s kube-system kube-proxy-zltl2 1/1 Running 0 94s kube-system nvidia-device-plugin-daemonset-4p24p 1/1 Running 0 73s kube-system nvidia-device-plugin-daemonset-67275 1/1 Running 0 59s kube-system nvidia-device-plugin-daemonset-kmh95 1/1 Running 0 61s kube-system nvidia-device-plugin-daemonset-kzhtv 1/1 Running 0 72s 해당 파드가 GPU 노드에만 배치될 수 있도록 데몬셋을 수정할 것이다. 다음의 명령어를 통해 수정하자.\n1 kubectl edit daemonset/nvidia-device-plugin-daemonset -n kube-system 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 spec: revisionHistoryLimit: 10 selector: matchLabels: name: nvidia-device-plugin-ds template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026#34;\u0026#34; creationTimestamp: null labels: name: nvidia-device-plugin-ds spec: nodeSelector: # 추가 gpu-count: \u0026#34;\u0026#34; #추가 수정 후 정상적으로 파드 에러가 사라진 것을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (terraform-eks@my-eks-kubeflow:N/A) [root@myeks-host example]# kubectl edit daemonset/nvidia-device-plugin-daemonset -n kube-system daemonset.apps/nvidia-device-plugin-daemonset edited (terraform-eks@my-eks-kubeflow:N/A) [root@hanhorang example]# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-9x5kp 1/1 Running 0 10m kube-system aws-node-bf4lw 1/1 Running 0 10m kube-system aws-node-v7gs6 1/1 Running 0 10m kube-system aws-node-wv9qj 1/1 Running 0 10m kube-system aws-node-xjwss 1/1 Running 0 10m kube-system coredns-595d647554-nlfv2 1/1 Running 0 10m kube-system coredns-595d647554-zx92r 1/1 Running 0 10m kube-system kube-proxy-24wxv 1/1 Running 0 10m kube-system kube-proxy-4dh4j 1/1 Running 0 10m kube-system kube-proxy-qp9xk 1/1 Running 0 10m kube-system kube-proxy-xcfqz 1/1 Running 0 10m kube-system kube-proxy-zvn2j 1/1 Running 0 10m kubeflow 배포 앞 과정에서 구성한 EKS 클러스터에 kubeflow를 배포하겠다. 배포하기 전 kubeflow가 무엇이고 아키텍처가 무엇인지 간단하게 확인하고 넘어가겠다.\n공식문서에 따르면 kubeflow는 오픈소스 기반의 ML 플랫폼이다. 플랫폼이라는 말이 중요한 데, 다른 머신러닝 서비스를 만드는 것이 아니라, 오픈소스 기반의 머신러닝 서비스을 합쳐 머신러닝 워크플로를 간소화 시켜주는 플랫폼 서비스로 제공한다는 의미이다.\nhttps://www.kubeflow.org/\n어떤 머신러닝 오픈소스를 사용하는 지는 아키텍처를 보면 확인할 수 있다. 클라우드 프로바이더나 로컬인 쿠버네티스 위에서 다양한 머신러닝 서비스 및 addon 서비스를 결합하여 워크플로를 구성한다고 이해하자.\nhttps://www.kubeflow.org/docs/started/architecture/\n머신러닝 컴포넌트가 많아 세부적으로는 확인할 수가 없고 큰 구성 별로 확인하겠다.\nML tools: 머신러닝 도구들은 데이터 전처리, 모델 학습, 평가, 최적화 및 배포와 같은 머신러닝 워크플로를 지원하는 소프트웨어 라이브러리 및 프레임워크이다. Kubeflow applications and scaffolding: Kubeflow 애플리케이션 및 스캐폴딩은 Kubeflow 플랫폼에서 제공하는 기본 뼈대와 도구들로, 사용자가 머신러닝 워크플로를 쉽게 구축하고 관리할 수 있도록 지원한다. 머신러닝 오픈소스 뿐만 아니라 istio, prometheus, argo 등의 오픈소스가 있는 것을 확인할 수 있는데 해당 서비스를 결합하여 대시보드, 서비스 메시, 파이프라인 구성에 사용된다. kubeflow 배포 전 작업으로 버전 확인 및 필요 addon 설치가 필요하다. 23년 5월 기준 EKS 버전 제공별 kubeflow 버전 지원은 다음의 그림을 통해 참고하자. 필자의 EKS 버전은 1.25로 kubeflow 1.7를 설치하겠다.\n다음 과정으로 필요 패키지 및 addon 설치를 진행하자. 패키지의 경우 공식 문서의 명령어를 통해 쉽게 설치가 가능하다.\n1 2 3 4 5 export KUBEFLOW_RELEASE_VERSION=v1.7.0 export AWS_RELEASE_VERSION=v1.7.0-aws-b1.0.0 git clone https://github.com/awslabs/kubeflow-manifests.git \u0026amp;\u0026amp; cd kubeflow-manifests git checkout ${AWS_RELEASE_VERSION} git clone --branch ${KUBEFLOW_RELEASE_VERSION} https://github.com/kubeflow/manifests.git upstream 1 2 # 패키지 설치 명령어 make install-tools 설치 중 파이썬 필요 패키지 설치 중 에러가 발생한다. 다음의 명령어를 통해 해결하자.\n1 2 3 pip install --ignore-installed PyYAML==5.3.1 pip3 install testresources python3.8 -m pip install -r tests/e2e/requirements.txt 패키지 설치 후, EKS addon인 EBS csi driver 설치가 필요하다.\nEBS csi driver은 AWS 공식 문서를 참고하여 설치를 진행하였다. EBS 볼륨 관리를 위한 IAM 정책 및 롤 생성과 드라이버 배포 과정으로 진행하였다.\n1 2 # OIDC 확인 aws eks describe-cluster --name my-eks-kubeflow --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; --output text 결과에서 region과 oidc 번호를 메모하자. 아래 IAM 정책 구성에 기입이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # vi aws-ebs-csi-driver-trust-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::955963799952:oidc-provider/oidc.eks.ap-northeast-2.amazonaws.com/id/D378D41514C8714C26A69DF6ECC0A999\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;oidc.eks.ap-northeast-2.amazonaws.com/id/D378D41514C8714C26A69DF6ECC0A999:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34;, \u0026#34;oidc.eks.ap-northeast-2.amazonaws.com/id/D378D41514C8714C26A69DF6ECC0A999:sub\u0026#34;: \u0026#34;system:serviceaccount:kube-system:ebs-csi-controller-sa\u0026#34; } } } ] } 1 2 3 4 5 6 7 8 9 # 롤 생성 aws iam create-role \\ --role-name AmazonEKS_EBS_CSI_DriverRole \\ --assume-role-policy-document file://\u0026#34;aws-ebs-csi-driver-trust-policy.json\u0026#34; # 정책 attach aws iam attach-role-policy \\ --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\ --role-name AmazonEKS_EBS_CSI_DriverRole 정책 attach까지 완료하였으면 해당 정책을 쿠버네티스 내에서 사용하기 위해 사용자 어카운트에 연동이 필요하다.\n1 2 3 4 5 6 # sa 생성 kubectl create sa ebs-csi-controller-sa -n kube-system # Role annotation kubectl annotate serviceaccount ebs-csi-controller-sa \\ -n kube-system \\ eks.amazonaws.com/role-arn=arn:aws:iam::955963799952:role/AmazonEKS_EBS_CSI_DriverRole 연동이 끝났으면 EBS 드라이버를 배포하자, 필자의 경우 eksctl를 통해 진행하였다.\n1 2 aws eks create-addon --cluster-name my-eks-kubeflow --addon-name aws-ebs-csi-driver \\ --service-account-role-arn arn:aws:iam::955963799952:role/AmazonEKS_EBS_CSI_DriverRole 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 root@hanhorang:/home/ubuntu/blog-share/aews-eksctl/example# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-6xqcv 1/1 Running 0 7m46s kube-system aws-node-dtm6v 1/1 Running 0 7m46s kube-system aws-node-kn5fj 1/1 Running 0 7m46s kube-system aws-node-s7grj 1/1 Running 0 7m46s kube-system coredns-595d647554-f7576 1/1 Running 0 6m39s kube-system coredns-595d647554-jzvlg 1/1 Running 0 6m39s kube-system ebs-csi-controller-b576f46c5-2c5sk 5/6 Running 0 16s kube-system ebs-csi-controller-b576f46c5-ffwnk 5/6 Running 0 16s kube-system ebs-csi-node-6tpm6 3/3 Running 0 16s kube-system ebs-csi-node-djrc4 3/3 Running 0 16s kube-system ebs-csi-node-qtngl 3/3 Running 0 16s kube-system ebs-csi-node-thbfc 3/3 Running 0 16s kube-system kube-proxy-r9csf 1/1 Running 0 7m46s kube-system kube-proxy-thglh 1/1 Running 0 7m46s kube-system kube-proxy-txzr4 1/1 Running 0 7m46s kube-system kube-proxy-zltl2 1/1 Running 0 7m46s kube-system nvidia-device-plugin-daemonset-4p24p 1/1 Running 0 7m25s kube-system nvidia-device-plugin-daemonset-67275 1/1 Running 0 7m11s kube-system nvidia-device-plugin-daemonset-kmh95 1/1 Running 0 7m13s kube-system nvidia-device-plugin-daemonset-kzhtv 1/1 Running 0 7m24 스토리지 addon 배포 이후 PVC의 Default 스토리지클래스 지정이 필요하다. 스토리지 클래스를 생성하고 기본 클래스 설정을 진행하자.\n1 2 3 4 5 6 7 8 9 # ebs-sc.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ebs-sc annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; provisioner: ebs.csi.aws.com volumeBindingMode: WaitForFirstConsumer 1 2 3 4 # 스토리지 클래스 배포 kubectl apply -f ebs-sc.yaml # 기본 클래스 수정 kubectl patch storageclass gp2 -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;false\u0026#34;}}}\u0026#39; 마지막으로 kubeflow 배포를 진행하겠다. 배포는 앞서 깃으로 클론한 레파지토리에 매니패스트 명령어를 통해 진행하겠다.\n1 2 3 4 5 export CLUSTER_NAME=my-eks-kubeflow export CLUSTER_REGION=ap-northeast-2 # 설치 명령어 make deploy-kubeflow INSTALLATION_OPTION=kustomize DEPLOYMENT_OPTION=vanilla 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ... All istio pods are running! ==========Installing dex========== Release \u0026#34;dex\u0026#34; does not exist. Installing it now. NAME: dex LAST DEPLOYED: Tue May 2 22:47:51 2023 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None Waiting for dex pods to be ready ... running command: kubectl wait --for=condition=ready pod -l \u0026#39;app in (dex)\u0026#39; --timeout=240s -n auth pod/dex-56d9748f89-99ggv condition met All dex pods are running! ==========Installing oidc-authservice========== Release \u0026#34;oidc-authservice\u0026#34; does not exist. Installing it now. NAME: oidc-authservice LAST DEPLOYED: Tue May 2 22:48:01 2023 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None Waiting for oidc-authservice pods to be ready ... running command: kubectl wait --for=condition=ready pod -l \u0026#39;app in (authservice)\u0026#39; --timeout=240s -n istio-system error: timed out waiting for the condition on pods/authservice-0 Waiting for oidc-authservice pods to be ready ... running command: kubectl wait --for=condition=ready pod -l \u0026#39;app in (authservice)\u0026#39; --timeout=240s -n istio-system error: timed out waiting for the condition on pods/authservice-0 Waiting for oidc-authservice pods to be ready ... running command: kubectl wait --for=condition=ready pod -l \u0026#39;app in (authservice)\u0026#39; --timeout=240s -n istio-system 스크립트를 통해 구성 요소들이 설치된다. 약 5분정도 소요된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 root@hanhorang:/home/ubuntu/blog-share/aews-eksctl/kubeflow-manifests# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE ack-system ack-sagemaker-controller-5667d978b-xhnmn 0/1 Error 4 (56s ago) 105s auth dex-56d9748f89-nk54k 1/1 Running 0 34m cert-manager cert-manager-74d949c895-25bt6 1/1 Running 0 35m cert-manager cert-manager-cainjector-d9bc5979d-m6kt8 1/1 Running 0 35m cert-manager cert-manager-webhook-84b7ddd796-m6dmp 1/1 Running 0 35m istio-system authservice-0 1/1 Running 0 34m istio-system cluster-local-gateway-6955b67f54-tlhp9 1/1 Running 0 8m16s istio-system istio-ingressgateway-67f7b5f88d-n6whr 1/1 Running 0 34m istio-system istiod-56f7cf9bd6-455ht 1/1 Running 0 34m knative-eventing eventing-controller-c6f5fd6cd-mzfzd 1/1 Running 0 7m45s knative-eventing eventing-webhook-79cd6767-pt9dt 1/1 Running 0 7m45s knative-serving activator-67849589d6-m7wlq 2/2 Running 0 8m6s knative-serving autoscaler-6dbcdd95c7-b5tdc 2/2 Running 0 8m6s knative-serving controller-b9b8855b8-ggzrg 2/2 Running 0 8m6s knative-serving domain-mapping-75cc6d667f-vc5hz 2/2 Running 0 8m5s knative-serving domainmapping-webhook-6dfb78c944-s4d5t 2/2 Running 0 8m5s knative-serving net-istio-controller-5fcd96d76f-pqvnt 2/2 Running 0 8m5s knative-serving net-istio-webhook-7ff9fdf999-48d9c 2/2 Running 0 8m5s knative-serving webhook-69cc5b9849-tbn9r 2/2 Running 0 8m5s kube-system aws-node-6xqcv 1/1 Running 0 127m kube-system aws-node-dtm6v 1/1 Running 0 127m kube-system aws-node-kn5fj 1/1 Running 0 127m kube-system aws-node-s7grj 1/1 Running 0 127m kube-system coredns-595d647554-f7576 1/1 Running 0 125m kube-system coredns-595d647554-jzvlg 1/1 Running 0 125m kube-system ebs-csi-controller-b576f46c5-76czd 6/6 Running 0 15m kube-system ebs-csi-controller-b576f46c5-svcbt 6/6 Running 0 15m kube-system ebs-csi-node-57dnr 3/3 Running 0 15m kube-system ebs-csi-node-dcn5z 3/3 Running 0 15m kube-system ebs-csi-node-hbsfg 3/3 Running 0 15m kube-system ebs-csi-node-qhhsm 3/3 Running 0 15m kube-system kube-proxy-r9csf 1/1 Running 0 127m kube-system kube-proxy-thglh 1/1 Running 0 127m kube-system kube-proxy-txzr4 1/1 Running 0 127m kube-system kube-proxy-zltl2 1/1 Running 0 127m kube-system nvidia-device-plugin-daemonset-4p24p 1/1 Running 0 126m kube-system nvidia-device-plugin-daemonset-67275 1/1 Running 0 126m kube-system nvidia-device-plugin-daemonset-kmh95 1/1 Running 0 126m kube-system nvidia-device-plugin-daemonset-kzhtv 1/1 Running 0 126m kubeflow-user-example-com ml-pipeline-ui-artifact-6cb7b9f6fd-jggk2 2/2 Running 0 110s kubeflow-user-example-com ml-pipeline-visualizationserver-7b5889796d-trjjd 2/2 Running 0 110s kubeflow admission-webhook-deployment-6db8bdbb45-7zfzq 1/1 Running 0 5m4s kubeflow cache-server-76cb8f97f9-wqstf 2/2 Running 0 6m25s kubeflow centraldashboard-655c7d894c-vmv5r 2/2 Running 0 6m40s kubeflow jupyter-web-app-deployment-76fbf48ff6-j7tkk 2/2 Running 0 4m55s kubeflow katib-controller-8bb4fdf4f-46zsh 1/1 Running 0 3m6s kubeflow katib-db-manager-f8dc7f465-4z2ch 1/1 Running 0 3m6s kubeflow katib-mysql-db6dc68c-xj6qr 1/1 Running 0 3m6s kubeflow katib-ui-7859bc4c67-khc44 2/2 Running 1 (2m59s ago) 3m6s kubeflow kserve-controller-manager-85b6b6c47d-qxxjp 2/2 Running 0 7m5s kubeflow kserve-models-web-app-99849d9f7-d67hg 2/2 Running 0 6m51s kubeflow kubeflow-pipelines-profile-controller-59ccbd47b9-9k9s4 1/1 Running 0 6m25s kubeflow metacontroller-0 1/1 Running 0 6m23s kubeflow metadata-envoy-deployment-5b6c575b98-rphl6 1/1 Running 0 6m24s kubeflow metadata-grpc-deployment-784b8b5fb4-mmfql 2/2 Running 2 (5m47s ago) 6m24s kubeflow metadata-writer-5899c74595-55kls 2/2 Running 0 6m24s kubeflow minio-65dff76b66-7g4q8 2/2 Running 0 6m24s kubeflow ml-pipeline-cff8bdfff-glgnb 2/2 Running 0 6m24s kubeflow ml-pipeline-persistenceagent-798dbf666f-kwjhf 2/2 Running 0 6m24s kubeflow ml-pipeline-scheduledworkflow-859ff9cf7b-fkskm 2/2 Running 0 6m24s kubeflow ml-pipeline-ui-6d69549787-v2vl6 2/2 Running 0 6m23s kubeflow ml-pipeline-viewer-crd-56f7cfd7d9-phhjn 2/2 Running 1 6m23s kubeflow ml-pipeline-visualizationserver-64447ffc76-zllv2 2/2 Running 0 6m23s kubeflow mysql-c999c6c8-2vhjq 2/2 Running 0 6m23s kubeflow notebook-controller-deployment-84c9bfdf76-r5dc9 2/2 Running 1 (4m33s ago) 4m41s kubeflow profiles-deployment-786df9d89d-mwlsj 3/3 Running 1 (2m4s ago) 2m15s kubeflow tensorboard-controller-deployment-6664b8866f-r6jtv 3/3 Running 1 (2m31s ago) 2m39s kubeflow tensorboards-web-app-deployment-5cb4666798-55j68 2/2 Running 0 2m53s kubeflow training-operator-7589458f95-zvrk9 1/1 Running 0 3m56s kubeflow volumes-web-app-deployment-59cf57d887-r9gt8 2/2 Running 0 4m17s kubeflow workflow-controller-6547f784cd-mzzmw 2/2 Running 1 (6m16s ago) 6m23s 🧐 배포 중 트러블슈팅\n필자의 경우 구성 요소 중 oidc-authservice 에서 트러블슈팅이 발생했다.\n이벤트가 없어 원인을 찾는데 며칠을 소요했다. 원인은 PVC 권한 문제로 EBS CSI Driver 에 대한 IAM role에 대한 OIDC 가 잘못 입력되어 발생하는 것이였다.\nAWS 콘솔에서 OIDC 번호를 수정하니 정상적으로 작동되었다.\nKubeflow 맛보기 kubeflow 배포가 완료되었으면 다음의 명령어를 통해 대시보드에 할 수 있다.\n1 kubectl port-forward --address 0.0.0.0 svc/istio-ingressgateway -n istio-system 8080:80 접속하면 dex 시스템에 아이디와 비밀번호를 입력하자. 공식 문서에 따르면 기본 아이디와 비밀번호는 user@example.com , 12341234 이다.\n이어서 개발 환경인 노트북 서버를 생성해보자. 왼쪽 메뉴에서 [Notebooks] 에서 개발 환경을 설정하자.\n🧐 notebook 생성 트러블슈팅\n필자의 경우 notebook 생성시 다음과 같이 에러가 나온다. 깃이슈를 확인하니 원인은 쥬피터 내부에서 HTTP 접근에서 생긴 보안 에러였다.\n1 2 [403] Could not find CSRF cookie XSRF-TOKEN in the request. http://3.38.94.212:8080/jupyter/api/namespaces/kubeflow-user-example-com/notebooks 필자의 경우 배포되어 있는 jupyer notebook을 수정하였다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 kubectl edit deploy/jupyter-web-app-deployment -n kubeflow --- ... maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: jupyter-web-app kustomize.component: jupyter-web-app spec: containers: - env: - name: APP_PREFIX value: /jupyter - name: UI value: default - name: USERID_HEADER value: kubeflow-userid - name: USERID_PREFIX - name: APP_SECURE_COOKIES value: \u0026#34;false\u0026#34; # ture 에서 false 로 수정 ! image: docker.io/kubeflownotebookswg/jupyter-web-app:v1.7.0 imagePullPolicy: IfNotPresent name: jupyter-web-app ports: - containerPort: 5000 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/config name: config-volume - mountPath: /src/apps/default/static/assets/logos name: logos-volume ... 수정 이후 포트포워딩을 다시해서 시도하면 정상적으로 노트북 서버가 배포된다.\n노트북 서버에 들어가서 간단하게 테스트해보자!\n마치며 이번 글에서는 kubeflow 인프라와 kubeflow 배포까지 구성하였다. 다음 시간에는 kubeflow 기능(파라미터 최적화, GPU 할당) 관련 인프라적인 측면을 딥하게 다뤄보겠다.\n","date":"May 06","permalink":"https://HanHoRang31.github.io/post/spot-and-kubeflow/","tags":["KANS","kubeflow","cloud","AWS","eksctl","eks"],"title":"[AEWS] EKS Spot 인스턴스와 Kubeflow 배포하기"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. EKS ? Amazon Elastic Kubernetes Service(EKS)는 AWS에서 제공하는 관리형 Kubernetes 서비스다. EKS를 사용하면 Kubernetes 클러스터를 생성, 운영 및 유지 관리할 수 있다.\nhttps://catalog.us-east-1.prod.workshops.aws/workshops/9c0aa9ab-90a9-44a6-abe1-8dff360ae428/ko-KR/10-intro/200-eks\nEKS의 주요 특징은 다음과 같다.\n관리형 서비스: EKS는 Kubernetes 컨트롤 플레인이나 데이터 플레인를 설치, 운영 및 유지 관리시켜주는 서비스이다. 사용자가 인프라 설치, 운영, 유지 관리를 할 필요가 없다. 높은 가용성: EKS는 여러 AWS 가용 영역(데이터 센터의 물리적 위치)에 걸쳐 컨트롤 플레인과 데이터 플레인를 분산시켜 서비스의 안정성을 제공한다. 이는 단일 장재 지점을 제거하도록 설계되었으며 자동 관리되는 컨트롤 플레인의 경우 리전 내 개별 가용 영역에서 최소 2개이상의API 서버 노드를 실행한다. AWS 서비스 통합: 타 AWS ECR(도커 레지스트리), ELB(네트워크 로드밸런싱), IAM(보안), VPC(네트워크)와 같은 AWS 서비스와 통합되어 컨테이너 이미지 저장, 로드 밸런싱, 인증 및 격리를 쉽게 관리할 수 있다. 오픈 소스 Kubernetes 호환성: 최신 버전의 오픈 소스 Kubernetes를 사용하여 기존 플러그인과 도구를 그대로 활용할 수 있다. 지원 버전 : 23년 4월 현재 kubernetes 버전 중 1.22~1.26 지원을 지원 중이다. 버전 출시 주기는 연 3회이며 각 버전은 12개월 동안 지원된다. 지원이 끝난 버전들은 자동으로 EKS가 업데이트시킨다. EKS 아키텍처 EKS 아키텍처는 크게 컨트롤 플레인(마스터 노드)과 데이터 플레인(워커 노드)로 나뉜다. 아키텍처 그림은 스터디에서 공유해주신 2022 AWS 마이그레이션 요점 정리 pdf를 기반으로 살펴보겠다. 컨트롤 플레인 EKS에서는 Kubernetes 컨트롤 플레인의 가용성과 내구성을 손상시킬 수 있는 단일 장애 지점을 제거하도록 설계되었다. 컨트롤 플레인은 API 서버 노드, etcd 클러스터로 구성된다.\nAPI 서버: 쿠버네티스 클러스터의 모든 작업을 처리하고 상태를 저장하는 중앙 허브이다. RESTful API를 통해 통신하며, 사용자 요청에 따라 클러스터의 상태를 변경하거나 정보를 반환시켜준다. etcd: etcd는 쿠버네티스 클러스터에서 사용하는 분산 키-값 저장소이다. 쿠버네티스의 모든 설정 데이터와 클러스터 상태 정보를 저장한다. 각 API 서버와 etcd는 개별 가용 영역에서 최소 2개 이상의 클러스터로 구성되어 실행된다. 이를 통해 단일 가용 영역의 이벤트가 EKS 클러스터 가용성에 영향을 미치지 않는다. 각 가용 영역에서는 NAT 게이트웨이를 통해 프라이빗 서브넷에서 실행되며 사용자는 해당 노드에 접근할 수 없다. 또한 API 서버는 NLB로 ETCD는 ELB를 사용하여 컨트롤 플레인 서버의 부하를 분산시킨다. API 서버와 ETCD 클러스터는 오토스케일링 그룹으로 구성되어 조건(클러스터 규모 , API 서버 및 etcd에 대한 요청 증가)에 따라 자동으로 리소스가 확장된다. 데이터 플레인 데이터 플레인은 쿠버네티스 클러스터 내에서 워크로드를 실행하고 관리되는 서버다. 아키텍처는 다음과 같다.\nkubelet: 노드 에이전트이다. kubelet은 API 서버로부터 파드를 실행하고 관리해야 하는 명령을 받아 서버에 반영시켜준다. 또 실행 중인 파드와 컨테이너를 관리하고, 상태를 모니터링하며, 필요한 경우 API 서버에 상태 정보를 보고하는 역할을 수행한다. kube-proxy: 네트워크 프록시 및 로드 밸런서이다. kube-proxy는 쿠버네티스 서비스에 대한 요청을 적절한 파드로 전달하고, 파드 간의 통신을 관리한다. EKS 의 기본 네트워크 CNI로는 VPC-CNI가 실행된다. 또한, 사용자의 요구에 맞게 데이터 플레인의 구성 옵션을 설정할 수 있다. 구성 옵션별 세부 정보는 다음 그림과 같다.\nhttps://aws.github.io/aws-eks-best-practices/reliability/docs/\n그림에서 비교한 것과 같이 사용자 책임 레벨에 따라 3가지로 구분된다.\nSelf Managed Workers :사용자가 직접 노드를 구성하고 관리하는 방식이다. 구체적으로는 Custom AMI를 통해 작업자 노드를 생성하며 AMI와 노드의 패치 및 업그레이드를 사용자가 직접 관리한다. Managed Node groups : AWS가 노드를 자동으로 프로비저닝하고, 업데이트 및 유지 관리를 처리한다. EKS Fargate : 서버리스 컴퓨팅 옵션이다. Micro VM를 이용하여 Pod별 VM이 할당된다. AWS가 설정에 맞게 파드 레벨의 스케일링을 자동으로 처리한다. Cluster Endpoint Access EKS 클러스터의 API 서버에 대한 접근을 제어하는 설정이다. 클러스터 엔드포인트를 설정함으로써, 클러스터에 대한 접근을 필요한 범위로 제한하여 보안을 강화할 수 있다. 옵션은 3가지 존재한다.\nPublic : 클러스터 엔드포인트가 Public IP로 할당되어 인터넷에서 접근이 가능한 설정이다. 클러스터 내부에서도 IGW 를 통해 Public IP와 통신한다.\n해당 옵션은 AWS 콘솔에서 확인 가능하다. Public 설정시 다음과 같이 접근이 가능하다.\n위와 같이 인터넷을 통해 접근이 가능하여 접근성을 높일 수 있다.\nPublic Private : 워크노드 내부에서는 Private(VPC 내부망에서만 접근 가능)로, API 접근은 Public(인터넷을 통한 접근 가능)으로만 통신이 가능하다. 워크 노드에서 민감데이터 노출을 막기 위해 Private subnet(사설망)에 위치시킨 경우 사용할 수 있는 옵션이다.\nPrivate : API 서버 엔드포인트 또한 허용된 VPC에서만 접근이 가능한 설정이다.\n해당 옵션으로 사용시 API Server URL은 Route53 privated hosted zone으로 변경되어 VPC 내부에서만 통신이 가능하다. 해당 URL은 AWS 콘솔의 EKS나 kubectl -v=6 옵션으로 확인이 가능하다.\n1 2 3 4 # API Server URL(Route53 privated hosted zone) (terraform-eks@my-eks:N/A) [root@myeks-host example]# kubectl get pods -A -v=6 I0428 14:26:36.609385 27821 loader.go:374] Config loaded from file: /root/.kube/config I0428 14:26:37.276259 27821 round_trippers.go:553] GET https://232B71CC97744BC94C09D40801D073B0.yl4.ap-northeast-2.eks.amazonaws.com/api/v1/pods?limit=500 200 OK in 661 milliseconds 위 [https://232B71CC97744BC94C09D40801D073B0.yl4.ap-northeast-2.eks.amazonaws.com](https://232B71CC97744BC94C09D40801D073B0.yl4.ap-northeast-2.eks.amazonaws.com) 가 API server URL이다. 해당 URL은 EKS 컨트롤 노드가 소유하고 있는 네트워크 인터페이스를 통해 통신된다. 통칭 EKS owned ENI는 AWS 콘솔에서 확인이 가능하다.\n네트워크 인터페이스에서 클러스터를 배포한 VPC를 입력하여 인스턴스 ID가 존재하지 않고(- 표시), 설명에 Amazone EKS cluster-name 으로 표시된 것이 EKS owned ENI 이다. 인스턴스 소유자 또한 AWS 소유자 번호가 다른 것을 확인할 수 있는데 AWS가 직접 관리하는 컨트롤 노드의 소유자 번호이기 때문이다.\nEKS 배포 방식 EKS 배포 방식은 Manual, Command line utility, Infrastructure as Code 에 따라 구성할 수 있다.\nManual : AWS Management Console 에서 직접 구성 Command line utility : 커맨드 명령어를 통한 구성 (AWS CLI, eksctl) Infrastructure as Code : 코드를 통한 구성(Terrafrom, AWS CDK 등) 이번 블로그 글에서는 eksctl 를 통해 클러스터를 구성하겠다. eksctl ? Amazon EKS (Elastic Kubernetes Service) 클러스터를 생성하고 관리하기 위한 명령줄 인터페이스(CLI) 툴이다. eksctl를 통해 클러스터를 구성 및 삭제, 업데이트할 수 있으며 VPC, 서브넷, 리소스 생성을 위한 정책 생성 등의 작업을 쉽게 처리할 수 있다. 공식 문서를 통해 다양한 인프라 환경에서의 EKS 구성을 참고할 수 있다.\n아쉬운 점은 EKS에서만 구축이 된다는 점이다. 추후 다른 클러스터나 멀티 클러스터 구축시 다른 툴을 사용하자.\n추가로 사용해보니.. eksctl는 형상관리가 불가능하다. 한 번 배포 이후에는 eksctl 구성 파일을 통해 추가 작업이 불가능하다. eksctl를 통해 기존의 리소스 변경이 불가능하며 추가만 가능하다. eksctl 를 통한 EKS 배포 eksctl를 통해 EKS Private Cluster 를 배포하겠다. 구성 아키텍처는 다음과 같다.\n노드 그룹은 unmanaged node group 으로, Cluster endpoint 는 Private로 구성하였다. 아키텍처에서 화살표는 통신 아키텍처이다.\nEKS Admin(빨간선) : 베스천 서버를 통해 EKS API Server로 접근하여 EKS를 관리하는 과정이다. Private Worker node(보라선): Private cluster 로 구성되어 EKS control node ENI를 통해 EKS Control node와 통신한다. 배포 과정은 다음과 같이 진행하겠다.\ncloudformation를 통한 베스천 서버 배포 eksctl를 통한 EKS 배포 1. AWS Cloudformation를 통한 베스천 서버 배포 AWS Cloudformation을 통해 VPC 구성 및 베스천 서버를 배포하고 eksctl 설치 및 관리 패키지를 설치하겠다. Cloudformation 스크립트는 스터디에서 공유해주신 것을 기반으로 내용을 추가하였다.\n1 2 # yaml 파일 다운로드 curl -O https://github.com/HanHoRang31/blog-share/blob/main/aews-eksctl/cloudformation-bastion.yaml 구성 코드에서 Private 클러스터 구축을 위한 중요 부분과 베스천 서버의 패키지 구성 정보를 확인하겠다. 먼저 Private 클러스터 구축을 위해 필요한 보안 그룹은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # cloudformation-bastion # 파라미터 ... # 베스천 서버 보안 그룹 설정 # EKSCTL-Host EKSEC2SG: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: eksctl-host Security Group VpcId: !Ref EksVPC Tags: - Key: Name Value: !Sub ${ClusterBaseName}-HOST-SG SecurityGroupIngress: - IpProtocol: tcp FromPort: \u0026#39;22\u0026#39; ToPort: \u0026#39;22\u0026#39; CidrIp: !Ref SgIngressSshCidr # 베스천 서버에서 EKS 배포를 위한 보안 그룹 설정 ControlPlaneSecurityGroup: DependsOn: - EKSEC2SG Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Cluster communication with worker nodes VpcId: !Ref EksVPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 443 ToPort: 443 SourceSecurityGroupId: !Ref EKSEC2SG Tags: - Key: Name Value: !Sub ${ClusterBaseName} Control Plane Security Group ... Private Cluster 구축으로 중요 사항은 두 가지이다.\nEKSEC2SG : 베스천 서버에 대한 보안 그룹 설정 부분이다. 해당 부분을 통해 베스천 서버에 접근할 수 있는 IP 범위를 제한할 수 있다. 코드 내 SgIngressSshCidr 에서 지정한 IP를 제한하며 해당 값은 배포 명령어로 override 을 통해 값을 수정할 수 있다. ControlPlaneSecurityGroup : 컨트롤 플레인과 워크 노드간 통신을 위한 보안 그룹 설정 부분이다. 코드에서는 베스천 서버와의 통신을 위해 443 포트의 보안 그룹을 추가하였다. userdata를 확인하면 베스천 서버에 설치되는 패키지를 확인할 수 있다. 설치 명령어는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 ... UserData: Fn::Base64: !Sub | #!/bin/bash hostnamectl --static set-hostname \u0026#34;${ClusterBaseName}-host\u0026#34; # Config convenience echo \u0026#39;alias vi=vim\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#34;sudo su -\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc # Change Timezone sed -i \u0026#34;s/UTC/Asia\\/Seoul/g\u0026#34; /etc/sysconfig/clock ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # Install Packages cd /root yum -y install tree jq git htop lynx # Install kubectl \u0026amp; helm #curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.26.2/2023-03-17/bin/linux/amd64/kubectl curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.25.7/2023-03-17/bin/linux/amd64/kubectl install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash # Install eksctl curl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp mv /tmp/eksctl /usr/local/bin # Install aws cli v2 curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 sudo ./aws/install complete -C \u0026#39;/usr/local/bin/aws_completer\u0026#39; aws echo \u0026#39;export AWS_PAGER=\u0026#34;\u0026#34;\u0026#39; \u0026gt;\u0026gt;/etc/profile export AWS_DEFAULT_REGION=${AWS::Region} echo \u0026#34;export AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION\u0026#34; \u0026gt;\u0026gt; /etc/profile # Install YAML Highlighter wget https://github.com/andreazorzetto/yh/releases/download/v0.4.0/yh-linux-amd64.zip unzip yh-linux-amd64.zip mv yh /usr/local/bin/ # Install krew curl -LO https://github.com/kubernetes-sigs/krew/releases/download/v0.4.3/krew-linux_amd64.tar.gz tar zxvf krew-linux_amd64.tar.gz ./krew-linux_amd64 install krew export PATH=\u0026#34;$PATH:/root/.krew/bin\u0026#34; echo \u0026#39;export PATH=\u0026#34;$PATH:/root/.krew/bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile # Install kube-ps1 echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;alias k=kubectl\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;complete -F __start_kubectl k\u0026#39; \u0026gt;\u0026gt; /etc/profile git clone https://github.com/jonmosco/kube-ps1.git /root/kube-ps1 cat \u0026lt;\u0026lt;\u0026#34;EOT\u0026#34; \u0026gt;\u0026gt; /root/.bash_profile source /root/kube-ps1/kube-ps1.sh KUBE_PS1_SYMBOL_ENABLE=false function get_cluster_short() { echo \u0026#34;$1\u0026#34; | cut -d . -f1 } KUBE_PS1_CLUSTER_FUNCTION=get_cluster_short KUBE_PS1_SUFFIX=\u0026#39;) \u0026#39; PS1=\u0026#39;$(kube_ps1)\u0026#39;$PS1 EOT # Install krew plugin kubectl krew install ctx ns get-all # ktop df-pv mtail tree # Install Docker amazon-linux-extras install docker -y systemctl start docker \u0026amp;\u0026amp; systemctl enable docker # Install nerdctl wget https://github.com/containerd/nerdctl/releases/download/v1.3.1/nerdctl-1.3.1-linux-amd64.tar.gz tar -xzf nerdctl-1.3.1-linux-amd64.tar.gz sudo mv nerdctl /usr/local/bin/ # CLUSTER_NAME export CLUSTER_NAME=${ClusterBaseName} echo \u0026#34;export CLUSTER_NAME=$CLUSTER_NAME\u0026#34; \u0026gt;\u0026gt; /etc/profile # Create SSH Keypair ssh-keygen -t rsa -N \u0026#34;\u0026#34; -f /root/.ssh/id_rsa ... 몇 가지 alias와 환경 설정을 추가하고, 시스템의 시간대를 Asia/Seoul로 변경한다. 필요한 패키지들(tree, jq, git, htop, lynx, kubectl, helm, eksctl, AWS CLI v2, yh, krew, docker, nerdctl, kube-ps1)을 설치한다. 클러스터 이름을 설정하고, SSH 키 페어를 생성한다. 배포는 다음과 같은 파라미터를 통해 진행하였다.\n1 aws cloudformation deploy --template-file myeks-1week.yaml --stack-name myeks --parameter-overrides KeyName=eks-terraform-key --region ap-northeast-2 배포 완료 후 다음의 명령어를 통해 베스천 서버에 접근하고 EKS 배포 과정으로 넘어가겠다.\n1 ssh -i eks-terraform-key.pem ec2-user@$(aws cloudformation describe-stacks --stack-name myeks --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text) 2. eksctl를 통한 EKS 배포 베스천 서버에 접속하여 AWS 인증 정보를 입력하자.\n1 2 3 4 5 [root@myeks-host example]# aws configure AWS Access Key ID [None]: ACCESS-KEY AWS Secret Access Key [None]: SECRET-KEY Default region name [None]: ap-northeast-2 Default output format [None]: json AWS 인증 정보 입력 후 eksctl를 통해 private cluster 구성하자. 구성 내용은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: my-eks region: ap-northeast-2 # 지역 설정(서울) version: \u0026#34;1.25\u0026#34; # 클러스터 버전 vpc: id: vpc-05a960a0837da1328 # 베스천 서버 VPC ID (아래 내용 참고) cidr: 192.168.0.0/16 # # 베스천 서버 VPC CIDR (아래 내용 참고) securityGroup: sg-0c59ddf1a9a73edc9 nat: gateway: HighlyAvailable subnets: public: public-2a: id: subnet-06391e7ab56a8ae9c # 서브넷 ID (아래 내용 참고) cidr: 192.168.1.0/24 # 서브넷 CIDR (아래 내용 참고) public-2c: id: subnet-00c193bd6e515a79b # 서브넷 ID (아래 내용 참고) cidr: 192.168.2.0/24 # 서브넷 CIDR (아래 내용 참고) private: private-2a: id: subnet-02d592518f7ae0755 # 서브넷 ID (아래 내용 참고) cidr: 192.168.3.0/24 # 서브넷 CIDR (아래 내용 참고) private-2c: id: subnet-0dcfc3b165e7b355d # 서브넷 ID (아래 내용 참고) cidr: 192.168.4.0/24 # 서브넷 CIDR (아래 내용 참고) clusterEndpoints: # 클러스터 엔드포인트 액세스 설정 부분 publicAccess: false # 공용 액세스 비활성화 privateAccess: true # 사설 액세스 활성화 nodeGroups: - name: ng-1 instanceType: m5.xlarge # 인스턴스 유형 desiredCapacity: 3 # 원하는 노드 수 privateNetworking: true # 사설 네트워크 사용 ssh: publicKeyName: ec2-key # ec2 보안 키 availabilityZones: - ap-northeast-2a - ap-northeast-2c iam: withAddonPolicies: imageBuilder: true # 이미지 빌더 정책 활성화 albIngress: true # ALB 인그레스 정책 활성화 cloudWatch: true # CloudWatch 정책 활성화 autoScaler: true # 오토 스케일러 정책 활성화 instanceName: EKS-WORKER-TEST volumeSize: 30 # 볼륨 크기 설정 구성 내용 중 VPC, Subnet ID, CIDR은 AWS Console에서 확인할 수 있다. VPC → Resource Road Map 을 통해 연결된 서브넷 및 라우팅 정보를 확인할 수 있다. 정보가 필요한 리소스를 클릭하면 바로 넘어가진다.\n보안 그룹 ID 는 EC2→ 보안 그룹에서 Name myeks Control Plane Security Group 에서 확인할 수 있다.\n구성 입력 후 eksctl 명령어를 통해 클러스터를 구축하겠다.\n1 eksctl create cluster -f private-cluster.yaml 배포는 약 20분정도 소요된다. 구성 후 클러스터 API 서버 접근을 위한 인증 정보가 필요하다. 인증 정보는 다음의 eksctl 명령어를 통해 저장할 수 있다.\n1 2 3 (terraform-eks@my-eks-2:N/A) [root@myeks-host example]# eksctl utils write-kubeconfig --name my-eks --region ap-northeast-2 Flag --name has been deprecated, use --cluster 2023-04-29 19:59:36 [✔] saved kubeconfig as \u0026#34;/root/.kube/config\u0026#34; 배포 확인을 위해 파드 리스트를 확인해보자.\n1 2 3 4 5 6 7 8 9 10 11 12 (terraform-eks@my-eks:N/A) [root@myeks-host example]# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-4kms4 1/1 Running 0 109m kube-system aws-node-8r2fq 1/1 Running 0 109m kube-system aws-node-hsgmr 1/1 Running 0 109m kube-system aws-node-nf758 1/1 Running 0 109m kube-system coredns-76b4dcc5cc-4mbw2 1/1 Running 0 127m kube-system coredns-76b4dcc5cc-vfpgp 1/1 Running 0 127m kube-system kube-proxy-68vgc 1/1 Running 0 109m kube-system kube-proxy-lgltk 1/1 Running 0 109m kube-system kube-proxy-lqngn 1/1 Running 0 109m kube-system kube-proxy-vl4tb 1/1 Running 0 109m 앞서 공유한 베스천 서버 인프라 구성 파일과 eks 구성 파일을 통해 진행하면 아무 문제 없이 구성이 완료될 것이다. 아래 내용은 필자가 private cluster 구성 중 생긴 에러로 트러블슈팅한 내용이다. 혹시 구성 중 에러가 발생한다면 다음 내용을 참고하자.\n트러블슈팅 클러스터 배포 중 timeout 발생과 워크 노드 클러스터 조인\n워크 노드 조인 중 에러가 생긴 에러이다. 에러 메세지는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 (N/A:N/A) [root@myeks-host example]# eksctl create cluster -f private-cluster.yaml 2023-04-28 12:24:04 [ℹ] eksctl version 0.138.0 2023-04-28 12:24:04 [ℹ] using region ap-northeast-2 2023-04-28 12:24:04 [!] warning, having public access disallowed will subsequently interfere with some features of eksctl. This will require running subsequent eksctl (and Kubernetes) commands/API calls from within the VPC. Running these in the VPC requires making updates to some AWS resources. See: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html for more details 2023-04-28 12:24:05 [✔] using existing VPC (vpc-06210c8560709c761) and subnets (private:map[private-2a:{subnet-0cceb4f7117a82214 ap-northeast-2a 192.168.3.0/24 0 } private-2c:{subnet-0c19b6b58320fce0a ap-northeast-2c 192.168.4.0/24 0 }] public:map[public-2a:{subnet-023a838543987d725 ap-northeast-2a 192.168.1.0/24 0 } public-2c:{subnet-09e710a10ef0fb5ef ap-northeast-2c 192.168.2.0/24 0 }]) 2023-04-28 12:24:05 [!] custom VPC/subnets will be used; if resulting cluster doesn\u0026#39;t function as expected, make sure to review the configuration of VPC/subnets 2023-04-28 12:24:05 [ℹ] nodegroup \u0026#34;ng-2\u0026#34; will use \u0026#34;ami-0fdcb707922882aef\u0026#34; [AmazonLinux2/1.25] 2023-04-28 12:24:05 [ℹ] using EC2 key pair \u0026#34;eks-terraform-key\u0026#34; 2023-04-28 12:24:05 [ℹ] using Kubernetes version 1.25 2023-04-28 12:24:05 [ℹ] creating EKS cluster \u0026#34;my-eks\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; region with un-managed nodes 2023-04-28 12:24:05 [ℹ] 1 nodegroup (ng-2) was included (based on the include/exclude rules) 2023-04-28 12:24:05 [ℹ] will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s) 2023-04-28 12:24:05 [ℹ] will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s) 2023-04-28 12:24:05 [ℹ] if you encounter any issues, check CloudFormation console or try \u0026#39;eksctl utils describe-stacks --region=ap-northeast-2 --cluster=my-eks\u0026#39; 2023-04-28 12:24:05 [ℹ] Kubernetes API endpoint access will use provided values {publicAccess=false, privateAccess=true} for cluster \u0026#34;my-eks\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; 2023-04-28 12:24:05 [ℹ] CloudWatch logging will not be enabled for cluster \u0026#34;my-eks\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; 2023-04-28 12:24:05 [ℹ] you can enable it with \u0026#39;eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-northeast-2 --cluster=my-eks\u0026#39; 2023-04-28 12:24:05 [ℹ] 2 sequential tasks: { create cluster control plane \u0026#34;my-eks\u0026#34;, 2 sequential sub-tasks: { wait for control plane to become ready, create nodegroup \u0026#34;ng-2\u0026#34;, } } 2023-04-28 12:24:05 [ℹ] building cluster stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:24:05 [ℹ] deploying stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:24:35 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:25:05 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:26:05 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:27:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:28:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:29:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:30:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:31:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:32:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:33:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:34:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:36:06 [ℹ] building nodegroup stack \u0026#34;eksctl-my-eks-nodegroup-ng-2\u0026#34; 2023-04-28 12:36:06 [ℹ] --nodes-min=2 was set automatically for nodegroup ng-2 2023-04-28 12:36:06 [ℹ] --nodes-max=2 was set automatically for nodegroup ng-2 2023-04-28 12:36:07 [ℹ] deploying stack \u0026#34;eksctl-my-eks-nodegroup-ng-2\u0026#34; 2023-04-28 12:36:07 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-nodegroup-ng-2\u0026#34; 2023-04-28 12:39:43 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-nodegroup-ng-2\u0026#34; 2023-04-28 12:39:43 [ℹ] waiting for the control plane to become ready 2023-04-28 12:39:44 [✔] saved kubeconfig as \u0026#34;/root/.kube/config\u0026#34; 2023-04-28 12:39:44 [ℹ] no tasks 2023-04-28 12:39:44 [✔] all EKS cluster resources for \u0026#34;my-eks\u0026#34; have been created 2023-04-28 12:39:44 [ℹ] adding identity \u0026#34;arn:aws:iam::955963799952:role/eksctl-my-eks-nodegroup-ng-2-NodeInstanceRole-283XKKCXM9GT\u0026#34; to auth ConfigMap 2023-04-28 12:39:44 [ℹ] nodegroup \u0026#34;ng-2\u0026#34; has 0 node(s) 2023-04-28 12:39:44 [ℹ] waiting for at least 2 node(s) to become ready in \u0026#34;ng-2\u0026#34; Error: timed out waiting for at least 2 nodes to join the cluster and become ready in \u0026#34;ng-2\u0026#34;: context deadline exceeded 보통 타임에러로 표시되며 kubectl 명령어를 확인하면 워크 노드가 조인이 안되어 파드가 정상적으로 배포되지 않는 것을 확인할 수 있다.\n해당 내용은 깃 이슈에서 참고할 수 있지만 답을 찾을 수 없어 며칠을 고생했다. 원인은 EKS 보안 그룹 설정이였다. 앞서 베스천서버 구성시 EKS 통신을 위한 보안 그룹을 설정하였는데 추가를 안하면 EKS 워크 노드와 베스천 서버와의 통신이 안되어 조인이 안된다. EKS 보안 그룹을 설정하고 다시 EKS를 배포하자.\nAPI 서버 접근이 안되는 경우\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (N/A:N/A) [root@myeks-host example]# eksctl create cluster -f private-cluster.yaml 2023-04-27 19:45:28 [ℹ] eksctl version 0.138.0 2023-04-27 19:45:28 [ℹ] using region ap-northeast-2 2023-04-27 19:45:28 [!] warning, having public access disallowed will subsequently interfere with some features of eksctl. This will require running subsequent eksctl (and Kubernetes) commands/API calls from within the VPC. Running these in the VPC requires making updates to some AWS resources. See: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html for more details 2023-04-27 19:45:28 [ℹ] setting availability zones to [ap-northeast-2a ap-northeast-2b ap-northeast-2d] 2023-04-27 19:45:28 [ℹ] subnets for ap-northeast-2a - public:192.168.0.0/19 private:192.168.96.0/19 2023-04-27 19:45:28 [ℹ] subnets for ap-northeast-2b - public:192.168.32.0/19 private:192.168.128.0/19 2023-04-27 19:45:28 [ℹ] subnets for ap-northeast-2d - public:192.168.64.0/19 private:192.168.160.0/19 2023-04-27 19:45:28 [ℹ] nodegroup \u0026#34;EKS-PRIVATE-NODE\u0026#34; will use \u0026#34;ami-0fdcb707922882aef\u0026#34; [AmazonLinux2/1.25] 2023-04-27 19:45:28 [ℹ] using Kubernetes version 1.25 2023-04-27 19:45:28 [ℹ] creating EKS cluster \u0026#34;my-private-eks\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; region with un-managed nodes ... Error: getting auth ConfigMap: Get \u0026#34;https://AA1694EDA538EFE2ADC5FCCABBB4F745.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/configmaps/aws-auth\u0026#34;: dial tcp 192.168.175.32:443: i/o timeout (terraform-eks@my-private-eks:N/A) [root@myeks-host example]# kubectl get pods -A Unable to connect to the server: dial tcp 192.168.134.188:443: i/o timeout → 앞에도 다뤘지만 Timeout 의 대부분의 원인이 보안 그룹이다. 이 경우는 클러스터 구성 파일에서 베스천 서버의 보안그룹을 설정하지 않아서 생긴 문제였다. 클러스터 구성에서 베스천 서버에 대한 보안 그룹(아웃 바운드 베스천서버 443 포트)을 설정하면 된다. 클러스터 구성에서 보안 그룹 설정은 AWS 콘솔 → EKS에서 가능하다.\nAWS CLI 및 eksctl 결과 i/o timeout\n1 2 (N/A:N/A) [root@myeks-host example]# eksctl get cluster --region ap-northeast-2 Error: checking AWS STS access – cannot get role ARN for current session: operation error STS: GetCallerIdentity, https response error StatusCode: 0, RequestID: , request send failed, Post \u0026#34;https://sts.ap-northeast-2.amazonaws.com/\u0026#34;: dial tcp 52.95.192.98:443: i/o timeout 마찬가지로 보안 그룹 문제였다. 베스천 서버의 아웃바운드에 0.0.0.0/0를 추가하면 해결된다.\n콘솔에서 컴퓨팅 리소스 확인이 안되는 경우\n다음 그림과 같이 EKS 배포이후 AWS 콘솔에서 워크 노드가 표시되지 않는 경우이다.\n원인은 EKS 클러스터 사용자 정보에 AWS 사용자 정보가 없기 때문이다. 다음과 같이 추가하도록 하자.\n1 kubectl edit cm/aws-auth -n kube-system 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::000000000:role/eksctl-my-eks-nodegroup-ng-1-NodeInstanceRole-ZG7JVL5Z4IPU username: system:node:{{EC2PrivateDNSName}} mapUsers: | - userarn: arn:aws:iam:000000000:user/hanhorang # AWS 인증에서 사용한 IAM 사용자 arn를 입력하자. username: hanhorang groups: - system:masters kind: ConfigMap metadata: name: aws-auth namespace: kube-system mapUsers에서 IAM 사용자 arn을 추가하면 해결된다.\n클러스터 구성 확인 클러스터 구성 후 클러스터 정보와 인스턴스 정보를 확인하겠다.\n클러스터 구성 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 클러스터 확인 (terraform-eks@my-eks:N/A) [root@myeks-host example]# eksctl get cluster NAME REGION EKSCTL CREATED my-eks ap-northeast-2 True # 클러스터 노드 그룹 확인 (terraform-eks@my-eks:N/A) [root@myeks-host example]# eksctl get nodegroup --cluster my-eks CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAMETYPE my-eks ng-1 CREATE_COMPLETE 2023-04-29T09:05:31Z 4 4 4 m5.xlarge ami-0fdcb707922882aef eksctl-my-eks-nodegroup-ng-1-NodeGroup-IWPRDQX6J0CG unmanaged # 클러스터 접근 정보 확인 및 노드 확인 (terraform-eks@my-eks:N/A) [root@myeks-host example]# kubectl get nodes -v6 I0429 20:21:39.855710 5753 loader.go:374] Config loaded from file: /root/.kube/config I0429 20:21:40.631509 5753 round_trippers.go:553] GET https://8F53A6D3D93C1D751527688A7CB07659.yl4.ap-northeast-2.eks.amazonaws.com/api/v1/nodes?limit=500 200 OK in 769 milliseconds NAME STATUS ROLES AGE VERSION ip-192-168-3-31.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 131m v1.25.7-eks-a59e1f0 ip-192-168-3-59.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 131m v1.25.7-eks-a59e1f0 ip-192-168-4-195.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 131m v1.25.7-eks-a59e1f0 ip-192-168-4-250.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 131m v1.25.7-eks-a59e1f0 # 클러스터 정보 확인 kubectl cluster-info dump ... 베스천 서버에서 워크 노드 접근을 위한 보안 그룹 설정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # 인스턴스 IP 확인 terraform-eks@my-eks:N/A) [root@myeks-host example]# aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output table ------------------------------------------------------------------- | DescribeInstances | +-----------------+-----------------+------------------+----------+ | InstanceName | PrivateIPAdd | PublicIPAdd | Status | +-----------------+-----------------+------------------+----------+ | EKS-WORKER-TEST| 192.168.4.230 | None | running | | EKS-WORKER | 192.168.4.195 | None | running | | EKS-WORKER | 192.168.4.250 | None | running | | EKS-WORKER | 192.168.3.59 | None | running | | EKS-WORKER | 192.168.3.31 | None | running | | myeks-host | 192.168.1.100 | 43.201.102.195 | running | +-----------------+-----------------+------------------+----------+ # 워크 노드로 Ping을 하나 안된다. (terraform-eks@my-eks:N/A) [root@myeks-host example]# ping 192.168.3.31 PING 192.168.3.31 (192.168.3.31) 56(84) bytes of data. # 노드 보안그룹 ID 확인하여 베스천 서버 IP를 추가하자 (terraform-eks@my-eks:N/A) [root@myeks-host example]# aws ec2 describe-security-groups --filters Name=group-name,Values=*nodegroup* --query \u0026#34;SecurityGroups[*].[GroupId]\u0026#34; --output text sg-0bf33458f7e193841 sg-0f4cd52a07786b9fb # --group-id 에 위 보안 그룹 하나 입력 (terraform-eks@my-eks:N/A) [root@myeks-host example]# aws ec2 authorize-security-group-ingress --group-id sg-0f4cd52a07786b9fb --protocol \u0026#39;-1\u0026#39; --cidr 192.168.1.100/32 { \u0026#34;Return\u0026#34;: true, \u0026#34;SecurityGroupRules\u0026#34;: [ { \u0026#34;SecurityGroupRuleId\u0026#34;: \u0026#34;sgr-01bca8eecf69c86b6\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;sg-0f4cd52a07786b9fb\u0026#34;, \u0026#34;GroupOwnerId\u0026#34;: \u0026#34;955963799952\u0026#34;, \u0026#34;IsEgress\u0026#34;: false, \u0026#34;IpProtocol\u0026#34;: \u0026#34;-1\u0026#34;, \u0026#34;FromPort\u0026#34;: -1, \u0026#34;ToPort\u0026#34;: -1, \u0026#34;CidrIpv4\u0026#34;: \u0026#34;192.168.1.100/32\u0026#34; } ] } # 접근 확인 (terraform-eks@my-eks:N/A) [root@myeks-host example]# ping 192.168.3.31 PING 192.168.3.31 (192.168.3.31) 56(84) bytes of data. 64 bytes from 192.168.3.31: icmp_seq=1 ttl=255 time=0.176 ms 64 bytes from 192.168.3.31: icmp_seq=2 ttl=255 time=0.152 ms 64 bytes from 192.168.3.31: icmp_seq=3 ttl=255 time=0.144 ms 인스턴스 정보 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # kubelet 확인 systemctl status kubelet ● kubelet.service - Kubernetes Kubelet Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet-args.conf, 30-kubelet-extra-args.conf Active: active (running) since Sat 2023-04-29 09:08:56 UTC; 2h 49min ago Docs: https://github.com/kubernetes/kubernetes Process: 3176 ExecStartPre=/sbin/iptables -P FORWARD ACCEPT -w 5 (code=exited, status=0/SUCCESS) Main PID: 3178 (kubelet) Tasks: 16 Memory: 77.3M CGroup: /runtime.slice/kubelet.service └─3178 /usr/bin/kubelet --config /etc/kubernetes/kubelet/kubelet-config.json --kubeconfig /var/lib/kubelet/kubeconfig --container-runtime-endpoint unix://... Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.377073 3178 kubelet.go:2117] \u0026#34;SyncLoop ADD\u0026#34; source=\u0026#34;api\u0026#34; pods=...sncki] Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.377103 3178 topology_manager.go:205] \u0026#34;Topology Admit Handler\u0026#34; Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.513465 3178 reconciler.go:357] \u0026#34;operationExecutor.VerifyControllerAt... Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.614154 3178 reconciler.go:269] \u0026#34;operationExecutor.MountVolume starte... Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.632046 3178 operation_generator.go:730] \u0026#34;MountVolume.SetUp succeeded... Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.703987 3178 util.go:30] \u0026#34;No sandbox for pod can be found. Need...sncki\u0026#34; Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.791915 3178 provider.go:102] Refreshing cache for provider: *c...ovider Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.792001 3178 provider.go:82] Docker config file not found: coul...t /] Apr 29 11:52:07 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:07.235202 3178 kubelet.go:2155] \u0026#34;SyncLoop (PLEG): event for pod\u0026#34; ...a6526} Apr 29 11:52:11 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:11.242075 3178 kubelet.go:2155] \u0026#34;SyncLoop (PLEG): event for pod\u0026#34; ...4351a} Hint: Some lines were ellipsized, use -l to show in full. # 볼륨 확인 lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:0 0 30G 0 disk ├─nvme0n1p1 259:1 0 30G 0 part / └─nvme0n1p128 259:2 0 1M 0 part # 컨테이너 런타임 확인 ps axf |grep /usr/bin/containerd 3013 ? Ssl 0:46 /usr/bin/containerd 4269 ? Sl 0:10 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 0ea51c82eb135fca4bdae99f89e8f5804d1e144efb857e8fae310a9d7039e21a -address /run/containerd/containerd.sock 4270 ? Sl 0:02 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id f723a90d2f436d77f37789bc29e26fcdbca82ca575bfacde43be2a0978573634 -address /run/containerd/containerd.sock 24478 ? Sl 0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id c1657f716d01283e8e7aa183f6583ec4c96af3d1d19b081004d1c565015a6526 -address /run/containerd/containerd.sock 26752 ? S+ 0:00 \\_ grep --color=auto /usr/bin/containerd 참고 https://awskoreamarketingasset.s3.amazonaws.com/2022 Summit/pdf/T14S4_Amazon EKS 마이그레이션 요점 정리.pdf\nhttps://341123.tistory.com/m/6\n","date":"Apr 29","permalink":"https://HanHoRang31.github.io/post/aews-1-eks/","tags":["KANS","eks","cloud","AWS","kubernetes"],"title":"[AEWS] EKS 아키텍처와 Private EKS 클러스터 배포하기"},{"categories":null,"contents":" 1 2 [테크 따라잡기]는 IT 기업 기술 스택 사례를 참고하여 구현하고 정리하는 시리즈입니다. 오로지 기술 성장을 위해 작성할 예정이며 스터디 과정에서 얻는 인사이트를 공유하고자 합니다. 도커 레지스트리인 Harbor에 대해 사용 사례를 조사하던 중 배울 것이 많은 테크 블로그 글을 발견하였다. 그대로 글로만 보기에는 필자가 아쉬워서 직접 구현하고 구현 과정에서 얻는 인사이트를 공유하고자 블로그 글을 작성하였다. 참고한 테크 블로그는 다음의 링크에서 확인이 가능하다.\nhttps://engineering.linecorp.com/ko/blog/harbor-for-private-docker-registry\n블로그 글을 요약하자면, Private Docker Registry로 Harbor를 선택하여 구성하였고, 오픈소스 오브젝트 스토리지인 Minio를 통해 스토리지 고가용성을 보장하였다. 또한 사내 LDAP 서버를 이용하여 직원 정보를 Harbor와 연계하여 로그인 정보를 연계하였다고 한다.\n기술 구현의 큰 틀은 같지만, 쿠버네티스 환경(AWS EKS)에서 Harbor를 배포하고 도커 이미지 백엔드 저장소로 Minio 사용하고자 한다. 또한 고가용성을 가지기 위해 다중 지역별 볼륨을 분산시키고, 백업에 대한 아키텍처를 설계하고 구현할 것이다. 아키텍처를 구현하면서 고민한 부분도 공유할 예정이다.\nHarbor Harbor는 CNCF에서의 레벨이 마지막 레벨인 Graduation(졸업) 단계인 오픈소스 도커 레지스트리이다. 도커 레지스트리 기능으로 안정성, 유용성, 커뮤니티 지원 등 여러 측면에서 충분한 성숙도를 갖춘 것으로 인정받았으며 필요 사례에 맞게 커스터 마이징이 가능하다. 대표적인 기능으로는 멀티 레파티토리 지원, 보안 스캔, 사용자 관리 권한, 이미지 사이클 관리, 마이그레이션, 저장소 복제, 스토리지 백업, 미러링 기능이 있다.\n기능면에서 미러링과 복제가 헷갈릴 수 있는데 목적이 분명 다르다. 미러링은 실시간 동기화를 통해 고가용성과 데이터 안정성을 보장하는 기능인 반면, 복제는 데이터 분산, 백업 및 로드 분산을 위해 주기적으로 또는 수동으로 데이터를 복사하는 방식이다.\nhttps://landscape.cncf.io/guide#provisioning\u0026ndash;container-registry\n아키텍처는 다음과 같다. 중앙 하늘색 네모 부분이 harbor 구성 요소이다. 크게 네트워크단, 서비스단, 데이터 액세스 계층으로 구성된다.\nhttps://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor\n네트워크 계층(Proxy) : Nginx 서버에 의해 형성된 리버스 프록시이다. 하버의 서비스 구성 요소는 모두 이 역방향 프록시 뒤에 있으며 Proxy를 통해 전달된다. 이를 통해 로드 밸런싱, 보안, SSL/TLS 부하 관리, 캐싱 부하 개선, 압축 개선등의 이점을 얻을 수 있다. 서비스 계층(Core) : Harbor Core는 프로젝트 관리, 사용자 인증 및 권한 관리, 시스템 설정 관리, 오케스트레이션 및 이벤트 처리와 같은 다양한 핵심 기능을 담당하는 중심 컴포넌트이다. 세부 기능은 다음과 같다. 프로젝트 관리: Harbor Core는 프로젝트의 생성, 수정, 삭제 및 구성을 관리한다. 프로젝트는 사용자가 컨테이너 이미지와 Helm 차트를 구성 및 관리할 수 있는 논리적 상위 레벨의 단위이다 사용자 인증 및 권한 관리: Harbor Core는 사용자 인증 및 권한 관리를 수행한다. 이를 통해 특정 프로젝트에 대한 접근 권한을 제어할 수 있으며, 사용자는 자신의 권한 범위 내에서 이미지 및 차트를 관리할 수 있다. Harbor는 여러 인증 방식을 지원하며, 외부 인증 시스템과의 통합도 가능하다. 시스템 설정: Harbor Core는 전체 시스템에 대한 다양한 설정을 관리한다. 이에는 공통 설정, 네트워크 설정, 스토리지 설정, 보안 설정, 로깅 및 모니터링 설정 등이 포함된다. 오케스트레이션: Harbor Core는 다른 서비스 컴포넌트와의 상호 작용을 조정한다. 예를 들어 이미지 스캔 요청이 들어오면, 해당 작업을 Job Service에 전달하고, 결과를 사용자에게 전달한다. 이벤트 처리: Harbor Core는 시스템 내부의 이벤트를 처리하며, 필요한 경우 다른 컴포넌트와 상호 작용하여 작업을 수행한다. 이러한 이벤트에는 이미지 및 차트의 생성, 수정, 삭제 등이 포함된다. 데이터 액세스 계층(Data Access Layer) : 3가지로 구성되며 kv-storage(Redis 작업 데이터에 대한 캐싱 스토리지), Local / Remote Storage(도커 레지스트리 저장소) , SQL Database(PostgreSQL 프로젝트, 사용자, 역할, 복제 정책, 태그 보존 정책, 스캐너, 차트 및 이미지와 같은 Harbor 모델의 관련 메타데이터를 저장)을 수행한다. 공식문서에 따르면 Harbor는 기본적으로 상태 비저장 상태이며 파드간 복제가 가능하여 파드 HA를 제공한다. 하지만 스토리지 계층 레벨에서는 사용자가 고가용성 PostgreSQL, 애플리케이션 데이터 및 PVC를 위한 Redis 클러스터 또는 이미지 및 차트를 저장하기 위한 스토리지를 구축해야 한다.\nhttps://goharbor.io/docs/1.10/install-config/harbor-ha-helm/\n이에 따라 스토리지 계층에서의 고가용성을 구성하고자 한다. Harbor 헬름 차트를 확인하면 스토리지 관련 설정부분이 6개이다. 각 스토리지은 다음과 같다.\nRegistry: Docker 레지스트리 스토리지이다. 컨테이너 이미지 및 Helm 차트와 같은 아티팩트를 저장하고 관리한다. ChartMuseum: Helm 차트 레포지토리 스토리지이다. Helm 차트를 저장하고 제공하는 데 사용된다. JobService: JobService의 로그 및 작업 데이터를 저장한다. JobService는 이미지 레플리케이션, 가비지 수집, 스캔 작업 등의 작업을 수행한다. Database: Harbor의 메타데이터를 저장하는 PostgreSQL 데이터베이스이다. 프로젝트, 사용자, 권한 및 구성 정보와 같은 메타데이터를 저장한다 Redis: Harbor에서 사용하는 캐싱 및 메시지 큐 서비스이다. Trivy: 컨테이너 이미지의 취약점 스캔을 수행하는 오픈 소스 스캐너이다. 이 구성 요소의 볼륨은 취약점 데이터베이스 및 스캔 결과를 저장한다. 6개의 스토리지 중 고가용성 구성을 위한 스토리지는 3개이다.\nRegistry : 도커 이미지 저장소 ChartMuseum : helm 차트 저장소 Database : Harbor 메타데이터 저장소 Harbor에서는 스토리지로 block storage, file storage, object stroage 로 설정이 가능하다. 다만, 저장소의 성능 및 활용 특성으로 저장소별 스토리지를 설정해야 한다. 헬름 차트를 확인하면 도커 이미지 저장소는 object 스토리지로 나머지 저장소는 블록 스토리지로 설정 추천하는데 이유는 다음과 같다.\n도커 이미지 저장소를 object storage(예: Amazon S3)로 사용하는 이유:\n확장성: Object storage는 거의 무제한의 저장 용량을 제공하므로, 많은 도커 이미지를 저장할 수 있다. 내구성: Object storage는 데이터를 여러 물리적 위치에 자동으로 복제하므로, 데이터 손실의 위험이 낮다. 비용 효율: 일반적으로 object storage는 용량 당 비용이 낮고, 저장된 데이터에 따라 비용이 증가한다. 기타 저장소를 블록 스토리지(예: AWS EBS)로 저장하는 이유:\n높은 IOPS: EBS는 높은 IOPS(Input/Output Operations Per Second) 성능을 제공하므로, 데이터베이스 작업에 적합하다. 지연 시간 최소화: 블록 스토리지는 데이터베이스 작업에 필요한 빠른 읽기/쓰기 작업이 가능하다. 일관된 성능: EBS는 일관된 성능을 제공하여 데이터베이스 작업의 안정성을 보장한다. 스냅샷 및 백업: EBS 볼륨의 스냅샷을 쉽게 생성할 수 있으며, 데이터베이스 백업 및 복구를 용이하게 한다. 추천 스토리지에 따라 스토리지 구성할 것인데 도커 이미지 저장소로는 Minio 오브젝트 스토리지를 사용할 것이다. 그리고 나머지 저장소는 AWS EBS를 사용할 것이다. 또한 백업을 위해 오픈소스 백업 솔루션인 Velero를 사용할 것이다. 종합하여 AWS 아키텍처를 구성하자면 다음과 같다.\n통신 시나리오에 따라 색깔을 달리 표현했다.이어서 통신 시나리오 및 세부 컴포넌트(MiniO)을 알아보겠다.\nusers (보라선) : Harbor에 접속해서 도커 이미지를 확인할 수 있는 개발자 및 레지스트리 관리자이다. Harbor 도메인을 통해 ELB를 거쳐 harbor ingress 로 통신한다. Storage(파랑선) : 스토리지 볼륨 연계도이다. 설계 목적과 같이 Harbor Registry 저장소는 Minio로 나머지 저장소는 PV(EBS) 로 적재했다. Backup(초록선) : 백업 툴인 Velero를 통해 S3 에 데이터를 백업시킨다. Admin(빨강선) : 인프라 관리자는 Bestion server에 접속하여 쿠버네티스 및 인프라를 관리한다. Minio MinIO 는 AWS의 S3 SDK와 호환되는 오픈소스 Object Storage이다. Minio 모드 중 Distributed Mode는 Minio의 분산 모드로, 여러 서버 또는 노드 간에 데이터를 분산 저장하고 관리할 수 있다. 분산 모드는 높은 가용성, 내구성, 그리고 스케일 아웃 확장성을 제공한다.\n구성시 중요한 점으로 Distributed MinIO 의 디스크이다. 분산 모드에서는 최소 4개의 디스크가 필요하다. 분산스토리지의 데이터 복구 기능으로 Erasure Coding 알고리즘을 채택하기 때문인데 원본 데이터를 여러 조각으로 나누고, 각 조각에 대해 패리티 정보를 생성하여 분산 저장한다. 이에따라 데이터 복구에 N/2 데이터 블록과 N/2 패리티 블록 조건을 만족해야 하며 최소 4개 이상의 디스크를 사용하는 것을 권장하기 때문이다. 이렇게 구성하면 최대 패리티에서 MinIO는 삭제 세트당 최대 절반의 드라이브 손실을 허용할 수 있다.\n배포 설치 환경 : EKS 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스 솔루션 최소 요구 사항 Harbor (CPU 2 core , 4GB, 40 GB) Distributed MinIO ( CPU 1~2 core, 2~4GB, 분산 Minio) Velero ( CPU 1 core, 1GB, 디스크 공간 알아서..) 원활한 테스트를 위해 약 CPU 5 core 이상, 메모리 8기가 이상이 필요하다. 필자의 경우 워크노드를 c5a.2xlarge 로 3개($0.344 *2) 설성하여 테스트를 진행하였다.\nDistributed MinIO 배포 설치는 minio helm 참고하여 구성하였다.\n1 2 3 4 helm repo add minio https://charts.bitnami.com/bitnami helm repo update helm fetch minio/minio --untar --version 12.2.1 구성 옵션별로 수정은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # values-minio.yaml mode: distributed auth: rootUser: admin rootPassword: \u0026#34;admin1234\u0026#34; statefulset: replicaCount: 2 zones: 2 drivesPerNode: 1 provisioning: config: - name: region options: name: ap-northeast-2 ingress: enabled: true hostname: minio.hanhorang.link path: /* annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}, {\u0026#34;HTTPS\u0026#34;:9090}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: {ACM arn 입력} persistence: storageClass: \u0026#34;kops-csi-1-21\u0026#34; 분산 모드에서 인스턴스 설정은 statuefulset에서 설정한다. replicaCount: 각 zones당 실행되는 실행되는 파드의 수를 설정한다. zones: AWS의 가용영역이라 생각하면 된다. 분산스토리지로 사용할 영역 개수를 지정한다. drivesPerNode: 각 워커 노드에 연결된 디스크 수를 1개로 설정한다. 이 설정에 따라 각 MinIO 파드는 하나의 디스크를 사용하게 된다. 배포\n1 2 kubectl create ns minio helm install minio minio/minio -f values-minio.yaml -n minio --version 12.2.1 배포 완료후, 도메인으로 접속하여 확인해보자. 도메인은 헬름 차트의 ingress.hostname에 입력한 값이다.\n어드민 계정은 차트에서 admin / admin1234 로 설정되어 있다. 로그인을 하자.\n로그인이 완료되면 다음과 같은 화면을 확인할 수 있다.\nHarbor 연동을 위하여 접속한 Minio 에서 버킷 생성과 Access Key를 발급받아야 한다. 버킷 이름과 Access keys는 Harbor 구성 헬름 차트에서 필요하다.\n버킷 생성\n초기 화면 Object Brower 에서 버킷 생성이 가능하다.\nAccess Key 발급\nAccess Keys 에서 발급받자.\nCreate 버튼 잊지말고 클릭하자.\nAccess Key 발급 후 MINIO 동작 권한을 등록해야 한다. 생성한 키를 클릭하면 정책 입력 칸이 나온다. 아래 정책을 입력하도록 하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;admin:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::*\u0026#34; ] } ] } Harbor 배포 Harbor 구성도 마찬가지로 helm로 배포할 것이다. 먼저 헬름 차트를 가져오겠다.\n1 2 3 4 helm repo add harbor https://helm.goharbor.io helm repo update # 차트 얻기 helm show values **harbor/harbor** \u0026gt; values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #values-harbor.yaml # network 설정 expose: tls: certSource: none ingress: hosts: core: harbor.hanhorang.link notary: notary.hanhorang.link controller: alb className: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: {ACM arn 입력} externalURL: https://harbor.hanhorang.link # Storage 설정 persistence: persistentVolumeClaim: registry: storageClass: \u0026#34;kops-csi-1-21\u0026#34; chartmuseum: storageClass: \u0026#34;kops-csi-1-21\u0026#34; database: storageClass: \u0026#34;kops-csi-1-21\u0026#34; imageChartStorage: disableredirect: true type: s3 s3: region: ap-northeast-2 bucket: registry accesskey: \u0026lt;minio-access-key\u0026gt; secretkey: \u0026lt;minio-secret-key\u0026gt; regionendpoint: http://minio.minio.svc.cluster.local:9000 # HA core: replicas: 3 스토리지 연동은 persistence에서 진행한다. registry 부분 연동은 imageChartStorage에서 설정한다. imageChartStorage.s3 에서 minio 에서 발급받은 키를 입력한다. imageChartStorage.s3.regionpoint 는 minio 의 DNS 정보를 입력했다. 인그래스로 설정이 가능하나, 다음과 같은 에러로 DNS 로 설정하였다. 네트워크 설정으로 오류 발생시 다음의 메세지를 확인하자. 🧐 minio 네트워크 설정 관련 트러블슈팅\nregionendpoint 설정에 따라 에러가 발생하여 정리한다.\n1 kubectl logs harbor-core-5b7864c575-rnfm9 -n harbor regionendpoint: minio.hanhorang.link 의 경우 도메인은 minio의 ALB 도메인 주소이다. Minio 는 S3와 호환되는 SDK를 가지고 있다. 아래 로그 메세지의 경우 포트를 지정하지 않아 생긴 오류다.\n1 2 3 4 5 2023-03-21T16:59:40Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;2236a0c4-55cd-4e5c-989d-1b9f941efb25\u0026#34;]: failed to populate properties for project library, error: get chart count of project 1 failed: http error: code 500, message InvalidArgument: S3 API Requests must be made to API port. status code: 400, request id: , host id: 2023-03-21T16:59:40Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;2236a0c4-55cd-4e5c-989d-1b9f941efb25\u0026#34;]: failed to populate properties for project test, error: get chart count of project 2 failed: http error: code 500, message InvalidArgument: S3 API Requests must be made to API port. status code: 400, request id: , host id: 2023-03-21T16:59:42Z [ERROR] [/lib/http/error.go:56]: {\u0026#34;errors\u0026#34;:[{\u0026#34;code\u0026#34;:\u0026#34;UNKNOWN\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;get chart count of project 2 failed: http error: code 500, message InvalidArgument: S3 API Requests must be made to API port.\\n\\tstatus code: 400, request id: , host id: \u0026#34;}]} regionendpoint: minio.hanhorang.link:9000 경우 포트 9000을 추가하니 Timeout이 발생한다. harbor ALB에서 다시 minio ALB로 거치는 과정에서 시간이 초과된 것으로 예상된다.\n1 2 3 4 5 2023-03-21T16:55:40Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;a0aed46e-4d11-4459-af04-2efa9f94cbf3\u0026#34;]: failed to populate properties for project library, error: get chart count of project 1 failed: get content failed: send request GET /library/index.yaml failed: Get \u0026#34;http://harbor-chartmuseum/library/index.yaml\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) 2023-03-21T16:55:40Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;a0aed46e-4d11-4459-af04-2efa9f94cbf3\u0026#34;]: failed to populate properties for project test, error: get chart count of project 2 failed: get content failed: send request GET /test/index.yaml failed: Get \u0026#34;http://harbor-chartmuseum/test/index.yaml\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) 2023-03-21T16:55:56Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;8e0c7248-9425-4862-b066-9b4b29151105\u0026#34;]: failed to populate properties for project test, error: get chart count of project 2 failed: get content failed: send request GET /test/index.yaml failed: Get \u0026#34;http://harbor-chartmuseum/test/index.yaml\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) 2023-03-21T16:55:56Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;8e0c7248-9425-4862-b066-9b4b29151105\u0026#34;]: failed to populate properties for project library, error: get chart count of project 1 failed: get content failed: send request GET /library/index.yaml failed: Get \u0026#34;http://harbor-chartmuseum/library/index.yaml\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) 2023-03-21T16:56:29Z [ERROR] [/lib/http/error.go:56]: {\u0026#34;errors\u0026#34;:[{\u0026#34;code\u0026#34;:\u0026#34;UNKNOWN\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;get chart count of project 2 failed: get content failed: send request GET /test/index.yaml failed: Get \\\u0026#34;http://harbor-chartmuseum/test/index.yaml\\\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)\u0026#34;}]} regionendpoint: minio.minio.svc.cluster.local:9000 경우\n도메인 연결의 최소로 하기 위해 CoreDNS를 사용했다. 그러나 HTTP 설정으로 에러가 발생했다.\n1 2023-03-21T16:35:45Z [ERROR] [/lib/http/error.go:56]: {\u0026#34;errors\u0026#34;:[{\u0026#34;code\u0026#34;:\u0026#34;UNKNOWN\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;get chart count of project 2 failed: http error: code 500, message RequestError: send request failed\\ncaused by: Get \\\u0026#34;https://minio.minio.svc.cluster.local:9000/minio-test?prefix=test\\\u0026#34;: http: server gave HTTP response to HTTPS client\u0026#34;}]} regionendpoint: http://minio.hanhorang.link:9000\nHTTP를 붙이니 정상적으로 연동이 확인된다. 쿠버네티스 내부에서 연결하는 것이라 HTTPS 트래픽 부하를 최소화하는 목적으로 HTTP를 사용하여 설정했다.\n1 2 3 kubectl create ns harbor helm install harbor harbor/harbor -f values-harbor.yaml --namespace harbor --version 1.11.0 배포 확인 헬름차트에서 입력 harbor 도메인으로 접근하면 로그인 화면이 나온다. 초기 admin 유저의 접속 정보는 admin / Harbor12345 이다. 로그인하면 레지스트리 정보가 나온다.\nNew Project 버튼을 눌러 레지스트리 프로젝트에 버킷을 생성하자\n연동 테스트 임의의 도커 이미지 PULL \u0026amp; PUSH 하여 Harbor 레지스트리, Minio 버킷에 데이터가 저장되는 지 확인해보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 docker pull nginx \u0026amp;\u0026amp; docker pull busybox \u0026amp;\u0026amp; docker images docker tag busybox harbor.$KOPS_CLUSTER_NAME/test/busybox:0.1 echo \u0026#39;Harbor12345\u0026#39; \u0026gt; harborpw.txt cat harborpw.txt | docker login harbor.$KOPS_CLUSTER_NAME -u admin --password-stdin WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded docker push harbor.$KOPS_CLUSTER_NAME/test/busybox:0.1 The push refers to repository [harbor.hanhorang.link/test/busybox] baacf561cfff: Pushed 0.1: digest: sha256:acaddd9ed544f7baf3373064064a51250b14cfe3ec604d65765a53da5958e5f5 size: 528 Harbor에 이미지가 저장되고 이어서 minio 버킷에 하버 데이터가 저장된 것을 확인할 수 있다!\n이어서 이번 블로그 글에서는 Harbor와 minio를 통해 EKS에서 고가용성 Private Docker Registry(Harbor) 구축을 하였다. 아키텍처 설계를 하면서 스토리지 특성에 대해 딥하게 다룰 수 있는 계기가 되었고, CNCF의 졸업 레벨의 시스템(Harbor)의 아키텍처를 구체적으로 분석할 수 있었다. 느끼는 거지만 간단하면서도 각 컴포넌트가 세부적으로 분산되어 처리된다는 점과 스토리지 연동 부분에서 사례 참고가 되었다.\n아키텍처 구축이 끝이 아니다. 운영 레벨의 기능들이 남았다. 다음 글에서는 Velero를 통한 백업과 LDAP 서버와의 연동, Harbor의 기능들에 대해 살펴보겠다.\n","date":"Apr 21","permalink":"https://HanHoRang31.github.io/post/tech-private-docker/","tags":["eks","cloud","AWS","kubernetes","harbor","minio"],"title":"[테크 따라잡기] EKS에서 고가용성 Private Docker Registry(Harbor) 구축하기"},{"categories":null,"contents":"전글 Loki 최신 버전 설치하기에서 인덱스 DB를 Dynamodb로 연동하지 못했다. 미련이 남아 스터디가 끝난 이후에도 여러 테스트를 해보았고, 마침내 연동을 성공하여 관련 경험을 공유한다.\n먼저, Loki에서 DynamoDB로 연동해야 하는 목적부터 알고 가자. 배경 지식 설명을 위해 Loki 의 지원 스토리지를 다시 확인하겠다.\nhttps://grafana.com/docs/loki/latest/operations/storage/\n스토리지 연동 목록을 살펴보면 Loki 2.0 이상에서 싱글 스토어(boltdb-shipper)가 추가되었고 연동으로 추천하고 있다. (기본 스토리지도 싱글 스토어이다.) 이유를 살펴보니 비용 감소인데 로컬에서 인덱스를 저장하기 때문에 외부 스토리지에 대한 종속성이 줄기 때문이라 한다. 다만, 원격 저장소로 백업하고 복원하는데 추가 작업이 필요하다. (앞 글에서는 S3로 연동했으나 시간 텀이 15분 정도 있었음)\n비용은 발생하지만, DyanmoDB 연동 목적은 다음과 같다.\n복원력 : 실시간으로 데이터가 적재되어 로키 파드가 죽어도 데이터가 유지된다. 확장성 : 멀티 클러스터에서의 실시간 로깅 확인 (로컬인 경우 S3에 적재가 가능하지만 시간 텀이 있다.) 결론적으로, 소규모 프로젝트나 초기 설정에 초점을 맞추는 경우 BoltDB \u0026amp; S3 옵션을 고려할 수 있다. 반면, 대규모 프로젝트나 장기적인 운영에 초점을 맞추는 경우 S3 \u0026amp; DynamoDB 옵션이 더 적합하다.\n연동 트러블슈팅 DynamoDB 연동을 위해 해결했던 문제점은 두가지였다. 항목별로 살펴보겠다.\nHelm Value Override 문제 Loki 스토리지 연동 설정 문제 Helm Value Override 문제 지난 글에서 스토리지 설정을 S3 및 기타 스토리지로 설정했음에도 기본 스토리지 (boltdb \u0026amp; snipper) 가 설정되어 관련 문제를 확인하였다. helm template 명령어를 통해서 스토리지 차트 랜더링을 확인하니 Values 오버라이드 값이 아닌 기본 값(boltdb \u0026amp; snipper)이 입력되어 있는 것을 확인할 수 있다.\n1 helm template loki grafana/loki -f loki.yaml -n loki --version 4.8.0 \u0026gt; result.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # Source: loki/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: loki labels: helm.sh/chart: loki-4.8.0 app.kubernetes.io/name: loki app.kubernetes.io/instance: loki app.kubernetes.io/version: \u0026#34;2.7.3\u0026#34; app.kubernetes.io/managed-by: Helm data: # override 적용 안된다! config.yaml: | auth_enabled: false common: compactor_address: \u0026#39;loki-read\u0026#39; path_prefix: /var/loki replication_factor: 3 storage: s3: bucketnames: chunks insecure: false s3forcepathstyle: false limits_config: enforce_metric_name: false max_cache_freshness_per_query: 10m reject_old_samples: true reject_old_samples_max_age: 168h split_queries_by_interval: 15m memberlist: join_members: - loki-memberlist query_range: align_queries_with_step: true ruler: storage: s3: bucketnames: ruler insecure: false s3forcepathstyle: false type: s3 runtime_config: file: /etc/loki/runtime-config/runtime-config.yaml schema_config: configs: - from: \u0026#34;2022-01-11\u0026#34; index: period: 24h prefix: loki_index_ object_store: s3 schema: v12 store: boltdb-shipper server: grpc_listen_port: 9095 http_listen_port: 3100 storage_config: hedging: at: 250ms max_per_second: 20 up_to: 3 table_manager: retention_deletes_enabled: false retention_period: 0 --- 스토리지 연동 부분인 data.config.yaml.schema_config 와 data.config.yaml.storage_config 에 기본 값이 적용되어 있는 것을 확인할 수 있다. 원인은 Loki 헬름 차트에서 if 문법과 override 값에서 생긴 랜더링 문제로 확인된다. 관리성 문제로 overrider 파일의 값을 적용시키려 했지만, 적용되지 않았다. (헬름 차트에 정확한 순서 및 문법을 검사했음에도..)\n우회적 해결 방법으로 기본 차트 vaules.yaml 에 값을 직접 입력하였다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # vaule.yaml Loki: ... config: | .... table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 chunk_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 ... # vaule.yaml Loki: ... # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas schemaConfig: configs: - from: \u0026#34;2022-01-11\u0026#34; store: aws object_store: s3 schema: v12 index: prefix: loki_ # -- Check https://grafana.com/docs/loki/latest/configuration/#ruler for more info on configuring ruler rulerConfig: {} # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig` structuredConfig: {} # -- Additional query scheduler config query_scheduler: {} # -- Additional storage config storage_config: hedging: at: \u0026#34;250ms\u0026#34; max_per_second: 20 up_to: 3 aws: s3: s3.ap-northeast-2.amazonaws.com bucketnames: han-loki region: ap-northeast-2 access_key_id: \u0026lt;access-key\u0026gt; secret_access_key: \u0026lt;aws-secret-key\u0026gt; dynamodb: dynamodb_url: dynamodb.ap-northeast-2.amazonaws.com ... config: | .... table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 chunk_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 ... ## Loki 스토리지 연동 설정 문제 DynamoDB 스토리지로 선택이 되나, loki-read Pod에서 다음의 연동 오류가 발생한다. ```bash level=info ts=2023-04-16T13:31:40.414190216Z caller=http.go:279 org_id=fake msg=\u0026#34;ended tailing logs\u0026#34; tenant=fake selectors=\u0026#34;{stream=\\\u0026#34;stdout\\\u0026#34;,pod=\\\u0026#34;loki-canary-c8s22\\\u0026#34;}\u0026#34; level=info ts=2023-04-16T13:31:50.419151688Z caller=http.go:276 org_id=fake msg=\u0026#34;starting to tail logs\u0026#34; tenant=fake selectors=\u0026#34;{stream=\\\u0026#34;stdout\\\u0026#34;,pod=\\\u0026#34;loki-canary-c8s22\\\u0026#34;}\u0026#34; level=error ts=2023-04-16T13:31:50.422289139Z caller=series_index_store.go:583 org_id=fake msg=\u0026#34;error querying storage\u0026#34; err=\u0026#34;QueryPages error: table=loki_: MissingRegion: could not find region configuration\u0026#34; level=error ts=2023-04-16T13:31:50.422642663Z caller=series_index_store.go:583 org_id=fake msg=\u0026#34;error querying storage\u0026#34; err=\u0026#34;QueryPages error: table=loki_: MissingRegion: could not find region configuration\u0026#34; level=info ts=2023-04-16T13:31:50.422757665Z caller=http.go:279 org_id=fake msg=\u0026#34;ended tailing logs\u0026#34; tenant=fake selectors=\u0026#34;{stream=\\\u0026#34;stdout\\\u0026#34;,pod=\\\u0026#34;loki-canary-c8s22\\\u0026#34;}\u0026#34; level=info ts=2023-04-16T13:32:00.112792136Z caller=http.go:276 org_id=fake msg=\u0026#34;starting to tail logs\u0026#34; tenant=fake selectors=\u0026#34;{stream=\\\u0026#34;stdout\\\u0026#34;,pod=\\\u0026#34;loki-canary-q52t8\\\u0026#34;}\u0026#34; level=error ts=2023-04-16T13:32:00.11538504Z caller=series_index_store.go:583 org_id=fake msg=\u0026#34;error querying storage\u0026#34; err=\u0026#34;QueryPages error: table=loki_: MissingRegion: could not find region configuration\u0026#34; level=error ts=2023-04-16T13:32:00.115479121Z caller=series_index_store.go:583 org_id=fake msg=\u0026#34;error querying storage\u0026#34; err=\u0026#34;QueryPages error: table=loki_: MissingRegion: could not find region configuration\u0026#34; 해당 이슈는 Loki 깃허브에서 확인이 가능했으나 답변이 없었다.\nhttps://github.com/grafana/loki/issues/1957\n원인은 Loki DynamoDB 스토리지 설정으로 생긴 문제였다. 깃 이슈를 확인하면 Loki 공식문서에서 제공하는 파라미터를 통해 설정한 것을 확인할 수 있었는데, 그대로 따라하면 오류가 발생한다.\nhttps://grafana.com/docs/loki/latest/configuration/#aws_storage_config\n필자의 경우 에러 메세지를 기반으로 소스 코드를 분석했다. 결론적으로는 DynamoDB Struct에 Region 설정값이 없어서 생긴 오류였다. 코드 분석 과정은 다음과 같다.\n에러 메세지을 찾아 확인하니 ValidateEndpointHandler 함수에서 발생하는 에러였다. 해당 함수는 AWS SDK의 일부로 AWS 서비스 요청에 대한 엔드포인트와 리전 정보를 검증하는 함수이다.\n해당 함수의 호출은 AWS 클라이언트를 생성하고, 이를 사용하여 서비스 요청을 시작할 때 발생한다. 각 스토리지 S3, DyanmoDB 클라이언트 구조체는 다음과 같은데 S3에는 region이 있지만, Dynamodb 에서는 해당 변수가 없었다.\n아래와 같이 DynamoDB는 변수 dynamdb.dynamodb_url 이 Endpoint 및 시크릿 키를 입력받도록 되어 있다. 이는 Loki 버전 업데이트 전(Version 2.0 이하)의 변수 입력과 동일하다. 필자가 추정하건데 차트와 연동하여 스토리지 변수 설정 부분은 S3, GCS, Azure 만 업데이트되어 있는 것 같다.\n오류 해결은 간단하다. Loki 스토리지 예제를 참고하여 DynamoDB_URL에 Region 과 AWS 키 값 형식을 확인하며 차트에 반영하면 된다. Loki 스토리지 예제는 다음과 같이 확인할 수 있었다.\nhttps://grafana.com/docs/loki/latest/configuration/examples/\n이를 반영하여 최종적으로 수정한 차트는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # values.yaml Loki: ... ... config: | .... table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 chunk_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 ... # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas schemaConfig: configs: - from: \u0026#34;2022-01-11\u0026#34; store: aws object_store: s3 schema: v12 index: prefix: loki_ ... # -- Check https://grafana.com/docs/loki/latest/configuration/#ruler for more info on configuring ruler rulerConfig: {} # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig` structuredConfig: {} # -- Additional query scheduler config query_scheduler: {} # -- Additional storage config storage_config: hedging: at: \u0026#34;250ms\u0026#34; max_per_second: 20 up_to: 3 aws: s3: s3://\u0026lt;AWS-ACCESS-KEY\u0026gt;:\u0026lt;AWS-SECRET-KEY\u0026gt;@ap-northeast-2/han-loki dynamodb: dynamodb_url: dynamodb:///\u0026lt;AWS-ACCESS-KEY\u0026gt;:\u0026lt;AWS-SECRET-KEY\u0026gt;@ap-northeast-2 앞선 깃 이슈에서 dynamodb_url에 inmemory:/// 로 변수를 설정하는 것이 있었는데 이는 가상의 인메모리로 구현시 사용된다고 한다. 연동시 헷갈리지 말자 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # values-loki.yaml tableManager: enabled: true test: enabled: true monitoring: lokiCanary: enabled: true selfMonitoring: enabled: true loki: auth_enabled: false Loki Config 부분을 제외하고는 오버라이드가 가능하여 나머지 설정 부분은 다음과 같이 설정하였다. 배포 확인 차트 배포를 통해 연동을 해보자.\n1 2 3 4 kubectl create ns loki helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm install loki grafana/loki -f values.yaml -f values-loki.yaml -n loki --version 4.8.0 배포 후 AWS 콘솔에서 확인하면 S3에는 청크가 DynamoDB에는 인덱스가 저장되는 것을 확인할 수 있다.\nS3 Chunk 확인\nDynamoDB 인덱스 조회 확인\n성공이다..! 돌고 돌아 공식 문서에서의 예제로 연동에 성공하였다. 헬름 차트 랜더링과ㅊ 파라미터 변수의 예제를 꼭 확인하도록 하자!\n운영시 고려사항 배포 과정에서 고려사항이 있어 추가로 남겨둔다.\n스토리지 사용 비용 첫 쨰로 비용이다. DynamoDB 연동시 기본 읽기용량이 100, 쓰기용량이 25로 설정되어 있다. 비용 계산를 확인 하자면 다음과 같다.\n한국 DynamoDB 비용\n쓰기 요청 유닛(WRU): 1,000,000 건당 $1.3556 읽기 요청 유닛(RRU): 1,000,000 건당 $0.271 기본 읽기용량이 100, 쓰기용량이 25로 설정된 경우, 용량 단위에 대한 실제 요청 수를 알아야 한다.\n예를 들어, 시간당 10,000건의 쓰기 요청과 50,000건의 읽기 요청이 있다고 가정하겠다.\n쓰기 요청 비용: 10,000 WRU x ($1.3556 / 1,000,000) = $0.013556 읽기 요청 비용: 50,000 RRU x ($0.271 / 1,000,000) = $0.01355 총 비용은 $0.013556 + $0.01355 = $0.027106 (1h) 이다.\n운영 환경에서 배포시 시스템 규모를 파악하여 쓰기 및 읽기 요청에 따라 용량을 설정하도록 하자. 용량 설정은 위 차트에서의 table_manager 에서 설정이 가능하다. 필자는 5로 설정했었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # values.yaml Loki: ... ... config: | .... table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 chunk_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 ... ","date":"Apr 14","permalink":"https://HanHoRang31.github.io/post/loki-2-8-0/","tags":["KANS","kops","cloud","AWS","kubernetes","PLG","loki","promtail","Observability"],"title":"Loki 최신버전(V2.8.0) DyanmoDB 연동하기"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 5주차 시간에는 모임장님께서 쿠버네티스 보안을 주제로 학습 내용을 공유해 주셨다. 이번 블로그 글에서는 쿠버네티스 보안에 대해 스터디한 내용을 공유하고자 한다.\nKubernetes 4C Layer 쿠버네티스 공식문서에 따르면 쿠버네티스 보안은 4계층(클라우드/ 클러스터 / 컨테이너 / 코드)으로 구성되며, 각 계층에 대해 보안 관점이 필요하다고 한다.\nhttps://kubernetes.io/docs/concepts/security/overview/\nCloud, Infra : 클라우드 계층에서는 쿠버네티스 클러스터가 실행되는 기반 인프라에 초점을 맞춘다. 이 계층에서는 가상 머신, 네트워크, 스토리지 및 기타 자원에 대한 보안을 강화하고, 클라우드 제공 업체의 보안 도구 및 기능을 활용하는 것으로 초점이 맞춰져 있다. 인프라 보호 대상과 클라우드 제공 업체(AWS) 의 제공 보안은 다음의 표를 참고하자. 보호 대상 AWS EKS에서의 보안 기능 API 서버에 대한 네트워크 액세스 클러스터 구축시 액세스 지점에 대해 Elastic Load Balancer를 구성한다. 이를 통해 API 서버에 대한 액세스를 VPC 내부로 제한하거나 인터넷 게이트웨이를 통해 인터넷에서도 접근이 가능하도록 설정할 수 있다. 또한, AWS Identity and Access Management (IAM)을 사용하여 사용자와 역할에 대한 접근 제어를 구성할 수 있다. 노드에 대한 네트워크 액세스 VPC 서비스를 이용하여 노드에 대한 액세스를 VPC 내부로 제한하거나 인터넷 게이트웨이를 통해 인터넷에서도 접근이 가능하도록 설정할 수 있다. 또한 보안 그룹을 통해 IP 주소 범위에 대한 인바운드 및 아운바운드 트래픽을 제어할 수 있다. 클라우드 제공 업체 API 에 대한 쿠버네티스 액세스 IAM 권한의 접근 키를 다룬다. 접근 키가 탈취되면 해당 리소스에 대한 제어가 탈취된다. 공식 문서에서는 최소 권한 원칙을 액세스 권한을 부여할 것을 권고한다. etcd에 대한 액세스 etcd 데이터베이스가 EKS 완전 관리형 컨트롤 플레인 내부에 숨겨져 있으며, 사용자는 etcd에 직접 액세스할 수 없다. etcd 암호화 기본적으로 EKS 클러스터를 생성할 때, etcd 데이터를 암호화하는 옵션을 선택할 수 있다. 이렇게 설정하면, 클러스터의 etcd 데이터는 AWS Key Management Service (KMS)에서 제공하는 고객 관리형 키 (CMK)를 사용하여 암호화되어 기밀성이 보장된다. Cluster: 클러스터 계층에서는 쿠버네티스 클러스터 자체의 보안에 중점을 둔다. 클러스터 상의 중요한 서비스 A와 보안이 취약한 서비스 B에 대해 격리를 위한 계층이라고 보면 된다. 쿠버네티스에서는 격리에 방법으로 다양한 방법을 제안한다. 대표적으로 RBAC 인증, 네트워크 정책, 파드 보안 표준, 인그래스용 TLS 등이 있다. 항목 설명 RBAC 인증 역할(Role) 및 클러스터 역할(ClusterRole)을 사용하여 쿠버네티스 사용자와 서비스 계정에 권한을 부여하는 Role-Based Access Control 방식이다. 이를 통해 세분화된 권한 제어로 클러스터의 보안을 강화할 수 있다. 네트워크 정책 쿠버네티스 네트워크 정책은 특정 파드, 네임스페이스, 또는 IP 범위와 같은 소스로부터 들어오거나 나가는 트래픽을 허용하거나 차단하는 규칙을 정의한다. 이를 통해 네트워크 보안을 강화하고, 민감한 데이터를 처리하는 파드에 대한 접근을 제한할 수 있다. 파드 보안 표준 파드 보안 표준은 컨테이너와 파드가 안전하게 실행되도록 하는데 도움이 되는 일련의 보안 지침 및 구성이다. 예를 들어, 보안 컨텍스트(Security Context), 네트워크 폴리시, 리소스 제한, 파드 안티-어피니티 등을 사용하여 파드의 보안을 강화할 수 있다. 인그래스용 TLS 쿠버네티스 인그래스를 사용하여 클러스터 외부에서 내부 서비스로의 요청을 중앙 집중식으로 관리할 때, TLS(Transport Layer Security)를 사용하여 클라이언트와 서버 간 통신을 암호화하여 데이터를 보호할 수 있다. 인증서와 개인 키를 제공하고 호스트 이름과 인증서의 일치 여부를 확인해야 한다. Container: 컨테이너 계층에서는 실행되는 워크로드에 직접적으로 영향을 주는 컨테이너에 초점을 맞춘다. 이 계층에서는 컨테이너 이미지 보안, 리소스 격리, 시크릿 관리 및 네트워크 정책을 포함된다. 여기에서는 보안 컨텍스트, 네트워크 폴리시, 컨테이너 런타임의 보안 기능(이미지 스캔) 등을 사용하여 컨테이너의 보안을 강화한다. 항목 설명 보안 컨텍스트 (Security Context) 쿠버네티스에서 컨테이너 또는 파드에 적용되는 보안 설정을 정의하는데 사용된다. 보안 컨텍스트를 사용하여 컨테이너의 파일 시스템에 대한 액세스 권한, 프로세스 ID, 사용자 ID, 그룹 ID 등을 제어할 수 있다. 이를 통해 컨테이너와 파드가 안전하게 실행되도록 할 수 있으며, 호스트 시스템과 다른 컨테이너로부터 분리되어 보안을 강화할 수 있다. 컨테이너 런타임의 보안 기능 컨테이너 런타임(예: Docker, containerd, CRI-O 등)은 컨테이너를 실행하고 관리하는데 사용되는 소프트웨어이다. 컨테이너 런타임은 다양한 보안 기능을 제공하여 컨테이너의 보안을 강화할 수 있다. 예를 들어, 컨테이너 런타임은 네임스페이스를 사용하여 컨테이너 프로세스를 격리할 수 있으며, cgroups을 사용하여 리소스 사용량을 제한하고, seccomp, AppArmor, SELinux와 같은 보안 프로파일을 적용하여 컨테이너의 시스템 호출을 제한할 수 있다. Code: 코드 계층에서는 애플리케이션의 소스 코드 및 구성에 초점을 맞춘다. 이 계층에서는 애플리케이션의 취약점 및 보안 결함을 찾고 수정하여 워크로드의 보안을 향상시켜야 한다. 방법으로는 TLS 액세스, 통신 포트 제한, 종속성 보안, 정적 및 동적 소스 코드 분석, 의존성 스캔 및 안전한 코딩 기법 등이 있다. 항목 설명 통신 포트 제한 통신 포트 제한은 서버, 컨테이너, 애플리케이션 등이 사용하는 네트워크 포트를 제한하여 보안을 강화하는 방법이다. 불필요한 포트를 차단하고, 필요한 포트에 대해서만 허용하면 악의적인 공격자가 시스템에 액세스하는 것을 방지할 수 있다. 종속성 보안 종속성 보안은 애플리케이션에서 사용하는 외부 라이브러리 및 패키지에 대한 보안을 관리하는 방법이다. 취약한 종속성을 사용하면 시스템이 공격에 노출될 수 있다. 따라서, 종속성을 최신 상태로 유지하고, 보안 취약점이 발견될 경우 적절한 조치를 취하는 것이 중요하다. 정적 및 동적 소스 코드 분석 정적 소스 코드 분석은 코드를 실행하지 않고 소스 코드를 검사하여 보안 취약점을 찾는 방법이다. 동적 소스 코드 분석은 애플리케이션을 실행하면서 코드를 분석하여 보안 취약점을 찾는 방법이다. 이러한 분석 방법들을 사용하여 애플리케이션 코드의 보안 취약점을 발견하고 수정할 수 있다. 의존성 스캔 및 안전한 코딩 기법 의존성 스캔은 애플리케이션의 종속성에 대한 보안 취약점을 찾기 위한 도구를 사용하는 것이다. 안전한 코딩 기법은 개발자가 코드를 작성할 때 고려해야 하는 보안 지침 및 원칙이다. 이러한 기법을 사용하면 애플리케이션의 보안을 강화하고, 취약점을 줄일 수 있다. 보안적으로 신경써야할 요소가 많다. 위의 설명한 보안 기능들을 전부 다루면 좋겠지만 내용이 방대하다. 이번 블로그 글에서는 보안 툴인 kubescape를 이용하여 앞서 4C에서 Cluster, Container, Code 계층의 보안 검증 과정을 다뤄보겠다.\nKubescape 쿠버네티스 클러스터 보안 설정을 평가하고 검증하는 오픈 소스이다. NSA와 MITRE의 Kubernetes Hardening Guidance를 기반으로 작동하며 웹 대시보드를 통해 검증 및 취약점 점검을 할 수 있다. 또한, 도커 레지스트리, 이미지 취약점 스캔이 가능하다. 23년 4월 기준으로 업데이트가 계속 진행 중이며 공식 문서 또한 정리가 잘 되어 있다.\n그렇지만 제한적인 요소도 존재한다. 클러스터를 ARMO 웹 대시보드로 연결해야한다는 점으로 온프레미스에서 도입시 고려해야 한다. 또한 프리티어 기준 워크 노드가 10개로 제한된다.\n보안 기능을 전부 사용하려면 Operator 설치가 동반된다. 아키텍처는 다음과 같이 구성된다.\nhttps://github.com/kubescape/helm-charts/blob/master/charts/kubescape-cloud-operator/README.md\nMaster Gateway: ARMO 백엔드에서 실행 중인 마스터 게이트웨이이며 사용자가 등록한 모든 게이트웨이에 메시지를 브로드캐스트하여 모든 클러스터 게이트웨이에 런타임 작업을 전달한다. In-cluster Gateway: 클러스터 내에서 다른 구성 요소와 통신하기 위해 마스터 게이트웨이와 연결되며, 웹소켓(Websocket)을 사용하여 등록된다. 브로드캐스트 메세지를 전달받아 다른 컴포넌트에 전달한다. Operator : 트리거 엔진으로 게이트웨이에서 잔달받은 작업을 실행하거나 스케줄링하는 역할을 담당한다. Kubevuln : 컨테이너 이미지 취약점을 스캔하는 컴포넌트이다. 취약점 스캔은 grype을 통해 진행한다. Kubescape : 클러스터 내부 검증을 스캔하는 컴포넌트이다. Kollector : kubernetes API Server와 통신하여 클러스터 정보와 변경 정보를 확인하며 정보를 백엔드 CloudEndpoint로 전달한다. 설치 배포 환경 : kops 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스\n먼저 https://cloud.armosec.io/ 회원가입이 필요하다. 회원가입 이후 회원 ID 값에 맞게 helm 설치 명령어가 나온다. 복사하여 클러스터에 설치하자.\n설치 이후 Verfiy installation 버튼을 누르면 아래와 같이 연결이 안된다고 나오나 kops 클러스터에서 설치했을 때의 버그같다.\n아래와 같이 로그 확인 및 대시보드를 확인하면 정상적으로 등록이 되어 있다.\n1 kubectl -n kubescape logs -f $(kubectl -n kubescape get pods | grep kollector | awk \u0026#39;{print $1}\u0026#39;) 기능 확인 대시보드를 접속하면 왼쪽의 메뉴를 통해서 보안 검증이 가능하다.\nCompliance : 쿠버네티스 준수 정책 검증 Vulnerabilities : 컨테이너 파드 취약점 점검 RBAC Visualizer : RBAC 시각화 Repository Scanning : 레파지토리 스캐닝 Registry Scanning : 도커 레지스트리 스캐닝 Compliance 쿠버네티스 모범 사례를 체계화하여 수백 개의 항목들을 통해 클러스터 검증시켜주는 기능이다. 검증 항목은 MITRA 와 NSA 에서 제시하는 보안 가이드라인이며 웹 대시보드에서 프레임워크별 제어가 가능하다.\n또한, 보안이 필요한 항목에는 fix버튼을 통해 구성적으로 확인이 가능하다.\n아래는 필자 클러스터 환경에서 high 레벨 항목이며 몇 가지 항목 원인은 다음과 같다.\nC-0057(Privileged container) : 호스트 시스템의 모든 기능을 포함하는 컨테이너가 있어서 발생한 항목이다. 파드 구성 중 spec.container.securityContext.privileged == true 일 때 발생한다. 필자의 경우 볼륨 관리 파드(ebs-csi-node) 에서 발생했다. C-0045(Writable hostPath mount) : 쓰기 가능한 hostPath 볼륨으로 컨테이너를 생성했을 때 발생하는 알람이다. 설명에서는 이를 통해 호스트 볼륨의 정보를 얻을 수 있다 한다. 파드 구성 중 mount.readOnly == false 이 됐을 때 발생한다. C-0015 (kubernetes secret list) : 쿠버네티스 사용자가 시크릿에 대해 접근이 가능할 때 발생하는 알람이다. C-0041(HostNetwork access) : 호스트 네트워크에 연결할 경우 발생하는 알람이다. 필자의 경우 external-dns 파드에서 발생했다. 항목을 살펴보니 대부분 add-on 파드들에 대해 발생한 경고이다. 이러한 경우 ignore 로 알람 무시가 가능하다.\n각 모범 사례 기준이 엄격하고 서드 파티 레벨의 애플리케이션의 구성 정보 확인시 검증 과정에서 유용하게 사용할 수 있을 것 같다.\nVulnerabilities 취약점 점검으로 배포 컨테이너들에 대해 취약점 점검을 진행한다. 취약점 점검은 grype 엔진을 통해 진행되며 CVE(Common Vulnerabilities and Exposures, 미국 NSA, CISA에서 제공하는 보안 가이드라인) 식별자로 검사가 진행된다.\n위와 마찬가지로 필자의 클러스터에서 CRITICAL한 레벨의 취약점을 살펴보겠다.\nGHSA-r48q-9g5r-8q2h(CVE-2022-1996) : emicklei/go-restful의 버전 3.8.0 이전에서 발견된 취약점이다. 사용자가 제어할 수 있는 키를 통해 권한을 우회할 수 있는 보안 문제가 있다고 한다. 한 가지 의문은 kubescape 내 파드인 kollector 에서 발생하는 것인데, 깃허브 이슈와 커밋에도 없는 사항이라 깃허브 이슈로 등록했다.\n그 외에도 metrics-server, coredns 파드에서도 발생했으며 관련 깃 이슈를 공유한다. 대부분 해당 패키지 버전 업데이트로 피드백하며 업데이트하는 것 같다.\nRBAC Visualizer 쿠버네티스 role 에 부여된 오브젝트 접근 제어 권한에 대해 시각화 기능을 제공해준다. 운영적으로 매력적인 기능이다. 아래 그림처럼 쿠버네티스 ServiceAccount 별로 접근 제어 목록을 한 눈에 확인할 수 있어 불필요한 권한 및 접근 제어에 대해 검증이 가능할 것 같다.\nRepository scanning 코드 관리 저장소인 깃허브, 깃랩 등의 레파지토리에 스캐닝 검사를 시켜준다.\n레파지토리 등록은 공식문서를 참고하자. 필자는 mac 환경에서 진행했다.\n깃허브 토큰을 환경 변수로 등록하고, kubescape CLI 를 통해 취약점 검사 및 웹 대시보드로 정보를 전달한다.\n1 export GITHUB_TOKEN=my-access-token Location 은 저장소 URL 이다. kubescape CLI 를 통해 검사하면 웹 대시보드에서 확인이 가능하다. 아래 화면은 필자의 깃허브 레파지토리를 등록해서 검사하였다.\n레파지토리의 코드와 헬름 차트의 value 값들에도 취약점 검사가 진행된다.\n필자의 레파지토리의 경우 C-0009(Resource limits) 항목이 많아 확인해보니 resource limit 설정 문제였다. 테스트 환경에서 진행함으로 리소스 제한을 풀었지만 운영 환경에서 도입시 파드 구성 환경에 따라 설정해야겠다.\nRegistry Scanning 이미지 레파지토리에도 검사가 가능하다. 하버에서도 이미지 취약점 점검이 가능하는 것으로 알고 있는데 차이점을 확인해보겠다. 기능 확인을 위해 하버를 클러스터에 설치하여 스캐닝 기능을 테스트하였다. 등록시 스캐닝 주기와 태그 개수를 설정할 수 있다.\n시간별로 이미지 레파지토리에 취약 점검이 가능하다. 마찬가지로 grype 엔진을 통해 이미지 취약점 점검을 진행한다.\nINTEGRATIONS kubescape 툴은 Code, CI / CD 에도 활용이 가능하다.\nhttps://github.com/kubescape/kubescape\n공식문서를 확인하니 jenkins, gitlab CI / CD 에서도 job 스케쥴링으로 취약점 점검이 가능하다. CI / CD 파이프라인 구축 이후 고려해보도록 하자.\n3rd party 애플리케이션 연동을 통해 알람 구성도 쉽게 가능하다. 슬랙에서도 알람 설정이 쉽게 가능한데, 스캐닝과 점검 레벨에 따라 알람 설정이 가능하다.\n마치며 이번 블로그 글에서는 쿠버네티스 보안 계층과 보안 툴인 kubescape를 살펴보았다. 개인적으로 쿠버네티스의 보안 생태계를 경험하여 역량 향상에 도움이 되었다. 필자가 생각하기에 kubescape 툴은 미국 가이드라인의 사례나 취약점 점검과 grype 엔진을 통합해서 평가해주는 operator 툴 느낌이 강했다. 비즈니스 모델에 따라 달라지겠지만 비슷한 operator 툴을 대안으로 찾으면 좋을 것 같다.\n","date":"Apr 07","permalink":"https://HanHoRang31.github.io/post/pkos-5-security/","tags":["KANS","kops","cloud","AWS","kubernetes","security","kubescape"],"title":"[PKOS] 쿠버네티스 보안과 Kubescape DeepDive"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 스터디 4주차 시간에는 쿠버네티스 모니터링과 로깅 시스템을 구축하여 기능들을 살펴보았다. 이번 블로그 글에서는 모니터링 시스템에 대해 심화 학습한 내용들을 공유하고자 한다.\n모니터링은 어떤 대상을 감시, 감찰한다는 뜻으로 모니터링의 목적은 지속적인 감시, 감찰을 통해 대상의 상태나 가용성, 변화 등을 확인하고 대비하는 것이다. 모니터링의 개념처럼 쿠버네티스 모니터링도 똑같다. 쿠버네티스에서 특정 기간에 측정한 일련의 숫자(메트릭)에 대해 감시와 감찰을 통해 대상의 상태나 가용성 변화를 확인하고 대비한다고 보면 되겠다. 쿠버네티스 모니터링 시스템으로는 Prometheus, InfluxDB, DataDog, 클라우드 프로바이더 등이 있으나 이번 블로그 글에서는 오픈소스 모니터링 시스템인 Prometheus와 기능 확장 시스템인 Thanos를 다루겠다.\nPrometheus 오픈 소스 모니터링 시스템이다. 시계열 데이터 수집, 저장 및 쿼리 기능을 제공하고 다양한 경고 기능을 제공한다. 오픈소스 진영에서 가장 많이 사용하는 모니터링 시스템으로 사실상 거의 표준처럼 사용하고 있다. 아키텍처는 다음과 같다. (공식문서)\nhttps://prometheus.io/docs/introduction/overview/\n빨간 네모로 표시된 것이 프로메테우스 구성 컴포넌트이다.\nPrometheus Server : Prometheus 서버는 메트릭 수집, 저장, 처리 및 쿼리 기능을 수행한다. 메트릭 수집 방식으로 Pull 방식을 기본적으로 사용한다. 해당 서버가 대상 서비스로부터 메트릭을 주기적으로 수집하고, 시계열 데이터베이스(TSDB, HDD/SDD)에 저장한다. 데이터베이스에 저장한 데이터는 쿼리 언어 PromQL을 통해 데이터를 필터링, 집계 시각화하는데 사용한다. Pushgateway : Pushgateway는 Push 방식을 사용하는 일부 유형의 메트릭을 Prometheus에서 수집하기 위한 중간 서버이다. 주로 일회성 작업(예: 배치 작업)으로부터 메트릭을 수집하는 데 사용된다. 작업이 종료되더라도 메트릭이 보존되어 Prometheus 서버가 해당 메트릭을 수집할 수 있게 한다. Alertmanager: Alertmanager는 Prometheus 서버에서 발생한 경고를 관리하고, 사용자에게 알림을 전달하는 컴포넌트이다. Prometheus UI : 내장된 웹 인터페이스로, 사용자가 Prometheus 서버에서 메트릭을 쿼리하고, 시각화된 그래프를 확인할 수 있다. 사용자는 PromQL을 사용하여 원하는 메트릭을 검색하고 분석할 수 있으며, 기본적인 대시보드 및 경고 설정을 관리할 수 있다. 아키텍처를 살펴보았는데 프로메테우스는 단일 노드 시스템으로 설계되어 있어 클러스터링 구조를 직접 지원하지 않는다. 이로인해 확장성과 고가용성에 일부 보완이 필요하다.\n확장성 문제 단일 노드에서 모든 메트릭을 처리하려 할 때 노드의 자원이 고갈되어 성능 저하를 초래할 수 있다. 대규모 인프라에서 많은 수의 메트릭을 수집하고 처리하는 데 있어 성능 저하와 저장소 부족 문제가 발생할 수 있다. 외부 스토리지 연결이 필요하다. 고가용성 문제 단일 노드에서 발생하는 장애나 다운타임이 생겨 프로메테우스 서버가 내려가면 그 시간 동안에는 메트릭을 수집할 수 없다. 볼륨이 AWS EBS 를 사용해도 단일 노드에서만 연결이 가능하다. 연결 노드에 다운 타임이 발생하면 메트릭을 가져올 수 없다. 이러한 문제를 해결하기 위한 도구로 Thanos를 사용할 것이다.\nThanos 프로메테우스의 확장성과 고가용성을 개선하기 위한 시스템이다. 사이트 정문에 대놓고 프로메테우스를 저격하고 있다. 타노스 아키텍처를 통해 어떻게 개선할 수 있는 확인해보겠다.\nhttps://thanos.io/v0.6/thanos/getting-started.md/\n파란 네모가 타노스 구성 컴포넌트이다. 설계 디자인은 공식 문서에서도 참고가 가능하다.\nThanos Sidecar : Prometheus에 연결되어 메트릭 데이터를 쿼리하고 클라우드 스토리지에 업로드한다. 노드마다 사이드카가 연결되며 외부 스토리지 저장을 통해 확장성을 개선시키는 역할의 컴포넌트이다. Thanos Store Gateway : 외부 스토리지에 메트릭 데이터를 읽어 Thanos Query로 전달한다. 해당 컴포넌트를 통해 외부 스토리지에서 과거 데이터도 쿼리할 수 있게 된다. Thanos Query : 사용자 쿼리를 요청 처리하며 짧은 시간의 데이터는 타노스 사이드카에서 가져오며, 오래된 데이터는 스토어 게이트웨이를 통해 외부 스토리지에서 가져온다. Prometheus Query API를 구현하여 사용자가 기존의 Prometheus 쿼리를 그대로 사용할 수 있게 한다. 프로메테우스단에서 고가용성을 제공해주는 컴포넌트이다. 통합 데이터간의 중복 제거 (de-duplication) 기능을 기본으로 제공하여 여러 프로메테우스 및 원격 스토리지의 메트릭 데이터를 통합하여 쿼리할 수 있게 해준다. 한 가지 주의할 점은 Thanos query 도 고가용성을 보장해줘야 한다. 공식 문서에 따르면 타노스 구성 파드들은 샤딩 수단을 제공하지 않아, 모두 수평적 확장이 가능하다. 타노스 배포시 쿼리 파드 개수를 2개 이상으로 조절하여 가용성을 보장시키자.\nhttps://observability.thomasriley.co.uk/prometheus/using-thanos/high-availability/\nhttps://thanos.io/tip/thanos/design.md/#metric-sources\nThanos Compactor : 타노스 쿼리와는 별개의 프로세스로, 객체 스토리지 버킷만 가리키며 여러 개의 작은 블록을 더 큰 블록으로 지속적으로 통합시켜주는 컴포넌트이다. 블록을 통합시키면 데이터가 압축하게 되므로 버킷의 총 스토리지 크기, 스토어 노드의 로드 및 버킷에서 쿼리 데이터를 가져오는 데 필요한 요청 수가 크게 줄어든다. Thanos Ruler : 프로메테우스 인스턴스들로부터 알림 규칙 정보를 가져와 통합하고, 프로메테우스와 함께 작동하는 외부 알림 시스템에게 알림을 전송하는 역할의 컴포넌트이다. 연계 배포 배포 환경 : kops 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스\n프로메테우스 \u0026amp; 타노스를 연계하여 배포한다. 배포를 위해 헬름 차트를 사용할 예정이며 프로메테우스 배포는 kube-prometheus-stack(그라파나, 추가 메트릭 자동 구성) 차트를 사용할 것이다. 또한 타노스는 bitnami/thanos 차트를 사용할 것이며 타노스 외부스토리지는 MinIO를 배포하여 연결할 것이다. 전체 배포 순서는 다음과 같다. 구성 차트는 필자의 깃허브에서 참고가 가능하다.\nMInIO 배포 kube-promethes-statck 설정 \u0026amp; 배포 타노스 설정 \u0026amp; 배포 그라파나 설정 및 대시보드 확인 1. MinIO 배포 타노스 외부 스토리지로 MinIO를 설정할 것이다. 이를 위한 사전 작업으로 MinIO를 먼저 배포하겠다.\n차트 가져오기\n1 2 3 helm repo add minio https://charts.bitnami.com/bitnami helm repo update helm fetch minio/minio --untar --version 12.2.1 차트 설정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # values-minio.yaml mode: distributed auth: rootUser: admin rootPassword: \u0026#34;admin1234\u0026#34; statefulset: replicaCount: 4 zones: 1 drivesPerNode: 1 provisioning: config: - name: region options: name: ap-northeast-2 ingress: enabled: true hostname: minio.hanhorang.link path: /* annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}, {\u0026#34;HTTPS\u0026#34;:9090}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: \u0026#34;$ACM arn \u0026#34; persistence: storageClass: \u0026#34;kops-csi-1-21\u0026#34; 분산스토리지 모드로 설정 (테스트환경 노드 4개) 노드당 파드 하나를 할당 Ingress(네트워크) : AWS ALB 설정 persistence(볼륨) : AWS gp2 기본 스토리지 클래스 설정 배포\n1 2 kubectl create ns minio helm install minio minio/minio -f values-minio.yaml -n minio --version 12.2.1 버킷 생성 및 접근 키 발급\n타노스에서 minio 버킷에 접근하기 위한 접근 키를 발급받자\nminio 도메인 접속\n어드민 계정은 차트에서 admin / admin1234 로 설정되어 있다. 로그인을 하자.\n로그인이 완료되면 다음과 같은 화면을 확인할 수 있다.\n버킷 생성 후 버킷 접근을 위한 액세스 키 발급이 필요하다. 왼쪽 메뉴 [Access Keys] 에서 키를 발급받자.\nAccess Key 발급 후 MINIO 동작 권한을 등록해야 한다. 생성한 키를 클릭하면 정책 입력 칸이 나온다. 아래 정책을 입력하도록 하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;admin:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::*\u0026#34; ] } ] } 필자의 경우 접근 키는 다음과 같이 생성되었다.\naccess_key : aajl91wFPCRVmfWR\nsecret_key: SfP4woqjY3fcyh1cuwF1CNQFEe6hs4X6\n발급받은 키를 기반으로 Secret을 생성하자.\n1 2 3 4 5 6 7 #minio-key.yaml type: s3 config: bucket: thanos endpoint: minio.hanhorang.link access_key: aajl91wFPCRVmfWR secret_key: SfP4woqjY3fcyh1cuwF1CNQFEe6hs4X6 1 2 kubectl create ns monitoring kubectl create secret generic thanos-minio-secret -n monitoring --from-file=minio-key.yaml 2. kube-promethes-statck 설정 \u0026amp; 배포 프로메테우스 배포 및 사이드 카에 타노스 연동을 위한 설정을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm fetch prometheus-community/kube-prometheus-stack --untar --version 45.7.1 # 사용 리전의 인증서 ARN 확인 CERT_ARN=`aws acm list-certificates --query \u0026#39;CertificateSummaryList[].CertificateArn[]\u0026#39; --output text` echo \u0026#34;alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN\u0026#34; KOPS_CLUSTER_NAME=\u0026#34;hanhorang.link\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # values-kube-prometheus-stack.yaml cat \u0026lt;\u0026lt;EOT \u0026gt; ./values-kube-prometheus-stack.yaml alertmanager: enabled: false grafana: defaultDashboardsTimezone: Asia/Seoul adminPassword: admin1234 ingress: enabled: true ingressClassName: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; hosts: - grafana.$KOPS_CLUSTER_NAME paths: - /* prometheus: # 사이드카 노출 서비스 설정 thanosService: enabled: true ingress: enabled: true ingressClassName: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; hosts: - prometheus.$KOPS_CLUSTER_NAME paths: - /* prometheusSpec: podMonitorSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false retention: 5d retentionSize: \u0026#34;10GiB\u0026#34; scrapeInterval: \u0026#34;15s\u0026#34; # alert 관련 설정으로 주석 처리 # evaluationInterval: 15s # 가용성 설정 replicas: 3 # 타노스 설정 thanos: image: \u0026#34;quay.io/thanos/thanos:v0.27.0\u0026#34; objectStorageConfig: key: minio-key.yaml name: thanos-minio-secret version: v0.27.0 # 볼륨 설정 storageSpec: {} ## Using PersistentVolumeClaim ## # volumeClaimTemplate: # spec: # storageClassName: gluster # accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] # resources: # requests: # storage: 50Gi # selector: {} EOT 알람을 사용하지 않음으로 alertmanager false로 설정하였다. 헬름 차트를 보면 prometheus.thanos 에 설정하는 부분이 있는데 여기서 설정하는 것이 아니다! 원격 스토리지 접근에 대한 오류가 발생하므로 prometheus.prometheusSpec.thanos 에 앞서 생성한 시크릿 키를 입력하자. (위에 차트 그대로 입력하면 문제없습니다.) 1 2 3 4 kubectl create ns monitoring helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.7.1 \\ -f values-kube-prometheus-stack.yaml --namespace monitoring 배포 이후 타노스 사이드카 연동을 확인하자.\n1 kubectl describe pods prometheus-kube-prometheus-stack-prometheus-0 -n monitoring 성공이다!\n3. 타노스 설정 \u0026amp; 배포 타노스 사이드카를 제외한 컴포넌트를 설치하고 thnaos query 가 프로메테우스 사이드카로, store gateway가 원격 스토리지인 minio 로 연동할 수 있도록 설정해야 한다. 차트부터 가져오도록 하자.\n1 2 3 helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm fetch bitnami/thanos --untar --version 12.3.2 타노스 연동을 위해 설정을 진행한다. 메트릭을 가져오기 위해 버킷 정보와 프로메테우스 배포시 같이 배포된 타노스 사이드카 서비스 주소를 입력한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 objstoreConfig: |- type: s3 config: bucket: monitoring endpoint: minio.minio.svc.cluster.local:9000 access_key: aajl91wFPCRVmfWR secret_key: SfP4woqjY3fcyh1cuwF1CNQFEe6hs4X6 insecure: true querier: stores: - kube-prometheus-stack-thanos-discovery.monitoring.svc.cluster.local:10901 - thanos-storegateway.monitoring.svc.cluster.local:10901 replicaCount: 2 ingress: enabled: true hostname: thanos.hanhorang.link ingressClassName: \u0026#34;alb\u0026#34; annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:955963799952:certificate/7569648c-bfd5-4860-b2c1-16ef02acbb58 alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; path : /* bucketweb: enabled: true compactor: enabled: true storegateway: enabled: true ruler: enabled: false objstoreConfig.config.endpoint 를 서비스 DNS로 대체했다. ALB 도메인 입력시 Timeout 으로 파드가 올라가지 않기 때문이다. querier.store 에 쿼리할 대상을 등록한다. 대상으로 타노스 사이드카의 서비스 주소와 스토어게이트웨이를 등록한다. 배포\n1 2 3 helm install thanos bitnami/thanos --version 12.3.2 \\ -f values-thanos.yaml --namespace monitoring 배포 완료 후 타노스 쿼리 호스트 도메인을 통해 접속하자. Store와 Status/Target를 확인하여 사이드카 연동을 확인한다.\n타노스가 정상적으로 배포된 것을 확인하였다. 배포 이후에는 프로메테우스 서버를 2개 이상 띄어서 프로메테우스 서버가 고가용성을 갖도록 구성하자. (앞서 프로메테우스 배포시 프로메테우스 서버를 3개를 배포하였다)\n3개의 프로메테우스 서버가 서로 독립적으로 메트릭을 수집한다. 타노스 쿼리는 프로메테우스에 등록된 사이드카를 통해 메트릭을 통합 수집한다. 이 때 고가용성이 보장되는데 하나의 프로메테우스가 다운타임이 가진다한들 다른 프로메테우스 서버에서 메트릭 수집 및 집계를 수행할 수 있기때문이다. 물론 중복 중복된 메트릭에 대해선 타노스 내 Use Deduplication 기능을 통해 소거가 가능하다. 중복 메트릭 설정은 프로메테우스 라벨 설정을 통해 가능하나 자동으로 설정이 되어 생략하겠다.\n4. 그라파나 설정 그라파나는 시각화 대시보드이다. 앞서 구축한 모니터링 시스템을 기반으로 메트릭 수집 파이프라인을 구성하고 대시보드를 확인하겠다. 그라파나 도메인에 접속하여 로그인을 진행한다. (초기 아이디: admin, 비밀번호: admin1234)\n먼저, 수집 메트릭 URL을 프로메테우스에서 타노스 쿼리로 수정할 것이다. 왼쪽 하단의 톱니바퀴 메뉴에서 Configuration에 들어간 다음 프로메테우스 설정 URL을 thanos-query:9090 으로 수정하자.\n바꾸고 나서 대시보드를 확인하면 정상적으로 작동하는 것을 확인할 수 있다.\n마치며 kube-prometheus-stack 자체적으로도 프로메테우스 고가용성을 보장한다. 하지만 이렇게 구성한 프로메테우스 HA는 여전히 중복 데이터와 쿼리 집계, 확장성에 대한 보완 요소가 있다. 이를 해결하기 위해 Thanos을 소개하였고 연동 방법과 구성 요소를 확인하였다.\n참고 https://aws.amazon.com/ko/blogs/opensource/improving-ha-and-long-term-storage-for-prometheus-using-thanos-on-eks-with-s3/\nhttps://velog.io/@seokbin/Kube-Prometheus-Thanos-구성#4-프로메테우스-ha-구성\n","date":"Apr 01","permalink":"https://HanHoRang31.github.io/post/pkos2-4-monitoring/","tags":["KANS","kops","cloud","AWS","kubernetes","monitoring","prometheus","thanos","Observability"],"title":"[PKOS] Thanos를 통한 고가용성 모니터링(프로메테우스) 시스템 구축하기"},{"categories":null,"contents":" 로깅 (Loki \u0026amp; Promtail ) 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. Logging? 애플리케이션 실행 중 발생하는 이벤트, 작업, 오류 등의 정보를 기록하는 프로세스이다. 로깅의 주요 목적은 프로그램의 실행 상태를 추적하고, 문제 발생 시 원인을 찾기고, 내부 감사를 기록하기 위함이다. 로그 파일은 시간 순서대로 저장되며, 대부분의 경우 텍스트 파일 또는 데이터베이스에 저장된다.\n컨테이너화된 애플리케이션에 가장 쉽고 가장 널리 사용되는 로깅 방법은 표준 출력(stdout)과 표준 에러(stderr) 스트림에 작성하는 것이다. 이를 이용하면 로깅 명령어를 통해 조회가 가능하다.\n1 2 3 4 5 6 # 로그 확인 예 kubectl logs metrics-server-5f65d889cd-znqw5 -n kube-system I0328 00:14:31.072509 1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key) I0328 00:14:31.477085 1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController .. 쿠버네티스 환경에서도 컨테이너 엔진이나 런타임이 제공하는 기본적인 로깅 기능이 있으나 충분하지 않다. 컨테이너가 크래시되거나, 파드가 축출되거나, 노드가 종료된 경우에도 애플리케이션의 로그에 접근할 수 없기 때문이다.\n따라서, 쿠버네티스에서 로그는 노드, 파드 또는 컨테이너와는 독립적으로 별도의 스토리지와 라이프사이클을 가져야 한다. 이 개념을 클러스터-레벨-로깅 이라 하며 이를 위해 별도의 벡엔드 솔루션이 필요하다. 쿠버네티스에 사용할 수 있는 로깅 솔루션은 3가가지 오픈소스 프로젝트를 결합한 PLG 스택(Promtail, Loki, Grafana) 또는 ELK(Elasticsearch, Logstash, Kibana)있다. 이번 블로그 글에서는 PLG 스택을 알아볼 것이며 로깅 시스템인 Loki 와 로그 수집 에이전트인 Promtail 을 설치하하여 클러스터-레벨-로깅을 테스트해보겠다.\nLoki Loki는 Grafana Labs에서 개발한 경량 로깅 시스템으로, 쿠버네티스 환경에서 메타데이터를 기반으로 로그를 수집하고 빠르게 처리할 수 있다. 그리고 Prometheus와 호환되는 레이블 기반 질의 및 필터링 기능을 제공하며, Grafana와 통합을 통해 로깅 데이터를 대시보드에서 확인이 가능하다. 로깅 수집 에이전트인 Promtail을 사용하여 로깅을 수집하며 이를 통해 쿠버네티스 리소스(노드, 파드 또는 컨테이너)와는 독립적으로 별도의 스토리지와 라이프사이클을 가진다.\nhttps://grafana.com/blog/2018/12/12/loki-prometheus-inspired-open-source-logging-for-cloud-natives/\n메타데이터 인덱싱?\nhttps://grafana.com/oss/loki/\n로키는 메타데이터를 인덱싱을 통해 경량화 및 빠른 쿼리 성능을 가진다. 메타인덱싱 원리는 위의 그림과 같은데 로그 전체 텍스트를 저장하는 것이 아니라 타임스탬프와 라벨을 묶어 인덱스(index) 로 그 외 나머지 텍스트를 청크(chunk)로 나눠 저장된다.\n인덱스: 로그 시간과 레이블을 묶어 해싱을 통해 고유한 식별자를 만든다. 이 식별자를 통해 로그 스트림을 참조하고 검색하는데 사용된다. 인덱스는 일반적으로 NoSQL DB에 저장하는데 Key 값에는 인덱스를 values 값에는 해당 청크의 데이터를 저장한다. 청크: 청크는 실제 로그 데이터이다. 청크는 데이터를 압축 및 저장하기 위해 여러 압축 알고리즘을 사용할 수 있으며, 기본적으로 압축이 적용되어 저장 공간을 최적화합니다. 일반적으로 오브젝트 스토리지에 저장한다. 아키텍처 https://grafana.com/blog/2018/12/12/loki-prometheus-inspired-open-source-logging-for-cloud-natives/\n그림에서 화살표 빨강은 로그 Write, 파랑은 Read를 의미한다. 또한, 각 구성 컴포넌트들은 HA를 지원하여 컴포넌트 내부 구성 하나에 장애가 발생하더라도 서비스가 중단되지 않는다.\nYour Jobs : 로그 수집 에이전트로 사용자 정의에 맞게 로그를 수집하여 로키 서버(Distributor)에 전달한다. 로그 수집 에이전트로 Promtail를 사용하나 fluent, fluentbit 과 호환이 가능하다. Distributor(디스트리뷰터): Distributor는 로그 데이터를 수신하고, 해당 데이터를 인제스터(Ingester)에 분산시키는 역할을 한다. 또한, 레이블의 해시 값을 사용하여 데이터를 적절한 인제스터에 전달하며 로드밸런싱을 통해 로그 데이터를 여러 인제스터에 고르게 분산시켜준다. Ingester(인제스터): Ingester는 Distributor로부터 로그 데이터를 받아서, 로그 스트림을 청크로 나누고 압축한 후 저장시킨다. 인제스터는 메모리 또는 영구 스토리지에 청크를 저장할 수 있으며, 쿼리어(Querier)에게 저장된 청크에 대한 질의 결과를 제공한다. 청크가 일정 시간 또는 크기에 도달하면 인제스터는 이를 영구 스토리지에 저장시킨다. Querier(쿼리어): Querier는 사용자로부터 질의를 받아 처리한다. 질의를 처리할 때, 쿼리어는 인덱스를 사용하여 관련된 청크를 찾고, 인제스터 및 영구 스토리지에서 해당 청크를 가져와 질의 결과를 반환한다. loki 버전 3.0 이상부터는 loki, loki-distributed가 통합되었다.\nhttps://grafana.com/docs/loki/latest/getting-started/\nLoki Write component : 로그 데이터를 수신하고 저장시켜주는 컴포넌트이다. 앞서 아키텍처의 빨간 flow를 담당하는 Distributor와 Ingester 로 구성되어 있다. Loki Read component: 로그 데이터를 조회하고 처리하는데 사용된다. 앞서 아키텍처의 파랑 flow를 담당하는 Querier와 Query Frontend 로 구성되어 있다. gateway : Loki 구성요소에 대한 프록시 서버이다. 로그를 까보면 NGINX 게이트웨이가 설치되며 로드밸런싱 기능을 수행하여 각 컴포넌트에 트래픽을 분산시킨다. 설치 설치 환경 : kops 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스\n설치는 다음과 같이 진행할 예정이다.\nLoki (helm grafana/loki 4.8.0) Promtail (helm grafana/promtail 6.9.2) 사족이지만, 로그 저장소로 mongoDB를 활용하려 했으나 안 된다! 로키 호환 저장소가 정해져있기 때문이다. 온프로미스에서 구성시 참고하자.\n본 블로그에서는 S3에 오브젝트 데이터를 DynamoDB에 인덱스 데이터를 저장시키겠다. (230402. DynamoDB 인덱스 연동문제로 S3에 인덱스, 오브젝트 데이터 저장)\nhttps://grafana.com/docs/loki/latest/operations/storage/\nLoki 설치 Loki 저장소로 S3 와 DyanmoDB 스토리지 생성과 IAM role 권한 연결이 필요하다. 본 테스트에서는 S3 이름을 han-loki , DyanmoDB 이름은 loki_ 로 생성하였다. DynamoDB 사용시 주의해야할 점은 다음과 같다.\nDynamoDB 테이블 이름을 헬름 차트의 schema_config.config DynamoDB 파티션 키 \u0026amp; 정렬 키를 (문자열, 바이너리)로 지정해야 한다. IAM 권한은 S3, DyanmoDB에 대한 정책을 부여했다.\nAmazonS3FullAccess AmazonDynamoDBFullAccess 필자는 사용자에 IAM 권한를 부여 후 access-key 와 secret-key를 가져왔다. 해당 키는 밑의 헬름 차트 버킷 연동에서 사용된다. 우선 헬름을 통해 로키 차트를 가져오겠다.\n1 2 3 4 kubectl create ns loki helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm fetch grafana/loki --untar --version 4.8.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 #values-loki.yaml schema_config: configs: - from: 2020-05-15 store: aws object_store: s3 schema: v11 index: prefix: loki period: 0 storage_config: aws: s3: s3://ap-northeast-2/han-loki dynamodb: dynamodb_url: dynamodb://ap-northeast-2 table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: write_scale: enabled: false read_scale: enabled: false chunk_tables_provisioning: write_scale: enabled: false read_scale: enabled: false table_prefix: \u0026#34;loki\u0026#34; tableManager: enabled: true monitoring: lokiCanary: enabled: true selfMonitoring: enabled: false loki: auth_enabled: false storage: bucketNames: chunks: han-loki ruler: han-loki admin: han-loki type: s3 s3: s3: han-loki endpoint: s3.ap-northeast-2.amazonaws.com region: ap-northeast-2 secretAccessKey: {SECRET KEY} accessKeyId: {ACCESS KEY} s3ForcePathStyle: false insecure: false http_config: {} access_key 와 Secret_access_key 노출에 주의하자! schema_config : 인덱스와 청크 데이터에 대한 저장 스키마를 정의한다. storage_config: 데이터 저장할 스토리지 정보를 입력한다. table_manager, tableManager : 테이블 기반 데이터 저장소에 인덱스 및 청크를 지원하는데 버전 호환 문제로 작동이 안되어 dynamodb 에 인덱스가 저장이 안된다. (현재 S3에 저장됨) monitoring.lokicanary: 시스템 검증에 사용된다. true 설정시 별도의 카나리 파드가 생성되어 일정시간마다 테스트 로그를 전달한다. monitoring.selfMonitoring : 대시보드에 Loki 관련 대시보드가 업로드된다하지만, loki: 로키 서버 설정을 정의한다. 최신 버전에는 스토리지 연동을 여기서 하는데 dynamodb에 대한 설정 예시가 없고 테스트가 안되서 s3 만 정의하였다. 1 helm install loki grafana/loki -f values-loki.yaml -n loki --version 4.8.0 파드 배포는 완료되었으나, 로키 스토리지 연동으로 안정화 작업이 필요하다.\n안정화 작업는 다음과 같다.\nstorage_config 스토리지 연동 문제\n1 2 level=error ts=2023-04-01T03:18:30.427197283Z caller=flush.go:144 org_id=self-monitoring msg=\u0026#34;failed to flush\u0026#34; err=\u0026#34;failed to flush chunks: store put chunk: NoCredentialProviders: no valid providers in chain. Deprecated.\\n\\tFor verbose messaging see aws.Config.CredentialsChainVerboseErrors, num_chunks: 1, labels: {app_kubernetes_io_component=\\\u0026#34;read\\\u0026#34;, app_kubernetes_io_instance=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_name=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_part_of=\\\u0026#34;memberlist\\\u0026#34;, cluster=\\\u0026#34;loki\\\u0026#34;, container=\\\u0026#34;loki\\\u0026#34;, controller_revision_hash=\\\u0026#34;loki-read-7749df4969\\\u0026#34;, filename=\\\u0026#34;/var/log/pods/loki_loki-read-2_a8fab5a2-e78b-4cff-9f8d-ba0ee5952a3c/loki/0.log\\\u0026#34;, job=\\\u0026#34;loki/loki-read\\\u0026#34;, namespace=\\\u0026#34;loki\\\u0026#34;, pod=\\\u0026#34;loki-read-2\\\u0026#34;, statefulset_kubernetes_io_pod_name=\\\u0026#34;loki-read-2\\\u0026#34;, stream=\\\u0026#34;stderr\\\u0026#34;}\u0026#34; level=info ts=2023-04-01T03:18:30.427244754Z caller=flush.go:168 msg=\u0026#34;flushing stream\u0026#34; user=self-monitoring fp=33d14d11bfd98f55 immediate=false num_chunks=1 labels=\u0026#34;{app_kubernetes_io_component=\\\u0026#34;read\\\u0026#34;, app_kubernetes_io_instance=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_name=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_part_of=\\\u0026#34;memberlist\\\u0026#34;, cluster=\\\u0026#34;loki\\\u0026#34;, container=\\\u0026#34;loki\\\u0026#34;, controller_revision_hash=\\\u0026#34;loki-read-7749df4969\\\u0026#34;, filename=\\\u0026#34;/var/log/pods/loki_loki-read-2_a8fab5a2-e78b-4cff-9f8d-ba0ee5952a3c/loki/0.log\\\u0026#34;, job=\\\u0026#34;loki/loki-read\\\u0026#34;, namespace=\\\u0026#34;loki\\\u0026#34;, pod=\\\u0026#34;loki-read-2\\\u0026#34;, statefulset_kubernetes_io_pod_name=\\\u0026#34;loki-read-2\\\u0026#34;, stream=\\\u0026#34;stderr\\\u0026#34;}\u0026#34; 공식문서 예제 storage_config 가 최신 기준으로 업데이트된 것 같지 않다. 필자의 경우 배포 yaml로 구성을 설정하니 정상적으로 작동했다.\nS3 디렉토리에 chunk 폴더가 없다? 이슈 에 따르면 멀티테넌트 구성에서 loki를 실행할때 인증이 비활성화하면 기본적으로 fake 폴더에 저장된다고 한다. fake폴더 안에는 정상적으로 chunk가 들어가있는 것을 확인할 수 있으나 다중 클러스터에서 배포시 loki 별로 인증이 필요할 것 같다.\ndynamoDB에 인덱스를 저장하고 싶어요.\n해결해야할 문제다. 현재 S3에 index값이 들어가는데 dynamodb에 옮겨야 한다. 헬름차트에 table-manager 설정이 두개(table-manager, tableManager)여서 설정이 안 먹힌다. 추가 원인으로는 깃허브 이슈( https://github.com/grafana/loki/issues/5070) 설정값( extraArgs)인데 적용이 안된다. 추후 해결시 업데이트하겠다.\n배포를 완료하면 로키 파드가 정상적으로 작동하는 것을 알 수 있다.\nhttp://loki-read-headless.loki.svc.cluster.local:3100\nPromtail 설치 1 2 3 helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm fetch grafana/promtail --untar --version 6.9.2 Promtail 차트에서 따로 설정할 것은 없지만, 로키 연결과 로그 라인 파이프라인 구성 확인을 위해 집어넣는다.\nconfig.clients.url : 로그 수집 후 전달한 로그 서버를 입력한다. 보통 로키 서버 설치시 설정되는 url로 지정된다. config.snippets: 로그 라인 분석과 추출, 필터링하는 스테이지들의 작업을 정의한다. 원하는 로그 데이터 형식을 정의해서 입력하면 된다. scraping 구문은 프로메테우스와 동일하다. 1 helm install promtail grafana/promtail -n loki --version 6.9.2 로그 수집 테스트 Promtail-Loki-Grafana 까지의 로그 수집을 테스트하겠다. 테스트를 위해 nginx 를 배포하고 파드 로그 확인과 그라파나(로키)에서 로그를 확인한다.\n1 2 helm repo add bitnami https://charts.bitnami.com/bitnami helm install nginx bitnami/nginx --version 13.2.23 -f nginx-values.yaml 파드 로그는 파드가 올라간 노드 /var/log/pods 에 저장되어 있다. 파드의 로그를 확인하고 그라파나 대시보드에서 로키가 해당 로그를 긁어오는지 확인하자.\n[Explorer] → Job = default/nginx 설정 후 로그 확인 잘 들어온다! 이어서 파드 라이프사이클과 독립적으로 로그가 관리되는 지 확인하겠다. nginx 파드를 삭제하고 파드 로그와 로키를 확인하겠다.\n1 helm uninstall nginx 아래 디렉토리를 확인하면 nginx 파드가 삭제되어 로그 디렉토리가 삭제된 것을 확인할 수 있다.\n파드가 삭제되었지만 그라파나(로키) 에서 nginx 로그를 확인할 수 있다.\n마치며 그라파나 공식 문서를 참고하니 엔터프라이즈에 대한 지원만 활발한 느낌이다. 오픈소스로 설치시 연동 부분과 최신 버전 호환 문제로 테스트하는 데도 오랜 시간이 걸렸다. 특히 인덱스 데이터를 dynamodb 에 연동해야 했지만 설정 문제로 실패했고 S3에 인덱스, 청크 데이터를 저장시켰다. 이 부분은 공식 문서를 최신 버전으로 업데이트를 하거나 예가 나오면 업데이트하겠다.\n","date":"Apr 01","permalink":"https://HanHoRang31.github.io/post/pkos2-4-logging/","tags":["KANS","kops","cloud","AWS","kubernetes","monitoring","PLG","loki","grafana","promtail","Observability"],"title":"[PKOS] 로깅 PLG 스택, 최신 버전(Loki v2.8.0) 배포하기"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 3주차 시간에는 Gitlab 과 ArgoCD를 배포하여 Gitops 시스템을 구축하였다. 이번 블로그 글에서는 GitOPS 시스템에 대한 실습 내용들을 정리하고 공유하고자 한다.\nGitOps는 GIt을 진실의 원천(SSOT, Single Source of Truth) 으로 사용하는 인프라와 애플리케이션 배포 관리 방식이다. 진실의 원천이라는 말은 Git에서만 소스를 관리할 수 있게 하여 단일 진실 원천을 구현한다는 말이다. 쿠버네티스에서는 GItOps를 ArgoCD를 이용하여 깃 저장소에 있는 소스를 정의된 클러스터 환경에 자동으로 반영시켜 준다.\nGitOps 시스템을 구축하면 얻을 수 있는 이점은 다음과 같다.\n버전 관리: Git을 사용하므로 인프라 및 애플리케이션의 모든 변경 사항이 추적되고 버전이 관리된다. 이를 통해 문제 발생 시 이전 상태로 쉽게 되돌릴 수 있다. 디커플링: 코드와 인프라를 분리함으로써 개발자와 운영팀 간의 협업이 쉬워진다. 자동화: 변경 사항이 자동으로 적용되므로 수동 인프라 관리 작업이 줄어들고, 실수를 방지할 수 있다. 보안: Git 저장소에 접근 권한을 제어함으로써 인프라 변경에 대한 보안을 강화할 수 있다. 신속한 피드백 루프: 문제가 발생하면 소스 코드에 대한 변경을 통해 빠르게 해결하고 적용할 수 있다. 이점만 존재하는 것은 아니다, 단점도 존재한다.\n학습 곡선: GitOps 및 관련 도구를 사용하려면 Git, 선언적 인프라 도구 및 오케스트레이션 플랫폼에 대한 지식이 필요하다. 복잡성: GitOps를 사용하면 초기 설정과 관리가 복잡할 수 있다. 적절한 도구와 프로세스를 구축하고 유지 관리하는 데 시간과 노력이 필요할 수 있다. 높은 의존성: GitOps는 Git 저장소에 대한 높은 의존성을 가지며, 저장소 접근이 불가능한 경우 인프라 변경이 제한될 수 있다. 간단하게 예제 시나리오를 구성하여 GitOps 시스템을 구축하고자 한다. 먼저 쿠버네티스 환경에 Gitlab과 ArgoCD를 배포할 것이고, 관리 대상을 지난 시간에 배운 Harbor 배포 차트로 지정할 것이다.\nGitlab 배포 Gitlab은 소스 원격 저장소이다. 흔히 쓰는 깃허브로 생각하면 이해가 빠르다. Gitlab의 차별점은 사설(공식문서에는 offline이라 칭함) 깃랩을 구축할 수 있다는 점인데 직접 헬름 차트를 구성하여 배포하겠다.\n깃랩 차트는 다음과 같이 불러올 수 있다.\n1 2 3 helm repo add gitlab https://charts.gitlab.io/ helm repo update helm fetch gitlab/gitlab --untar --version 6.8.1 깃랩 차트에 부가 옵션이 많다.. 공식 문서를 참고하자. 다음은 필자가 차트를 보고 간략히 정리한 내용이다.\n유료 버전 무료 제공 : 쿠버네티스에서 오프라인 깃랩 설치시 enterprise edtion (유료, 이하 EE라 칭함) 을 무료로 사용할 수 있다. 이유를 찾아보니 고객 유치 전략이라 한다. EE 사용시 고급 보안, 인증, 권한 관리, 진행률 보고, 고급 CI/CD 기능, 멀티 프로젝트 파이프라인 등의 고급 기능을 제공한다. 스토리지 관련 가용성 제공 : Gitlab 자체적으로 도커 레지스트리를 제공할 수 있다. 또한,레지스트리 이미지 및 깃랩 페이지, 등의 데이터등에 대한 저장소 스토리지로 Minio 를 사용한다. 앞선 블로그 글에서 harbor 레지스트리에 대한 고가용성 구축을 다루었는데, 깃랩에서는 자동으로 연동해주는 것 같다. 네트워크 최적화 기능 제공 : gitlab 서버(Gitlay)에 대한 로드밸런싱(Prafect) 기능을 제공하며, Workhorse라는 컴포넌트를 통해 중앙 프록시 및 파일 처리를 관리한다. 보안 : Oauth, 인증 및 권한 관리(gitlab shell) 을 제공한다. Observability 제공 : 그라파나 연동 기능, Tracing 기능을 제공한다. CI / CD 제공 : gitlab-runner 라는 컴포넌트를 통해 CI / CD 기능을 제공한다. 뭐지..? 단순히 깃 저장소를 확장하여 대부분의 addon를 자동으로 연계시켜 제공하다니, 심지어 고가용성, 최적화에 대한 구성도 자동으로 제공해준다. 기능별로 세세히 보고 싶은 마음이 굴뚝같지만, 이번 블로그글에서는 gitops 시스템 대한 내용만 다룬다. 차트에서 다음과 같은 부분을 수정하였다.\nvalues-gitops.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 global: hosts: domain: {도메인 입력} ingress: configureCertmanager: false provider: aws class: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: {ACM arn 입력} alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;gitlab\u0026#34; tls: enabled: false certmanager: install: false nginx-ingress: enabled: false prometheus: install: false gitlab-runner: install: false 헬름 차트에서는 차트 오버라이드가 가능하다. 위의 차트에서 ACM값만 수정해서 설치를 진행하자.\n1 helm install gitlab gitlab/gitlab -f values-gitops.yaml --namespace gitlab --version 6.8.4 설치 후 도메인을 통해 로그인을 진행한다. 초기 admin 계정의 아이디는 root 이며, 비밀번호는 다음의 명령어에서 확인한다.\n1 2 # 웹 root 계정 암호 확인 kubectl get secrets -n gitlab gitlab-gitlab-initial-root-password --template={{.data.password}} | base64 -d ;echo 접속하면 깃랩 프로젝트 화면이 보인다. 이어서 PKOS 스터디에서 진행한 실습 내용을 테스트하겠다. 사용자 계정을 생성하여 토큰을 발급받고, 신규 프로젝트를 파일을 업로드해보자.\nArgoCD 배포 ArgoCD는 Git 리포지토리에 저장된 쿠버네티스 매니페스트와 실제 클러스터 상태를 동기화시켜주는 지속적인 배포(Continuous Delivery, CD) 툴이다. Argo CD를 사용하면 Git 리포지토리를 기반으로 인프라 구성을 코드로 관리할 수 있게 된다. 이는 단일 진실 원천(SSOT)로 자동화, 보안, 버전 관리 측면에서 유용하다.\nArgoCD 배포는 Helm 차트로 진행한다.\n1 2 3 helm repo add argo https://argoproj.github.io/argo-helm helm repo update helm fetch argo/argo-cd --untar --version 5.19.14 ArgoCD 차트 분량도 상당하다. 아키텍처를 보니 이해가 빠른 것 같아서 먼저 공유한다.\nhttps://blog.searce.com/argocd-gitops-continuous-delivery-approach-on-google-kubernetes-engine-2a6b3f6813c0\nargo-cd-server: Argo CD API 서버와 웹 UI를 제공하는 컴포넌트이다. 사용자 인증 및 권한 관리를 처리하며, 클러스터와 Git 리포지토리 간의 동기화를 관리한다. argo-cd-repo-server: Git 리포지토리와 통신하여 사용자가 관리하는 쿠버네티스 매니페스트 파일을 가져오는 컴포넌트이다. 또한, 리포지토리 내의 Helm 차트와 Kustomize 구성을 처리한다. argo-cd-application-controller: Argo CD의 핵심 컴포넌트로, 쿠버네티스 클러스터 상태와 Git 리포지토리 상태를 비교하고 동기화를 수행한다. 클러스터의 실제 상태와 원하는 상태를 일치시키는 작업을 담당한다. argo-cd-dex-server: 인증 프록시 역할을 하는 컴포넌트로, 다양한 OAuth 및 OIDC 프로바이더와 통합하여 Argo CD 인증을 처리한다. argo-cd-redis: 캐싱 및 세션 관리를 위한 Redis 데이터베이스이다. Argo CD는 Redis를 사용하여 성능 향상과 빠른 응답 시간을 제공한다. Kustomize ?\n쿠버네티스 리소스 구성을 커스터마이징을 위한 도구이다. 동일한 구성을 가진 매니패스트에 수정할 부분만 추가해서 오버라이드가 가능하다.\n아래 예는 원본 베이스 앱(my-app) 에 dev, ops 환경별 필요 값을 오버라이드하는 예제이다.\n먼저 베이스가 되는 deployment를 선언하고 Kustomize 구성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # base/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-app image: my-app-image:latest ports: - containerPort: 80 1 2 3 4 5 # base/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml 이제 dev, Ops 환경에 대한 kustomization.yaml 파일을 생성하고 다음 내용을 추가하자.\nDev 환경 설정\n1 2 3 4 5 6 7 # overlays/dev/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - ../../base patchesStrategicMerge: - deployment-patch.yaml 1 2 3 4 5 6 7 8 9 10 11 12 # verlays/dev/deployment-patch.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 2 template: spec: containers: - name: my-app image: my-app-image:dev Ops 환경 설정\n1 2 3 4 5 6 7 # overlays/ops/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - ../../base patchesStrategicMerge: - deployment-patch.yaml 1 2 3 4 5 6 7 8 9 10 11 12 # overlays/ops/kustomization.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 4 template: spec: containers: - name: my-app image: my-app-image:ops 눈치 챘는가? Ops와 dev의 replicas 개수와 이미지 설정만 달랐고 그 부분만 추가했다. 배포는 다음 식으로 진행한다.\n1 kubectl kustomize overlays/dev | kubectl apply -f - 위의 컴포넌트별 차트에서 설정이 가능하다. 추가 설정은 아래 config.param 에서 오버라이드하는 것을 추천한다. 사용 예는 깃허브에 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 # Argo CD configuration parameters ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/argocd-cmd-params-cm.yaml params: ## Controller Properties # -- Number of application status processors controller.status.processors: 20 # -- Number of application operation processors controller.operation.processors: 10 # -- Specifies timeout between application self heal attempts controller.self.heal.timeout.seconds: 5 # -- Repo server RPC call timeout seconds. controller.repo.server.timeout.seconds: 60 깃허브의 사용 예에는 중요한 reSyncPreiod 설정(저장소 동기화 시간) 이 없는 것 같다. 이럴 때는 직접 차트를 확인하여 구성 값을 확인하고 config.params에 추가하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # argo-cd/templates/argocd-application-controller - command: - argocd-application-controller - --metrics-port={{ .Values.controller.containerPorts.metrics }} {{- if .Values.controller.metrics.applicationLabels.enabled }} {{- range .Values.controller.metrics.applicationLabels.labels }} - --metrics-application-labels - {{ . }} {{- end }} {{- end }} {{- with .Values.controller.args.statusProcessors }} - --status-processors - {{ . | quote }} {{- end }} {{- with .Values.controller.args.operationProcessors }} - --operation-processors - {{ . | quote }} {{- end }} {{- with .Values.controller.args.appResyncPeriod }} # config.params 추가 - --app-resync - {{ . | quote }} {{- end }} {{- with .Values.controller.args.appHardResyncPeriod }} - --app-hard-resync - {{ . | quote }} {{- end }} {{- with .Values.controller.args.selfHealTimeout }} - --self-heal-timeout-seconds - {{ . | quote }} {{- end }} 실제 배포는 CLB에 externalDNS 로 진행하였다.\n1 2 3 4 5 6 # values-argocd.yaml server: service: type: LoadBalancer annotations: external-dns.alpha.kubernetes.io/hostname: argocd.\u0026lt;도메인 입력\u0026gt; 배포\n1 2 kubectl create ns argocd helm install argocd argo/argo-cd -f values-argocd.yaml --namespace argocd --version 5.19.14 실제로 배포할 시 고려할 점은 접네트워크 대역이다. ArgoCD는 클러스터를 직접적으로 관리할 수 있기 때문이다. 실제 Devops 팀이나 Admin 사용자가 사용할 것이라 예상한다. 이를 위해 허용된 네트워크 대역에서만 로드밸런서 접근이 가능하도록 설정하는 것이 중요할 것 같다. 로드밸런스 설정 후 적절한 보안 그룹을 생성하여 접근을 제어하도록 하자.\n초기 admin 로그인 정보는 다음의 명령어로 확인이 가능하다.\n1 2 3 #비밀번호 ARGOPW=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) echo $ARGOPW 로그인 이후 ArgoCD에 클러스터와 깃 저장소 등록이 필요하다. ArgoCD UI 나 CLI 를 통해 확인 및 등록이 가능하다. 클러스터는 구축한 클러스터 정보가 기본으로 등록되어 있다. 향후 생산성을 위해 CLI를 통해 확인해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 최신버전 설치 curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 install -m 555 argocd-linux-amd64 /usr/local/bin/argocd chmod +x /usr/local/bin/argocd # 버전 확인 argocd version --short # argocd 서버 로그인 argocd login argocd.$KOPS_CLUSTER_NAME --username admin --password $ARGOPW WARNING: server certificate had error: x509: certificate is valid for localhost, argocd-server, argocd-server.argocd, argocd-server.argocd.svc, argocd-server.argocd.svc.cluster.local, not argocd.hanhorang.link. Proceed insecurely (y/n)? y \u0026#39;admin:login\u0026#39; logged in successfully Context \u0026#39;argocd.hanhorang.link\u0026#39; updated # argocd repo 등록 argocd repo add https://gitlab.hanhorang.link/Horang/test-stg.git --username horang --password PASSWORDa! Repository \u0026#39;https://gitlab.hanhorang.link/Horang/test-stg.git\u0026#39; added # argocd 확인 argocd repo list TYPE NAME REPO INSECURE OCI LFS CREDS STATUS MESSAGE PROJECT git https://gitlab.hanhorang.link/Horang/test-stg.git false false false true Successful argocd cluster list SERVER NAME VERSION STATUS MESSAGE PROJECT https://kubernetes.default.svc in-cluster Unknown Cluster has no applications and is not being monitored. GitOps 구축 구축한 Gilab, ArgoCD 를 통해서 GitOps 시스템을 구축해보겠다. gitops 정의대로 깃 저장소에 있는 헬름 차트가 쿠버네티스 환경에 실시간으로 동기화되는 지 테스트해보겠다. 사용 헬름 차트는 앞서 스토리지 테스트를 위해 구축한 minIO 를 대상으로 진행하겠다. 해당 차트는 필자의 깃허브에서 확인이 가능하다.\n1 2 3 helm repo add minio https://charts.bitnami.com/bitnami helm repo update helm fetch minio/minio --untar --version 12.2.1 먼저, gitlab 저장소에 헬름 차트를 PUSH한다.\n다음은 ArgoCD를 통해 동기화를 진행한다. 동기화 구성은 ArgoCD CRD로 작성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: minio-helm namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: minio server: https://kubernetes.default.svc project: default source: repoURL: https://gitlab.hanhorang.link/Horang/test-stg path: minio/ targetRevision: HEAD helm: valueFiles: - values-minio.yaml syncPolicy: syncOptions: - CreateNamespace=true spec.destination : 동기화 클러스터 대상을 나타낸다 spec.source : 깃허브 저장소를 입력한다. spec. syncpolicy : 동기화 정책을 구성한다. 옵션은 공식 문서를 참고바란다. CreateNamespace=true 옵션은 대상 네임스페이스가 없으면 생성시킨다는 옵션이다. 배포 후 ArgoCD UI에서 확인하자.\n1 kubectl apply -f minio-helm-argo.yaml 배포시 OutofSync 의 상태가 되는데 상단의 Sync App눌러 동기화를 진행한다.\n동기화시 옵션에 따라 세부 동작이 가능하다.\nPRUNE: 리포지토리에 없는 리소스를 삭제함 DRY RUN : 테스트로 실제 변경하지 않음 APPLY ONLY: 리소스 생성 및 수정만 수행하고 삭제하지 않음 FORCE : 강제 적용\nSKIP SCHEMA VALIDATION: 리소스 매니페스트의 JSON 스키마 검증을 건너뛰는 옵션이다. 이 옵션은 매니페스트에 포함된 스키마가 유효하지 않거나 검증되지 않아도 배포를 진행하고자 할 때 사용한다. AUTO-CREATE NAMESPACE: ArgoCD가 리소스를 배포할 네임스페이스가 존재하지 않는 경우 자동으로 해당 네임스페이스를 생성하는 옵션이다. PRUNE LAST: 클러스터에 존재하지 않아야 하는 리소스를 자동으로 제거하여 깔끔한 상태를 유지할 수 있도록 돕는다. APPLY OUT OF SYNC ONLY: ArgoCD가 오직 동기화되지 않은 리소스에 대해서만 kubectl apply 명령을 실행하는 옵션이다. 이렇게 하면 이미 동기화된 리소스는 건드리지 않고, 변경된 리소스에 대해서만 업데이트를 진행한다.\nRESPECT IGNORE DIFFERENCES: ArgoCD가 리소스를 비교할 때, 무시해야 하는 차이점을 존중하도록 설정하는 옵션이다. 이렇게 하면 사용자가 지정한 특정 필드의 변경 사항을 무시하고 동기화 여부를 결정할 수 있다.\nSERVER-SIDE APPLY: ArgoCD가 서버 측에서 리소스를 적용하도록 설정하는 옵션이다. 이 옵션을 사용하면, 리소스의 변경 사항이 서버 측에서 자동으로 병합되어 관리자가 수동으로 병합할 필요가 없다. 이 방식은 클라이언트 측에서 **kubectl apply**를 사용하는 것보다 더 효율적인 리소스 관리를 가능하게 한다.\nREPLACE : 리소스 변경시 기존 리소스를 삭제하고 새로운 리소스를 생성하여 대체한다.\nRETRY : 동기화 실패시 재시도\nSync 후 배포까지 모니터링 후 정상적으로 작동하는 것을 확인할 수 있다.\nArgoCD는 기본적으로 수동적으로 Sync 작업이 필요하다. 자동으로 깃 저장소에 내용으로만 동기화시키려면 self-healing 옵션이 필요하다. App Detail의 Policy 설정에서 활성화하자.\n마치며 이번 글에서는 Gitlab 와 ArgoCD 차트를 분석하여 구성하였고 GitOps 시스템을 구축해보았다. 인프라 관리자 측면에서 SSOT를 구성하면 코드 구성 관리 측면에서 편리해질 것이 느껴진다. 그리고 Gitlab와 ArgoCD 메뉴얼이 잘 정리되어 있다. 확장 기능(메트릭, 알람, 보안) 필요시 메뉴얼을 참고하자!\n","date":"Mar 25","permalink":"https://HanHoRang31.github.io/post/pkos2-3-gitops/","tags":["KANS","kops","cloud","AWS","kubernetes","GitOps","Gitlab","ArgoCD","CI/CD"],"title":"[PKOS] GitOps와 ArgoCD DeepDive"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 2주차 스터디에서는 쿠버네티스의 네트워크와 스토리지를 중점적으로 공부하였다. 분량이 많아 네트워크와 스토리지를 나눠서 블로그를 작성할 예정이다. 이번 블로그 글에서는 로컬 스토리지에 대해 공유하겠다. 일반적으로 로컬 스토리지는 IOPS 성능이 특화되어 있지만 노드에 종속되어 있어 고가용성이나 스토리지 기능에 제한이 있다. 이러한 제한을 없애기 위한 과정으로 로컬 스토리지의 Hostpath, local 볼륨을 마운트하여 테스트를 진행할 것이고, 마지막으로는 로컬 볼륨에서 고가용성과 스토리지 기능(백업)을 가진 Mysql 데이터베이스를 구성하겠다. 추가로 스토리지 성능 측정과 모니터링 과정, QnA를 준비하였다. 본론으로 들어가서, 쿠버네티스에서 스토리지를 사용하는 이유는 무엇일까? 스토리지가 데이터를 저장하는 용도인 것처럼 데이터 저장을 위해서이다. 예를 들어, 데이터베이스같은 애플리케이션을 파드로 운영한다고 가정해보자, 파드 라이프사이클과 별개로 데이터가 보존되어야 한다. 이를 위해 쿠버네티스에서는 PV(Persistent Volume)과 PVC(Persistent Volume Claim) 리소스를 제공한다. 또한, 데이터 관리 방법(데이터 저장 위치, 데이터 공유, 확장성)에 따라 여러 스토리지 볼륨과 기능을 제공한다. 이처럼 데이터를 보존해야 하는 애플리케이션을 상태있는(Stateful) 애플리케이션이라 칭하며 스토리지를 통해 클러스터 내의 컨테이너에 안정적이고 지속적인 데이터를 제공할 수 있다.\n로컬 스토리지 로컬 스토리지는 말 그대로 파드의 스토리지로 서버 내부 볼륨의 스토리지를 사용하는 것이다. AWS EC2는 내부 볼륨인 인스토어 스토어를 사용한다. 로컬 스토리지 구현은 쿠버네티스에서 HostPath, Local 볼륨 마운트로 나뉘어 사용이 가능하다. HostPath 볼륨과 Local 볼륨의 차이는 쿠버네티스 볼륨 리소스(PV) 사용 유무에 따라 구분한다.\n일반적으로 내부 볼륨의 스토리지를 사용하는 만큼, 다른 원격 스토리지와 비교했을때 IOPS 성능이 뛰어나다. cncf 공식사이트에서 IOPS 기준 약 2~3배의 차이가 난다고 하니 성능 필요의 애플리케이션에서는 도입을 고려할 만하다. 그리고 로컬스토리지는 EC2 에 종속되어 있어 고가용성 구성과 백업같은 기능 사용에 추가 구성이 필요하다. 필자는 이를 해결하기 위해 볼륨프로비저닝 플러그인인 Local-path-provisioner 와 백업 솔루션인 Velero를 사용하였다.\n먼저 Local-path-provisioner 플러그인 설치 과정을 살펴볼 것이고, 로컬스토리지 이해를 위해 3가지의 케이스로 나뉘어 테스트를 진행하겠다.\nLocal-path-provisioner PV를 통한 고가용성 테스트 Hospath PV를 통한 고가용성 테스트 파드간 데이터 동기화 구성 local-path-provisioner 볼륨 프로비저닝 플러그인 중 하나로, 로컬 노드의 파일 시스템 경로를 사용하여 PVC(Persistent Volume Claim)를 만들어주는 역할을 한다. PVC를 통해 볼륨을 요청하면 PV가 자동으로 생성되어 연결된다고 보면 된다.\n설치시, 로컬 노드에 대한 볼륨 설정이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 curl -s -O https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml vim local-path-storage.yaml ---------------------------- apiVersion: apps/v1 kind: Deployment metadata: name: local-path-provisioner namespace: local-path-storage spec: replicas: 1 selector: matchLabels: app: local-path-provisioner template: metadata: labels: app: local-path-provisioner spec: # 추가 부분 nodeSelector: kubernetes.io/hostname: \u0026#34;마스터 노드 이름 입력 \u0026#34; tolerations: - effect: NoSchedule key: node-role.kubernetes.io/control-plane operator: Exists ... kind: ConfigMap apiVersion: v1 metadata: name: local-path-config namespace: local-path-storage data: config.json: |- { \u0026#34;nodePathMap\u0026#34;:[ { \u0026#34;node\u0026#34;:\u0026#34;DEFAULT_PATH_FOR_NON_LISTED_NODES\u0026#34;, \u0026#34;paths\u0026#34;:[\u0026#34;/data/local-path\u0026#34;] # 추가 부분 } ] } ---------------------------- Deployment / spec.spec 에서 nodeselector 와 tolerations 로 볼륨 배치 노드 설정(마스터 노드로 파드 배치) Configmap / data config.json에서 로컬 볼륨 PATH 설정 설치 확인\n1 2 3 4 kubectl get sc local-path ---------------------------- NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path rancher.io/local-path Delete WaitForFirstConsumer false 29m Case 1. Local-path-provisioner PV를 통한 고가용성 테스트 HostPath 볼륨이 IOPS 성능이 좋은 것을 앞서 확인하였다. 성능적으로 사용하기 좋은 볼륨이라 할 수 있으나 고가용성 구성이 필요하다.\nHostpath 볼륨의 문제점으로 노드간 마이그레이션에 자유롭지 못하기 때문이다. 이해를 위해 직접 예제 파드를 배포해보고 고가용성을 테스트 해보겠다.\nLocal-path-provisioner PV를 통한 고가용성 테스트\n앞서 배포한 Local-path-provisioner 를 통해 PV를 생성하여 파드를 배포하고 고가용성 구성을 테스트해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # 파드 예제 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: localpath-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi storageClassName: \u0026#34;local-path\u0026#34; --- apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: pod-persistent-volume mountPath: /data volumes: - name: pod-persistent-volume persistentVolumeClaim: claimName: localpath-claim 파드 배포 후 노드 드레인을 진행해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 배포 파드의 노드 확인 PODNODE=$(kubectl get pod -l app=date -o jsonpath={.items[0].spec.nodeName}) echo $PODNODE # 노드 드레인과 파드 모니터링 kubectl drain $PODNODE --force --ignore-daemonsets --delete-emptydir-data \u0026amp;\u0026amp; kubectl get pod -w --------------------------------- node/i-0c41dc0f6eeb01730 cordoned Warning: ignoring DaemonSet-managed Pods: kube-system/aws-node-w8bxm, kube-system/ebs-csi-node-thndq, kube-system/node-local-dns-v2hhc evicting pod default/date-pod-d95d6b8f-q9skb evicting pod kube-system/metrics-server-5f65d889cd-9btc7 pod/metrics-server-5f65d889cd-9btc7 evicted pod/date-pod-d95d6b8f-q9skb evicted node/i-0c41dc0f6eeb01730 drained NAME READY STATUS RESTARTS AGE date-pod-d95d6b8f-x5vrb 0/1 Pending 0 2m 노드 드레인시, 상태가 Pending 인 것을 확인할 수 있다. 이는 다른 노드에 PV볼륨이 없기 때문이다.\n마찬가지로 파드를 5개로 추가해도 볼륨이 있는 노드에만 파드가 올라온 것을 확인할 수 있다.\n1 2 # 예제 파드 개수 5개로 증가 kubectl scale deployment date-pod --replicas=5 Case2. HostPath를 통한 고가용성 테스트 볼륨 Hostpath로 노드 PATH를 직접 지정하여 고가용성을 테스트해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: log mountPath: /data volumes: - name: log hostPath: path: \u0026#34;/data\u0026#34; type: DirectoryOrCreate 파드 드레인과 개수 조절시 노드에 상관없이 배포가 진행된다.\n하지만, 볼륨별로 데이터가 쌓이는 것이 다르다. 각 노드에 들어가서 로그를 확인해보면 찍히는 것이 다른 것을 확인할 수 있다.\n고가용성이라 할 수 있지만 적재되어 있는 데이터가 달라 stateful 애플리케이션(ex. MySQL)을 운영하기엔 한계가 있다.\n참고 책에서는 이러한 제약사항을 해결하기 위해서는 애플리케이션 단에서 다른 노드의 파드와 데이터를 동기화해서 해결할 수 있다고 한다.\n동기화 방법을 찾아보니 NFS 볼륨(ex. AWS EFS)을 구성하여 HostPath 를 연결하거나, 볼륨간 rsync를 사용하라 나오지만, 성능(로컬SSD가 아님)이 떨어져 해결 방법은 아닌 것 같다.\nCase3. 파드간 데이터 동기화 구성 그렇다면 성능 좋고, 고가용성도 보장되고, 데이터 동기화를 보장할 수 있는 Stateful 애플리케이션을 구성할 수 있을까?\n쿠버네티스 공식문서 예제에서 이를 확인할 수 있는데 해당 예제를 구성해보고 테스트해보겠다. 해당 예제에서는 데이터베이스 리소스로 StatefulSet를 사용한다.\nStatefulSet 리소스는 이름처럼 Stateful한 애플리케이션을 위해 만든 리소스이다. StatefulSet 리소스의 특징은 다음과 같다.\nStatefulSet 리소스의 특징\nPod 이름\nStatefulSet에 의해 생성된 파드들은 {Pod 이름}-{순번} 식으로 이름이 정해진다. 이는 클러스터 내부 환경에서 데이터베이스에 접근할 때 사용하기 위해서이다.\n파드 순차적 배포\nPod 생성시 모든 Pod 가 동시에 생성되지 않고 순서대로 하나씩 생성된다. 이는 데이터베이스에서 마스터 파드 → 슬레이브 파드로 기동해야 하는 조건등에서 유용하게 사용 될 수 있다.\n파드별 볼륨 마운트\n일반적으로 PVC \u0026amp; PV에 중복적으로 Pod를 사용할 수 없다. 연결된 Pod가 존재하면 그 다음 파드들은 PVC를 얻지 못해 볼륨을 사용하지 못한다. 반면, Statefulset에서 PVC를 템플릿 형태로 정의하여 Pod마다 PVC, PV를 생성하여 파드별로 볼륨을 마운트할 수 있게 된다.\n다시 돌아가서 쿠버네티스 공식문서의 예제는 클러스터에 MySQL 스테이트풀셋이 배포되고 각 레플리카에 순서대로 배포되는 예제이다. 중요하게 볼 점은 스테이트 풀셋의 매니페스트이다.\n해당 StatefulSet 매니페스트는 3개의 replica를 가진 MySQL을 생성한다. init 컨테이너는 두개 배포되며, init-container는 MySQL init 설정을 수행하고, xtrabackup init-container는 MySQL 클러스터 복제를 위해 데이터를 클론하여 동기화를 진행한다. init 컨테이너 이후 MySQL 컨테이너는 데이터베이스 작업을 수행하며, xtrabackup 컨테이너는 클론 작업을 진행한다.\n공식 문서의 예제를 그대로 배포하면 스토리지 클래스가 default로 EBS 볼륨(외부)에 연결된다. 이대로 진행하면 local-path-provisioner에서 배포한 로컬 볼륨에서 마운트되지 않는다.\n로컬 볼륨에 마운트하기 위해서는 추가 작업이 필요하다.\nlocal-path-provisioner 배포 파일 수정\nlocal-path-provisioner 파드를 워크 노드에만 배포하도록 설정한다. 해당 설정을 통해 워크 노드에만 로컬 볼륨을 생성하고 파드에 연결할 수 있도록 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim local-path-storage.yaml ---------------------------- apiVersion: apps/v1 kind: Deployment metadata: name: local-path-provisioner namespace: local-path-storage spec: replicas: 1 selector: matchLabels: app: local-path-provisioner template: metadata: labels: app: local-path-provisioner spec: spec: # 아래 부분 추가 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/node operator: Exists 스테이트 풀셋의 매니페스트 내 스토리지클래스 지정 1 2 3 4 5 6 7 8 9 volumeClaimTemplates: - metadata: name: data spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: local-path # 로컬 볼륨 추가 resources: requests: storage: 10Gi 스테이트 풀셋의 매니페스트 replica count 를 워크노드(2) 개수 만큼 수정 1 2 3 serviceName: mysql replicas: 2 # 수정 template: 배포 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl apply -f ./ ---------------------------- configmap/mysql created service/mysql created service/mysql-read created statefulset.apps/mysql created kubectl get pods ---------------------------- NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 31m mysql-1 2/2 Running 0 31m 동기화 테스트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # 파드 0에서 Mysql Data 생성 kubectl exec -it pod/mysql-0 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, init-mysql (init), clone-mysql (init) --------------------------------- bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 140 Server version: 5.7.41-log MySQL Community Server (GPL) Copyright (c) 2000, 2023, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. --------------------------------- mysql\u0026gt; create database testdb; --------------------------------- Query OK, 1 row affected (0.01 sec) --------------------------------- mysql\u0026gt; use testdb; --------------------------------- Database changed --------------------------------- mysql\u0026gt; create table test(name varchar(10), testdata varchar(50)); --------------------------------- Query OK, 0 rows affected (0.02 sec) --------------------------------- mysql\u0026gt; insert into test values(\u0026#39;han\u0026#39;, \u0026#39;mysql example test\u0026#39;); --------------------------------- Query OK, 1 row affected (0.01 sec) --------------------------------- mysql\u0026gt; select * from test; --------------------------------- +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.00 sec) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 파드 1에서 확인 kubectl exec -it pod/mysql-1 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, init-mysql (init), clone-mysql (init) --------------------------------- bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 306 Server version: 5.7.41 MySQL Community Server (GPL) Copyright (c) 2000, 2023, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. --------------------------------- mysql\u0026gt; use testdb --------------------------------- Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed --------------------------------- mysql\u0026gt; select * from test; --------------------------------- +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.00 sec) 됐다! 로컬 볼륨 기반으로 Mysql 예제를 배포하였고 고가용성 구성을 위해 동기화까지 진행을 완료했다!\n로컬 볼륨 백업과 복원 쿠버네티스에서는 HostPath 볼륨의 대한 백업과 복원 기능은 지원하지 않는다. 노드 별로 백업 스크립트를 작성하거나 써드파티 솔루션을 사용해야 한다. 이번 절에서는 local-path-provisioner 을 사용해서 볼륨을 프로비저닝한만큼, 해당 볼륨에 맞게 백업을 할 수 있는 써드파트 솔루션을 찾아보았다.\nlocal-path-provisioner 깃허브 이슈를 찾아보니 Velero 솔루션을 이용해서 백업을 할 수 있다고 하여 테스트를 진행해보고자 한다.\nVelero? 는 쿠버네티스 클러스터의 리소스와 퍼시스턴트 볼륨을 백업하고 복원하는 데 사용되는 오픈 소스 툴이다.\nVelero 을 사용하기전 local-path-provisioner 볼륨 타입을 Local로 수정해야 한다. Hostpath 볼륨을 지원하지 않으나 Local 볼륨은 Restic 과 연계하여 백업을 지원하기 때문이다. (공식문서)\n이는 Local 볼륨이 쿠버네티스의 자원으로 관리되며, 스토리지 클래스와 퍼시스턴트 볼륨 클레임(PVC)을 사용할 수 있기 때문으로 보인다.\nlocal-path-provisioner 사전 작업\nLocal 볼륨을 수정하기 위해서는 local-path-provisioner 파드의 수정 작업이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 volumeClaimTemplates: - metadata: name: data annotations: volumeType: local # 추가 spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: local-path resources: requests: storage: 10Gi 로컬 볼륨 확인\n1 2 3 4 5 6 kubectl get pv \u0026lt;pv-name\u0026gt; -o yaml --------------------------------- spec: local: # local or HostPath 볼륨 path: /mnt/local-storage/ssd/vol1 ... Velero 설치\n필자는 Velero 백업 버킷을 AWS S3 설정하여 설치를 진행하였다.\n설치는 S3 버켓 생성 및 설정 / Veleo CLI 로 나뉜다.\nS3 버켓 지정 및 IAM 설정\nVelero 에서 S3 버킷을 접근하기 위한 IAM USER ID와 KEY 생성\n1 aws s3 mb s3://\u0026lt;bucket-name\u0026gt; --region ap-northeast-2 IAM Policy 생성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 # 버킷 변수 설정 export BUCKET=\u0026lt;bucket-name\u0026gt; # IAM Policy 생성 cat \u0026gt; velero-policy.json \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:DescribeSnapshots\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateVolume\u0026#34;, \u0026#34;ec2:CreateSnapshot\u0026#34;, \u0026#34;ec2:DeleteSnapshot\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${BUCKET}/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${BUCKET}\u0026#34; ] } ] } EOF # IAM Policy Attach aws iam put-user-policy \\ --user-name velero \\ --policy-name velero \\ --policy-document file://velero-policy.json # IAM user 정보 가져오기 aws iam create-access-key --user-name velero --------------------------------- { \u0026#34;AccessKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;velero\u0026#34;, \u0026#34;AccessKeyId\u0026#34;: \u0026#34;{ID}\u0026#34;, # 밑의 credentials-velero ID에 저장 \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;{KEY}\u0026#34;, # 밑의 credentials-velero KEY에 저장 \u0026#34;CreateDate\u0026#34;: \u0026#34;2023-03-16T04:31:23+00:00\u0026#34; } } # credentials-velero 생성 및 IAM 정보 저장 cat \u0026lt;\u0026lt; EOF \u0026gt; credentials-velero [default] aws_access_key_id=\u0026lt;AWS_ACCESS_KEY_ID\u0026gt; aws_secret_access_key=\u0026lt;AWS_SECRET_ACCESS_KEY\u0026gt; EOF Velero CLI 설치 후 서버 설치\nrestic은 버전 velero 1.10(최신버전) 이상에서 더 이상 지원되지 않는다. 버전을 1.9.6으로 맞춰서 다운받아야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # arch 확인 uname -m --------------------------------- x86_64 # velero CLI 설치 wget https://github.com/vmware-tanzu/velero/releases/download/v1.9.6/velero-v1.9.6-linux-amd64.tar.gz tar xzvf velero-v1.9.6-linux-amd64.tar.gz cp velero-v1.9.6-linux-amd64/velero ~/bin # CLI 확인 velero --------------------------------- Velero is a tool for managing disaster recovery, specifically for Kubernetes cluster resources. It provides a simple, configurable, and operationally robust way to back up your application state and associated data. If you\u0026#39;re familiar with kubectl, Velero supports a similar model, allowing you to execute commands such as \u0026#39;velero get backup\u0026#39; and \u0026#39;velero create schedule\u0026#39;. The same operations can also be performed as \u0026#39;velero backup get\u0026#39; and \u0026#39;velero schedule create\u0026#39;. ... Velero 설치\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 export BUCKET=\u0026lt;bucket-name\u0026gt; export REGION=ap-northeast-2 velero install \\ --provider aws \\ --bucket $BUCKET \\ --secret-file ./credentials-velero \\ --backup-location-config region=$REGION \\ --use-volume-snapshots=false \\ --plugins velero/velero-plugin-for-aws:v1.3.0 \\ --use-restic --------------------------------- ... Deployment/velero: created DaemonSet/restic: attempting to create resource DaemonSet/restic: attempting to create resource client DaemonSet/restic: created Velero is installed! ⛵ Use \u0026#39;kubectl logs deployment/velero -n velero\u0026#39; to view the status. # Velero 확인 kubectl get all -n velero NAME READY STATUS RESTARTS AGE pod/restic-f5ngz 1/1 Running 0 38s pod/restic-x9sk9 1/1 Running 0 37s pod/velero-5f6657d4c8-jttxv 1/1 Running 0 38s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/restic 2 2 2 2 2 \u0026lt;none\u0026gt; 38s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/velero 1/1 1 1 38s NAME DESIRED CURRENT READY AGE replicaset.apps/velero-5f6657d4c8 1 1 1 38s Velero 백업\nVelero는 Restic을 사용하여 PV 볼륨에 대해 백업하는 방법에 두 가지 접근 방식을 지원한다. (공식문서)\n옵트인 접근 방식(default): Restic을 사용하여 백업할 볼륨이 포함된 모든 포드에 annotation을 달아야 한다. 옵트아웃 접근 방식: 모든 포드 볼륨이 Restic을 사용하여 백업되고 백업되지 않아야 하는 볼륨을 옵트아웃할 수 있는 기능이 있다. 이번 절에서는 옵트인 접근 방식을 택할 것이고 Case3 의 Mysql 볼륨을 백업할 예정이다. 백업할 볼륨에 대해 backup.velero.io/backup-volumesannotation 달고 백업을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 주석 추가 pod에 볼륨 정보를 추가 kubectl annotate pod/mysql-0 backup.velero.io/backup-volumes=data # 백업 velero backup create mysql --include-namespaces default --wait --------------------------------- Backup request \u0026#34;mysql\u0026#34; submitted successfully. Waiting for backup to complete. You may safely press ctrl-c to stop waiting - your backup will continue in the background. .................. Backup completed with status: Completed. You may check for more information using the commands `velero backup describe mysql` and `velero backup logs mysql`. # 백업 목록 확인 velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-16 14:22:39 +0900 KST 29d default \u0026lt;none\u0026gt; S3 버킷 조회할 수 있다.\nS3 조회시, backups(쿠버네티스 리소스 저장 경로) 와 restic(PV 볼륨 데이터 저장 경로)에 데이터를 확인할 수 있다.\nVelero 복원\nmysql 배포 파일과 PV를 지우고 복원을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #mysql 지우기 kubectl delete -f ./ kubectl delete pvc/\u0026lt;PVC 볼륨\u0026gt; #velero 복원 velero restore create --from-backup mysql --wait --------------------------------- Restore request \u0026#34;mysql-20230316155542\u0026#34; submitted successfully. Waiting for restore to complete. You may safely press ctrl-c to stop waiting - your restore will continue in the background. ........... Restore completed with status: Completed. You may check for more information using the commands `velero restore describe mysql-20230316155542` and `velero restore logs mysql-20230316155542`. # 쿠버네티스 리소스 복원 확인 kubectl get all --------------------------------- NAME READY STATUS RESTARTS AGE pod/mysql-0 2/2 Running 0 39s pod/mysql-1 0/2 Init:CrashLoopBackOff 2 (18s ago) 39s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 4h39m service/mysql ClusterIP None \u0026lt;none\u0026gt; 3306/TCP 39s service/mysql-read ClusterIP 100.69.52.194 \u0026lt;none\u0026gt; 3306/TCP 39s kubectl get pv --------------------------------- NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-601b919a-cf20-4478-9f28-10d541c66844 10Gi RWO Delete Bound default/data-mysql-0 local-path 71s pvc-b8a766a6-411f-47df-a548-d6b0ee091ea1 10Gi RWO Delete Bound default/data-mysql-1 local-path 70s # Mysql data 확인 kubectl exec -it pod/mysql-0 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, restic-wait (init), init-mysql (init), clone-mysql (init) bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 73 Server version: 5.7.41-log MySQL Community Server (GPL) mysql\u0026gt; use testdb; --------------------------------- Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed --------------------------------- mysql\u0026gt; select * from test; +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.01 sec) 데이터가 그대로 보존되어 있다!\nVelero 스케쥴 백업\n크론탭처럼 백업도 Velero가 가능하다. 다음은 5분마다 백업을 진행하는 예제이다.\n백업된 오브젝트별 데이터 변화를 위해 데이터베이스의 데이터를 수정 후 복원을 해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 velero schedule create mysql-crontab --include-namespaces default --schedule=\u0026#34;*/5 * * * *\u0026#34; velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-16 15:51:44 +0900 KST 29d default \u0026lt;none\u0026gt; mysql-crontab-20230316072517 Completed 0 0 2023-03-16 16:25:17 +0900 KST 29d default \u0026lt;none\u0026gt; mysql-crontab-20230316072017 Completed 0 0 2023-03-16 16:20:17 +0900 KST 29d default \u0026lt;none\u0026gt; #DB 접속 후 데이터 삭제 select * from test; +------+---------------------+ | name | testdata | +------+---------------------+ | han | mysql example test | | han | mysql example test2 | | han | mysql example test3 | +------+---------------------+ 3 rows in set (0.00 sec) mysql\u0026gt; delete from test; Query OK, 3 rows affected (0.00 sec) mysql\u0026gt; select * from test; Empty set (0.00 sec) #velero 복원 velero restore create --from-backup mysql-crontab-20230316072017 --wait --------------------------------- Restore request \u0026#34;mysql-crontab-20230316072017-20230316163030\u0026#34; submitted successfully. Waiting for restore to complete. You may safely press ctrl-c to stop waiting - your restore will continue in the background. Restore completed with status: Completed. You may check for more information using the commands `velero restore describe mysql-crontab-20230316072017-20230316163030` and `velero restore logs mysql-crontab-20230316072017-20230316163030`. # Mysql DATA 확인 mysql\u0026gt; select * from test; Empty set (0.00 sec) Mysql DATA 확인 의 결과가 예상과 다르다. 백업 진행 이후 백업 데이터인 행3개가 있어야 하나, 최신 데이터가 조회된다.\n이는 Mysql 파드가 2개 있어 파드간 데이터 무결성이 보장되어 백업 데이터 파일을 옮긴다 한들 수정이 안되기 때문이다. 따라서 백업본의 결과를 얻기 위해서는 PV 볼륨과 Mysql 리소스를 지우고 다시 복원을 해야한다.\nVelero 클러스터 마이그레이션\n클러스터간 마이그레이션 방법으로 Velero를 활용할 수 있다. 공식문서를 참고하여 테스트를 진행해보겠다.\n진행 전 Velero 에서 클러스터 마이그레이션시 제약사항이 있으니 확인이 필요하다.\nVelero는 기본적으로 클라우드 공급자 간에 PV 스냅샷의 마이그레이션을 지원하지 않는다. 이를 위한 방안으로 restic을 이용하여 파일시스템 레벨의 마이그레이션을 진행해야 한다. Velero는 백업이 수행된 위치보다 낮은 Kubernetes 버전이 있는 클러스터로의 복원을 지원하지 않는다. 동일한 버전의 Kubernetes를 실행하지 않는 클러스터 간에 워크로드를 마이그레이션하는 것이 가능할 수 있지만 마이그레이션 전에 각 사용자 정의 리소스에 대한 클러스터 간 API 그룹의 호환성을 포함하여 몇 가지 요인을 고려해야 한다. AWS 및 Azure용 Velero 플러그인은 리전 간 데이터 마이그레이션을 지원하지 않는다. 이를 위한 방안으로 restic을 사용해야 한다. 우리는 restic을 사용하니 제약사항에 자유롭다. 클러스터 마이그레이션의 예에 대한 일환으로 클러스터를 재 구축하여 앞서 생성한 Mysql 애플리케이션을 복원해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # velero 설치 export BUCKET=hanhorang-velero-s3 export REGION=ap-northeast-2 velero install \\ --provider aws \\ --bucket $BUCKET \\ --secret-file ./credentials-velero \\ --backup-location-config region=$REGION \\ --use-volume-snapshots=false \\ --plugins velero/velero-plugin-for-aws:v1.3.0 \\ --use-restic --------------------------------- velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-17 00:31:21 +0900 KST 29d default \u0026lt;none\u0026gt; velero 백업 개체가 그대로 있다. 복원 과정은 앞 과정과 동일하기에 생략하였다. 클러스터를 재구축해도 Velero 백업 객체가 남아있는 이유가 무엇일까?\nVelero 리소스는 오브젝트 스토리지의 백업 파일과 동기화되기 때문이다. 설치 과정에서 삭제한 클러스터와 새로운 클러스터의 Velero 버킷이 동일하므로 백업 객체가 그대로 있음을 확인하였다. 클러스터1과 클러스터2가 병행 운영시에도 버킷 데이터에 따라 Velero 리소스가 동기화가 이루어진다는데 기본 동기화 간격이 1분으로 이 부분을 확인하여 마이그레이션을 진행하면 될 것 같다.\n로컬 볼륨 모니터링 PV 볼륨 성능 확인할 수 있는 krew df-pv 도구가 있으나, HostPath 볼륨은 인스토어스토어라서 확인되지 않는다. 하지만 Local 볼륨은 확인이 가능하다.\n1 2 3 4 5 kubectl krew df-pv \u0026amp;\u0026amp; kubectl df-pv --------------------------------- PV NAME PVC NAME NAMESPACE NODE NAME POD NAME VOLUME MOUNT NAME SIZE USED AVAILABLE %USED IUSED IFREE %IUSED pvc-678e7407-9f76-4fd1-a9ad-8c2581b8df36 data-mysql-0 default i-0314088c74eee3276 mysql-0 data 123Gi 4Gi 119Gi 3.68 119172 16395900 0.72 pvc-d2c3cbcb-13ec-46de-81e9-1178d25dd4ad data-mysql-1 default i-0ab66ac834dc8710d mysql-1 data 123Gi 4Gi 119Gi 3.74 121203 16393869 0.73 성능 측정 로컬 스토리지의 성능 측정 방법으로 iostat 명령어와 krew 툴인 kubestr을 사용하여 성능을 측정하겠다.\nkubestr 스토리지 IOPS 측정 툴이다. 스토리지 사용에 따른 검증용으로 사용하기 좋은 툴인 것 같다. 예제도 많으니 링크를 통해 확인하자. 이번 예제에서는 실습 참고용 책에서 제공해주신 예제 스크립트를 통해 성능을 측정할 것이다.\nfio-read.fio\nfio를 사용하여 4KB 블록 크기를 가지는 랜덤 읽기 및 쓰기\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [global] ioengine=libaio direct=1 bs=4k runtime=120 time_based=1 iodepth=16 numjobs=4 # numjobs=16 size=1g group_reporting rw=randrw rwmixread=100 rwmixwrite=0 [read] ioengine=libaio : Asynchronous I/O를 수행하기 위해 libaio 라이브러리를 사용합니다. direct=1 : Direct I/O를 사용합니다. bs=4k : I/O 요청에 사용되는 블록 크기는 4KB입니다. runtime=120 : 120초 동안 작업을 실행합니다. time_based=1 : 시간 기반으로 작업을 수행합니다. iodepth=16 : 각 작업에 대한 I/O 요청 수를 16개로 설정합니다. numjobs=4 : 4개의 작업을 수행합니다. size=1g : 각 작업에 대한 데이터 크기는 1GB입니다. group_reporting : 모든 작업 결과를 통합하여 보고합니다. rw=randrw : 랜덤 읽기 및 쓰기 작업을 수행합니다. rwmixread=100 : 작업 중 읽기 작업의 비율은 100%입니다. rwmixwrite=0 : 작업 중 쓰기 작업의 비율은 0%입니다. fio-write.fio\n루트 디렉토리에서 4KB 블록 크기로 16개의 job이 16개의 i/o depth로 실행되며, 실행 시간이 120초인 1GB 파일에 대해 100% 쓰기 랜덤 테스트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [global] ioengine=libaio numjobs=16 iodepth=16 direct=1 bs=4k runtime=120 time_based=1 size=1g group_reporting rw=randrw rwmixread=0 rwmixwrite=100 directory=/ [read] rwmixwrite=100: 100% 쓰기 테스트를 수행합니다. directory=/: 테스트할 디렉토리를 루트 디렉토리로 설정합니다. 성능 측정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubestr fio -f fio-write.fio -s local-path --size 10G --------------------------------- PVC created kubestr-fio-pvc-hp69m Pod created kubestr-fio-pod-fcvcp Running FIO test (fio-write.fio) on StorageClass (local-path) with a PVC of Size (10G) Elapsed time- 4m11.664545514s FIO test results: FIO version - fio-3.30 Global options - ioengine=libaio verify= direct=1 gtod_reduce= JobName: blocksize= filesize= iodepth= rw= write: IOPS=3023.577881 BW(KiB/s)=12094 iops: min=992 max=8640 avg=3023.464355 bw(KiB/s): min=3968 max=34564 avg=12093.908203 Disk stats (read/write): nvme0n1: ios=0/362587 merge=0/173 ticks=0/6330627 in_queue=6330627, util=99.954132% fio-write 실행 결과, 쓰기 평균 iops가 3034인 것을 확인할 수 있다.\nQ. 테스트가 안될 경우, PV 상태 Pending?\n해당 경우는 PVC 요청에 맞는 볼륨의 PV가 없을때 발생한다. PVC 요청에 맞는 볼륨이 있는지 또는 Local-path-provisioner 설정을 확인하자. 필자의 경우 Local-path-provisioner 설정으로 Pending 이 발생했다.\n1 2 3 4 kubectl get pvc -A --------------------------------- NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default kubestr-fio-pvc-dl7vx Pending local-path 4m34 노드 볼륨 IO 성능 측정 iostat 명령어를 통해 실시간으로 스토리지의 성능과 사용량에 관한 정보를 확인할 수 있다. 클러스터 운영시 스토리지 트러블슈팅의 일환으로 사용하자.\n아래 예제는 fio-write 스크립트 실행 중 iostat 를 실행하여 스토리지 정보를 확인한 예제이다.\n1 2 3 4 5 6 7 # iostat 패키지 설치 sudo apt install -y sysstat iostat -xmdz 1 -p nvme2n1 --------------------------------- Device r/s rMB/s rrqm/s %rrqm r_await rareq-sz w/s wMB/s wrqm/s %wrqm w_await wareq-sz d/s dMB/s drqm/s %drqm d_await dareq-sz aqu-sz %util nvme2n1 0.00 0.00 0.00 0.00 0.00 0.00 24.00 0.10 2.00 7.69 0.75 4.33 0.00 0.00 0.00 0.00 0.00 0.00 0.02 4.00 w/s: 초당 쓰기 요청 수 wMB/s: 초당 쓴 데이터 양 (메가바이트/초) wrqm/s: 초당 쓰기 요청 큐에 들어간 요청 수 %wrqm: 쓰기 요청 큐에 들어간 요청 비율 w_await: 쓰기 요청 대기 시간, 드라이버 요청 대기열에서 기다린 시간과 장치의 I/O 응답시간을 모두 포함한다. (밀리초) wareq-sz: 평균 쓰기 요청 크기 (섹터) aqu-sz: 요청 대기열의 평균 길이 %util: 디스크 사용률 (0 ~ 100%) 로컬 스토리지 QnA Q1. AWS 인스토어 스토어에 대한 볼륨 스토리지 조절이 가능한가?\n인스턴스 스토어는 EC2 인스턴스의 로컬 디스크를 사용하는 것이기 때문에 크기 조정이 불가능하다. 인스턴스 스토어를 사용하는 EC2 인스턴스를 변경하거나 새로운 인스턴스를 시작하여 크기를 조정해야 한다. 일반적으로 DB(Mysql) 볼륨 사용량으로 10G~100G을 설정한다고 하지만, 애플리케이션 규모와 기간에 따라 사용량 예측이 힘들다. 필요시 백업을 통해 인스토어 스토어 볼륨을 변경할 수 있도록 하자. EC2 인스턴스에 따른 볼륨(SSD) 는 링크에서 확인이 가능하다.\nQ2. hostpath 볼륨을 여러개의 파드가 동시에 사용할 수 있을까?\n노드의 파일 시스템은 여러 개의 프로세스가 동시에 접근할 수 있는 공유 리소스이기 때문에 여러 개의 파드가 하나의 hostpath를 사용할 수 있다. 하지만, 데이터 손상이나 권한 오류가 발생할 수 있다. 하나의 파드가 파일을 쓴 후에 다른 파드가 동일한 파일을 읽을 경우나, 여러 개의 파드가 동시에 동일한 파일을 쓰는 경우가 있기 때문이다. 이를 위해 적절한 동기화 및 락 메커니즘을 구현이 필요하다.\n테스트로 스터디에서 공유해주신 localpath-fail.yaml 를 수정해서 로그를 확인해봤다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 0.01; done\u0026#34;] # 0.01 로 수정 volumeMounts: - name: pod-persistent-volume mountPath: /data volumes: - name: pod-persistent-volume persistentVolumeClaim: claimName: localpath-claim 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 파드 10개로 증가후 테스트 kubectl scale deployment date-pod --replicas=10 -------------------------------------- # 로그 확인 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:05 | wc -l 355 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:05 | wc -l 355 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:06 | wc -l 457 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:07 | wc -l 511 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:08 | wc -l 513 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:09 | wc -l 530 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:10 | wc -l 506 0.01초 x 10 개 파드로 1초당 약 1000개의 결과가 나와야 하지만 350~530 개의 결과가 나오는 것을 확인하였다. 앞서 kubestr 측정에서 쓰기 IOPS를 측정하여 약 3000이 나왔음에도 턱없이 부족한 것을 알 수 있다.\nQ3. Velero 의 백업 용량은 몇 인가?\n사용 버킷에 따라 달라진다. S3 버킷 기준으로는 해당 버킷의 용량에 따라가는데 최대 5테라까지 지원이 가능하다.\n","date":"Mar 18","permalink":"https://HanHoRang31.github.io/post/pkos2-2-localstorage/","tags":["KANS","kops","cloud","AWS","kubernetes","Volume","velero","local-path-provisioner"],"title":"[PKOS] 쿠버네티스 로컬스토리지와 Velero를 통한 백업 테스트"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)님이 진행하시며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. PKOS 1주차 스터디 내용과 느낌점을 정리하고자 한다. 스터디하면서 매번 느끼는 거지만 정말 괴수분들 너무 많고, 배울 점이 정말 많다… 특히 모임장님이신 가시다님의 스터디 내용은 볼 때마다 감탄만 나온다. 쿠버네티스에 대한 원리와 세부 컴포넌트에 대한 명령어까지 배운 점이 많다. 본 글에서는 필자가 배운 내용을 정리함과 동시에 개념에 대해 몰랐던 부분이나, 트러블슈팅에 대한 내용을 중점적으로 정리하였다.\n추가로, 몰랐던 부분은 ChatGPT를 활용하여 작성하였다. ChatGPT가 주는 답변은 대체로 만족하지만, 공식 문서에 대한 내용과 비교하여 다른 내용이 일부 존재한다. ChatGPT 활용시, 공식 문서와 이중 검증이 필요하다고 본다. 본 글에서도 답변 내용을 일부 수정하여 작성한다.\nkops? kops는 쿠버네티스 클러스터를 생성, 업그레이드, 관리하는 데 사용되는 오픈 소스 도구이다. 명령어 툴로 쉽게 쿠버네티스 클러스터를 구성하고 관리할 수 있는 툴이라고 이해하면 되겠다. 쿠버네티스 클러스터의 인프라를 코드로 정의하고 관리하는데 사용할 수 있어 IaC 이며, 같은 IaC 툴인 Terraform(테라폼)과의 특징을 비교하면 다음과 같다.\nkops과 Terraform 특징 비교 표\n특징 kops Terraform 지원하는 클라우드 플랫폼(Provider) AWS, GCP, OpenStack, DigitalOcean 등 AWS, GCP, Azure, Oracle Cloud, Alibaba Cloud, VMware, OpenStack 등 관리 대상 Kubernetes 클러스터 인프라스트럭처 (서버, 네트워크, 데이터베이스 등) 코드 작성 방식 YAML 파일 HCL (HashiCorp Configuration Language) 상태 관리 상태를 지정된 저장소(state)에 저장 상태를 지정된 저장소(backend)에 저장 장점 간단하고 직관적인 클러스터 구성 AWS뿐만 아니라 다른 클라우드 프로바이더도 지원 단점 Kubernetes 클러스터만 지원 Kops에 비해 배우기가 어려움 그렇다면 kops는 언제 써야 할까?\n모임장님 의견과 동일하게 교육용이 적합하다고 본다. 쿠버네티스 클러스터 구축이 간단하고 배우기 쉽다. 또한, 배포 속도도 빠르다. 쿠버네티스 관리형 서비스인 EKS 와 비교했을 때, 마스터 노드들을 세부적으로 알 수 있어 세부 원리 이해에 좋고, 비용도 저렴하다. kops 공식문서도 정리가 잘 되어 있다. 눈여겨 볼 점은 공식문서의 addon과 Operation 부분이다. 클러스터 관리를 및 addon 배포를 간단하게 테스트할 수 있고 배포 yaml를 확인할 수 있기 때문이다.\n실습 스터디 내용으로 kops를 통해 AWS에 쿠버네티스 클러스터를 구축하고, 게임 마리오를 예제로 배포하였다. 과정은 크게 3가지로 진행하였다.\n베스천 서버(kops-ec2) 구성 kops 를 통한 클러스터 구축 및 확인 External DNS 와 게임, 슈퍼마리오 배포 실습 과정 전 사전 작업으로 퍼블릭 도메인 구입, 키 페어 생성, S3 버킷 생성, AWS IAM 자격 증명을 진행하였다. 사전 작업 내용은 공식 문서에서 참고가 가능하다.\nQ. 퍼블릭 도메인 구입 이유 ?\n퍼블릭 도메인은 쿠버네티스 클러스터 이름으로 사용하기 위하여 구입하였다. 클러스터 이름을 도메인으로 설정하면 외부에서 서비스 디스커버리 및 클러스터 액세스가 쉽게 가능해지기 때문이다. 스터디에서는 해당 도메인을 통해 클러스터 구성 확인과 게임 배포 후 접근을 위해 사용하였다.\n1. 베스천 서버(kops-ec2) 구성 베스천 서버 구성은 모임장님이 공유해주신 CloudFormation 템플릿을 통해 구성하였다. 템플릿 구성은 VPC 와 igw 구성같은 AWS 네트워크 구성과 EC2 서버 설정으로 되어있다. 그 중 EC2 서버 설정 스크립트를 확인할 필요가 있는데 kops와 필요 패키지를 같이 설치해주기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash # 호스트 이름 변경 hostnamectl --static set-hostname kops-ec2 # EC2 서버 시간을 서울로 변경 ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # Install Packages cd /root yum -y install tree jq git htop ## kubectl 설치 curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl ## kops 설치 curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d \u0026#39;\u0026#34;\u0026#39; -f 4)/kops-linux-amd64 chmod +x kops mv kops /usr/local/bin/kops ## awscli 설치 curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install ## 환경 변수 설정 export PATH=/usr/local/bin:$PATH source ~/.bash_profile ## aws cli 자동 완성 설정 complete -C \u0026#39;/usr/local/bin/aws_completer\u0026#39; aws ## SSH 설정 ssh-keygen -t rsa -N \u0026#34;\u0026#34; -f /root/.ssh/id_rsa ## vi -\u0026gt; vim 으로 설정 echo \u0026#39;alias vi=vim\u0026#39; \u0026gt;\u0026gt; /etc/profile ## root 계정 변환 echo \u0026#39;sudo su -\u0026#39; \u0026gt;\u0026gt; /home/ec2-user/.bashrc ## helm 설치 curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash wget https://github.com/andreazorzetto/yh/releases/download/v0.4.0/yh-linux-amd64.zip unzip yh-linux-amd64.zip mv yh /usr/local/bin/ 베스천 서버 구성 과정 중 에러 발생 시, AWS Cloudformation에서 로그를 확인할 수 있다. 필자는 VPC가 최대여서 베스천 서버 구성이 안되었다. 해결을 위해 필요없는 VPC를 삭제하고, 템플릿으로 생성한 Cloudformtation 스택을 삭제하고 재실행하였다.\n2. kops를 통한 클러스터 구축 및 확인 쿠버네티스 클러스터 구축은 kops 명령어로 구축하였다. 클러스터 생성에 대한 옵션은 공식 문서에서 참고할 수 있었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 클러스터 생성 bash ## export KOPS_STATE_STORE=s3://( 클러스터 관리 저장소로 활용할 버킷 이름) ## export KOPS_CLUSTER_NAME=\u0026lt;도메인 메인 주소\u0026gt; ## export REGION=ap-northeast-2 지역 kops create cluster \\ --cloud aws \\ --name $KOPS_CLUSTER_NAME \\ --state s3://$KOPS_STATE_STORE \\ --zones \u0026#34;$REGION\u0026#34;a,\u0026#34;$REGION\u0026#34;c \\ --networking amazonvpc \\ --network-cidr 172.30.0.0/16 \\ --kubernetes-version \u0026#34;1.24.10\u0026#34; \\ --node-count 2 \\ --node-size t3.medium \\ --master-size t3.medium \\ --ssh-public-key ~/.ssh/id_rsa.pub \\ -y --state : 클러스터 관련 설정 파일들이 저장된다. 이렇게 저장된 설정 파일들은 나중에 클러스터를 업데이트하거나 삭제할 때 사용된다. 실습에서는 외부 스토리지인 S3를 이용하였다. —state 플래그를 사용하지 않으면 로컬 저장소인 ~/.kops 에 클러스터 설정 파일이 저장된다. 하지만 로컬 시스템에 대한 의존성이 높아지고 협업시에 대한 공유성이 떨어져 외부 스토리지를 사용하는 것을 권장한다. 필자는 클러스터 생성시 VPC 개수 이유(error creating VPC: VpcLimitExceeded) 로 클러스터 생성이 되지 않았다.\n재설치를 위해서는 기존 클러스터 삭제가 필요하다. 실습 내용의 구성 단계에 따라 삭제하면 된다.\n1 2 3 4 5 1. EC2 Auto Scaling 그룹 삭제 2. EC2 시작 템플릿 Launch Templates 삭제 3. S3 버킷 비우기 4. Route53에 추가된 A 레코드 3개 삭제 5. CloudFormation 삭제 VPC 문제는 S3 버킷만 지우면 됐었다. Cloudformtation 스택 생성시 에러가 발생하여 AWS 리소스들은 생성되지 않았기 때문이다. 아래는 kops 를 통한 클러스터 구성 과정을 정리하였다\n1 2 3 4 5 6 7 8 1. kops create cluster 명령어 실행 2. 클러스터 구성 정보를 S3에 저장 3. 클러스터 구성 정보를 기반으로 CloudFormation 스택 생성 4. VPC 및 관련 리소스 생성 5. 마스터 노드 EC2 인스턴스 생성 6. 노드 그룹 EC2 인스턴스 생성 7. 노드 그룹 EC2 인스턴스가 마스터 노드를 참조하여 클러스터에 가입 8. 클러스터가 실행되고 kubectl을 통해 액세스 가능해짐 클러스터 구성 확인 확인\n스터디 실습 내용으로 클러스터 구성 확인에 대한 것도 시간을 할당하여 확인하였다. 쿠버네티스 클러스터가 복잡한 만큼 확인할 것이 많았는데, 실습 내용을 참고로 하여 명령어를 정리해보았다.\n클러스터 도메인 확인\n클러스터 구성시 클러스터 이름을 퍼블릭 도메인으로 입력하였다. 필자는 퍼블릭 도메인을 AWS Route53 에서 구매하였는데, 이 같은 경우 route53에서 A레코드도메인이 추가된 것을 확인할 수 있다.\n클러스터 구성 정보 확인\n클러스터 구성 정보는 kubectl 와 kops 툴로 확인이 가능하다. kops 툴을 통해 클러스터 정보 뿐만 아니라 이미지 확인(assets), 보안 정보 확인이 가능하다. 공식문서를 링크한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 클러스터 확인 kops get cluster # 클러스터 인스턴스 그룹 확인 kops get ig # 클러스터 인스턴스 확인 kops get instances # 클러스터 접근 정보 확인 kubectl get nodes -v6 # 클러스터 배포 파드 확인 kubectl get pods -A # 클러스터 정보 확인 k**ubectl cluster-info dump** 클러스터 세부 구성 확인\n클러스터 세부 구성 확인으로 스토리지, 네트워크, 파드, 마스터 노드 컴포넌트들의 구성 정보를 확인하였다. 추후 참고용을 위해 정리해둔다. 퍼블릭 도메인을 이용하여 구성 정보를 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # storage class 확인 kubectl get sc # 노드별 스토리지 확인 lsblk df -hT # 네트워크 확인 sudo iptables -t nat -S # 마스터 노드 컴포넌트 확인 **tree /etc/kubernetes/manifests/ # kubelet 작동 확인 systemctl status kubelet # 컨테이너 접근 방법 확인 ps axf | grep /usr/bin/containerd # ec2 메타데이터 확인 - IAM role 확인** TOKEN=`curl -s -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34;` echo $TOKEN curl -s -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; –v http://169.254.169.254/latest/meta-data/iam/security-credentials/\u0026lt;도메인\u0026gt; | jq kubectl get sc : kops로 클러스터 구성시 스토리지 CSI 가 기본 제공된다. 스토리지 CSI 는 클러스터 스토리지 관리 기능 플러그인이다. kops-csi 와 kops-ssd 가 존재하는데 kops-csi 는 AWS EBS(gp3), EFS, GCP 등의 스토리지를 사용할 수 있는 플러그인이고, kops-ssd 는 NVMe SSD 기반 인스턴스 스토리지를 사용할 수 있는 플러그인이다.\nsudo iptables -t nat -S : Kubernetes에서 사용하는 iptables rule 을 확인할 수 있다. 이번 장에서 구성한 네트워크 CNI는 VPC로 노드 IP와 서비스 IP의 할당별 iptables rule을 확인할 수 있다.\n파드 IP 설정 iptables rule\n임의의 애플리케이션 파드 IP를 확인했다. 체인을 따라가보면 들어오는 패킷 중 목적지 IP 주소가 100.64.59.121/32이고, 프로토콜이 TCP이며, 목적지 포트가 80인 패킷이 172.30.41.131:8080(파드 할당 IP) 으로 가는 것을 확인 할 수 있다. 노드 IP 설정 iptables rule\n임의의 노트 IP를 확인했다. 노드 IP에 대해NAT 규칙을 적용하는 것을 확인할 수 있었다. tree /etc/kubernetes/manifests/ : 마스터 노드에서 작동하는 컴포넌트 mainfest 정보들이다. kubelet에 의해 Static Pod로 배포되어 있으며 API 서버의 명령어(kubectl apply, delete) 등으로 관리가 불가능하다. kubelet을 통해 아래 컴포넌트의 manifest 를 모니터링하고 있으며 manifest 수정시 자동으로 업데이트된다.\n1 2 3 4 5 6 7 /etc/kubernetes/manifests/ ├── etcd-events-master-ap-northeast-2a.manifest ├── etcd-main-master-ap-northeast-2a.manifest ├── kube-apiserver.manifest ├── kube-controller-manager.manifest ├── kube-proxy.manifest └── kube-scheduler.manifest 3. External DNS, 게임 마리오 배포 실습 예제로는 게임 마리오를 배포하였다. External DNS은 쿠버네티스 addon으로, 내부에서 동작하는 마리오 서비스의 IP 주소를 외부의 DNS 서버에 자동으로 등록시켜주기 위해 배포하였다. 인상 깊은 점은 addon인 ExternalDNS 배포였는데, kops를 통해 클러스터 정보를 수정하니 자동으로 배포되었다.\n공식 문서1, 공식 문서2 에는 클러스터 스펙 수정시의 옵션이 잘 정리되어 있다. 추후 기능 테스트시 접근을 위해 남겨둔다.\n1 2 3 4 5 6 7 8 9 10 11 12 # 클러스터 수정 kops edit cluster # 아래 spec 에 ExternalDNS 정보 추가** -------------------------- spec: externalDns: provider: external-dns -------------------------- # 클러스터 업데이트 후, 롤링 업데이트 진행 kops update cluster --yes \u0026amp;\u0026amp; echo \u0026amp;\u0026amp; sleep 3 \u0026amp;\u0026amp; kops rolling-update cluster 트러블슈팅 API 서버 접근시, 아래의 에러 메세지(couldn’t get current server API group list: ~ dial tcp connect refused )가 확인되었다.\n원인은 클러스터 접근 토큰 만료였다. kops 명령어를 통해 자격 증명을 재발급(kops export kubeconfig) 받을 수 있다하여 시도해보았지만, 쿠버네티스 유저가 없어서 그런지 접근 토큰이 null 로 발급되었다. kops 클러스터 관리 저장소인 s3에도 접근 토큰 정보가 없었다. 필자는 마스터 노드에 접근(~/.kube/config)하여 토큰을 복사하니 해결하였다.\n마치며 kops 간단하다! 특히 마스터 노드를 직접 접근하여 컨트롤할 수 있어 클러스터 동작 이해에 좋은 툴이였다. 공식문서도 참고할 것이 많다. 공식 문서 kops operation 에 재밌는 주제들(카펜터)이 많던데 얼른 테스트해보고 싶은 마음이다. 시간나는대로 정리해서 올려보겠다.\n","date":"Mar 10","permalink":"https://HanHoRang31.github.io/post/pkos2-1-kops/","tags":["KANS","kops","cloud","AWS","kubernetes"],"title":"[PKOS] Kops로 클러스터 구축하기"},{"categories":null,"contents":"","date":"Jan 01","permalink":"https://HanHoRang31.github.io/articles/","tags":null,"title":"Articles"}]