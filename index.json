[{"categories":null,"contents":"","date":"Mar 05","permalink":"https://HanHoRang31.github.io/projects/a_project/","tags":null,"title":"HanHoRang Git Repo"},{"categories":null,"contents":" 1 2 AWS EKS Workshop Study (=AEWS)는 EKS Workshop 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며,공개된 AWS EKS Workshop을 기반으로 진행하고 있습니다. EKS ? Amazon Elastic Kubernetes Service(EKS)는 AWS에서 제공하는 관리형 Kubernetes 서비스다. EKS를 사용하면 Kubernetes 클러스터를 생성, 운영 및 유지 관리할 수 있다.\nhttps://catalog.us-east-1.prod.workshops.aws/workshops/9c0aa9ab-90a9-44a6-abe1-8dff360ae428/ko-KR/10-intro/200-eks\nEKS의 주요 특징은 다음과 같다.\n관리형 서비스: EKS는 Kubernetes 컨트롤 플레인이나 데이터 플레인를 설치, 운영 및 유지 관리시켜주는 서비스이다. 사용자가 인프라 설치, 운영, 유지 관리를 할 필요가 없다. 높은 가용성: EKS는 여러 AWS 가용 영역(데이터 센터의 물리적 위치)에 걸쳐 컨트롤 플레인과 데이터 플레인를 분산시켜 서비스의 안정성을 제공한다. 이는 단일 장재 지점을 제거하도록 설계되었으며 자동 관리되는 컨트롤 플레인의 경우 리전 내 개별 가용 영역에서 최소 2개이상의API 서버 노드를 실행한다. AWS 서비스 통합: 타 AWS ECR(도커 레지스트리), ELB(네트워크 로드밸런싱), IAM(보안), VPC(네트워크)와 같은 AWS 서비스와 통합되어 컨테이너 이미지 저장, 로드 밸런싱, 인증 및 격리를 쉽게 관리할 수 있다. 오픈 소스 Kubernetes 호환성: 최신 버전의 오픈 소스 Kubernetes를 사용하여 기존 플러그인과 도구를 그대로 활용할 수 있다. 지원 버전 : 23년 4월 현재 kubernetes 버전 중 1.22~1.26 지원을 지원 중이다. 버전 출시 주기는 연 3회이며 각 버전은 12개월 동안 지원된다. 지원이 끝난 버전들은 자동으로 EKS가 업데이트시킨다. EKS 아키텍처 EKS 아키텍처는 크게 컨트롤 플레인(마스터 노드)과 데이터 플레인(워커 노드)로 나뉜다. 아키텍처 그림은 스터디에서 공유해주신 2022 AWS 마이그레이션 요점 정리 pdf를 기반으로 살펴보겠다. 컨트롤 플레인 EKS에서는 Kubernetes 컨트롤 플레인의 가용성과 내구성을 손상시킬 수 있는 단일 장애 지점을 제거하도록 설계되었다. 컨트롤 플레인은 API 서버 노드, etcd 클러스터로 구성된다.\nAPI 서버: 쿠버네티스 클러스터의 모든 작업을 처리하고 상태를 저장하는 중앙 허브이다. RESTful API를 통해 통신하며, 사용자 요청에 따라 클러스터의 상태를 변경하거나 정보를 반환시켜준다. etcd: etcd는 쿠버네티스 클러스터에서 사용하는 분산 키-값 저장소이다. 쿠버네티스의 모든 설정 데이터와 클러스터 상태 정보를 저장한다. 각 API 서버와 etcd는 개별 가용 영역에서 최소 2개 이상의 클러스터로 구성되어 실행된다. 이를 통해 단일 가용 영역의 이벤트가 EKS 클러스터 가용성에 영향을 미치지 않는다. 각 가용 영역에서는 NAT 게이트웨이를 통해 프라이빗 서브넷에서 실행되며 사용자는 해당 노드에 접근할 수 없다. 또한 API 서버는 NLB로 ETCD는 ELB를 사용하여 컨트롤 플레인 서버의 부하를 분산시킨다. API 서버와 ETCD 클러스터는 오토스케일링 그룹으로 구성되어 조건(클러스터 규모 , API 서버 및 etcd에 대한 요청 증가)에 따라 자동으로 리소스가 확장된다. 데이터 플레인 데이터 플레인은 쿠버네티스 클러스터 내에서 워크로드를 실행하고 관리되는 서버다. 아키텍처는 다음과 같다.\nkubelet: 노드 에이전트이다. kubelet은 API 서버로부터 파드를 실행하고 관리해야 하는 명령을 받아 서버에 반영시켜준다. 또 실행 중인 파드와 컨테이너를 관리하고, 상태를 모니터링하며, 필요한 경우 API 서버에 상태 정보를 보고하는 역할을 수행한다. kube-proxy: 네트워크 프록시 및 로드 밸런서이다. kube-proxy는 쿠버네티스 서비스에 대한 요청을 적절한 파드로 전달하고, 파드 간의 통신을 관리한다. EKS 의 기본 네트워크 CNI로는 VPC-CNI가 실행된다. 또한, 사용자의 요구에 맞게 데이터 플레인의 구성 옵션을 설정할 수 있다. 구성 옵션별 세부 정보는 다음 그림과 같다.\nhttps://aws.github.io/aws-eks-best-practices/reliability/docs/\n그림에서 비교한 것과 같이 사용자 책임 레벨에 따라 3가지로 구분된다.\nSelf Managed Workers :사용자가 직접 노드를 구성하고 관리하는 방식이다. 구체적으로는 Custom AMI를 통해 작업자 노드를 생성하며 AMI와 노드의 패치 및 업그레이드를 사용자가 직접 관리한다. Managed Node groups : AWS가 노드를 자동으로 프로비저닝하고, 업데이트 및 유지 관리를 처리한다. EKS Fargate : 서버리스 컴퓨팅 옵션이다. Micro VM를 이용하여 Pod별 VM이 할당된다. AWS가 설정에 맞게 파드 레벨의 스케일링을 자동으로 처리한다. Cluster Endpoint Access EKS 클러스터의 API 서버에 대한 접근을 제어하는 설정이다. 클러스터 엔드포인트를 설정함으로써, 클러스터에 대한 접근을 필요한 범위로 제한하여 보안을 강화할 수 있다. 옵션은 3가지 존재한다.\nPublic : 클러스터 엔드포인트가 Public IP로 할당되어 인터넷에서 접근이 가능한 설정이다. 클러스터 내부에서도 IGW 를 통해 Public IP와 통신한다.\n해당 옵션은 AWS 콘솔에서 확인 가능하다. Public 설정시 다음과 같이 접근이 가능하다.\n위와 같이 인터넷을 통해 접근이 가능하여 접근성을 높일 수 있다.\nPublic Private : 워크노드 내부에서는 Private(VPC 내부망에서만 접근 가능)로, API 접근은 Public(인터넷을 통한 접근 가능)으로만 통신이 가능하다. 워크 노드에서 민감데이터 노출을 막기 위해 Private subnet(사설망)에 위치시킨 경우 사용할 수 있는 옵션이다.\nPrivate : API 서버 엔드포인트 또한 허용된 VPC에서만 접근이 가능한 설정이다.\n해당 옵션으로 사용시 API Server URL은 Route53 privated hosted zone으로 변경되어 VPC 내부에서만 통신이 가능하다. 해당 URL은 AWS 콘솔의 EKS나 kubectl -v=6 옵션으로 확인이 가능하다.\n1 2 3 4 # API Server URL(Route53 privated hosted zone) (terraform-eks@my-eks:N/A) [root@myeks-host example]# kubectl get pods -A -v=6 I0428 14:26:36.609385 27821 loader.go:374] Config loaded from file: /root/.kube/config I0428 14:26:37.276259 27821 round_trippers.go:553] GET https://232B71CC97744BC94C09D40801D073B0.yl4.ap-northeast-2.eks.amazonaws.com/api/v1/pods?limit=500 200 OK in 661 milliseconds 위 [https://232B71CC97744BC94C09D40801D073B0.yl4.ap-northeast-2.eks.amazonaws.com](https://232B71CC97744BC94C09D40801D073B0.yl4.ap-northeast-2.eks.amazonaws.com) 가 API server URL이다. 해당 URL은 EKS 컨트롤 노드가 소유하고 있는 네트워크 인터페이스를 통해 통신된다. 통칭 EKS owned ENI는 AWS 콘솔에서 확인이 가능하다.\n네트워크 인터페이스에서 클러스터를 배포한 VPC를 입력하여 인스턴스 ID가 존재하지 않고(- 표시), 설명에 Amazone EKS cluster-name 으로 표시된 것이 EKS owned ENI 이다. 인스턴스 소유자 또한 AWS 소유자 번호가 다른 것을 확인할 수 있는데 AWS가 직접 관리하는 컨트롤 노드의 소유자 번호이기 때문이다.\nEKS 배포 방식 EKS 배포 방식은 Manual, Command line utility, Infrastructure as Code 에 따라 구성할 수 있다.\nManual : AWS Management Console 에서 직접 구성 Command line utility : 커맨드 명령어를 통한 구성 (AWS CLI, eksctl) Infrastructure as Code : 코드를 통한 구성(Terrafrom, AWS CDK 등) 이번 블로그 글에서는 eksctl 를 통해 클러스터를 구성하겠다. eksctl ? Amazon EKS (Elastic Kubernetes Service) 클러스터를 생성하고 관리하기 위한 명령줄 인터페이스(CLI) 툴이다. eksctl를 통해 클러스터를 구성 및 삭제, 업데이트할 수 있으며 VPC, 서브넷, 리소스 생성을 위한 정책 생성 등의 작업을 쉽게 처리할 수 있다. 공식 문서를 통해 다양한 인프라 환경에서의 EKS 구성을 참고할 수 있다.\n아쉬운 점은 EKS에서만 구축이 된다는 점이다. 추후 다른 클러스터나 멀티 클러스터 구축시 다른 툴을 사용하자.\n추가로 사용해보니.. eksctl는 형상관리가 불가능하다. 한 번 배포 이후에는 eksctl 구성 파일을 통해 추가 작업이 불가능하다. eksctl를 통해 기존의 리소스 변경이 불가능하며 추가만 가능하다. eksctl 를 통한 EKS 배포 eksctl를 통해 EKS Private Cluster 를 배포하겠다. 구성 아키텍처는 다음과 같다.\n노드 그룹은 unmanaged node group 으로, Cluster endpoint 는 Private로 구성하였다. 아키텍처에서 화살표는 통신 아키텍처이다.\nEKS Admin(빨간선) : 베스천 서버를 통해 EKS API Server로 접근하여 EKS를 관리하는 과정이다. Private Worker node(보라선): Private cluster 로 구성되어 EKS control node ENI를 통해 EKS Control node와 통신한다. 배포 과정은 다음과 같이 진행하겠다.\ncloudformation를 통한 베스천 서버 배포 eksctl를 통한 EKS 배포 1. AWS Cloudformation를 통한 베스천 서버 배포 AWS Cloudformation을 통해 VPC 구성 및 베스천 서버를 배포하고 eksctl 설치 및 관리 패키지를 설치하겠다. Cloudformation 스크립트는 스터디에서 공유해주신 것을 기반으로 내용을 추가하였다.\n1 2 # yaml 파일 다운로드 curl -O https://github.com/HanHoRang31/blog-share/blob/main/aews-eksctl/cloudformation-bastion.yaml 구성 코드에서 Private 클러스터 구축을 위한 중요 부분과 베스천 서버의 패키지 구성 정보를 확인하겠다. 먼저 Private 클러스터 구축을 위해 필요한 보안 그룹은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # cloudformation-bastion # 파라미터 ... # 베스천 서버 보안 그룹 설정 # EKSCTL-Host EKSEC2SG: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: eksctl-host Security Group VpcId: !Ref EksVPC Tags: - Key: Name Value: !Sub ${ClusterBaseName}-HOST-SG SecurityGroupIngress: - IpProtocol: tcp FromPort: \u0026#39;22\u0026#39; ToPort: \u0026#39;22\u0026#39; CidrIp: !Ref SgIngressSshCidr # 베스천 서버에서 EKS 배포를 위한 보안 그룹 설정 ControlPlaneSecurityGroup: DependsOn: - EKSEC2SG Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Cluster communication with worker nodes VpcId: !Ref EksVPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 443 ToPort: 443 SourceSecurityGroupId: !Ref EKSEC2SG Tags: - Key: Name Value: !Sub ${ClusterBaseName} Control Plane Security Group ... Private Cluster 구축으로 중요 사항은 두 가지이다.\nEKSEC2SG : 베스천 서버에 대한 보안 그룹 설정 부분이다. 해당 부분을 통해 베스천 서버에 접근할 수 있는 IP 범위를 제한할 수 있다. 코드 내 SgIngressSshCidr 에서 지정한 IP를 제한하며 해당 값은 배포 명령어로 override 을 통해 값을 수정할 수 있다. ControlPlaneSecurityGroup : 컨트롤 플레인과 워크 노드간 통신을 위한 보안 그룹 설정 부분이다. 코드에서는 베스천 서버와의 통신을 위해 443 포트의 보안 그룹을 추가하였다. userdata를 확인하면 베스천 서버에 설치되는 패키지를 확인할 수 있다. 설치 명령어는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 ... UserData: Fn::Base64: !Sub | #!/bin/bash hostnamectl --static set-hostname \u0026#34;${ClusterBaseName}-host\u0026#34; # Config convenience echo \u0026#39;alias vi=vim\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#34;sudo su -\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc # Change Timezone sed -i \u0026#34;s/UTC/Asia\\/Seoul/g\u0026#34; /etc/sysconfig/clock ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # Install Packages cd /root yum -y install tree jq git htop lynx # Install kubectl \u0026amp; helm #curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.26.2/2023-03-17/bin/linux/amd64/kubectl curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.25.7/2023-03-17/bin/linux/amd64/kubectl install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash # Install eksctl curl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp mv /tmp/eksctl /usr/local/bin # Install aws cli v2 curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 sudo ./aws/install complete -C \u0026#39;/usr/local/bin/aws_completer\u0026#39; aws echo \u0026#39;export AWS_PAGER=\u0026#34;\u0026#34;\u0026#39; \u0026gt;\u0026gt;/etc/profile export AWS_DEFAULT_REGION=${AWS::Region} echo \u0026#34;export AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION\u0026#34; \u0026gt;\u0026gt; /etc/profile # Install YAML Highlighter wget https://github.com/andreazorzetto/yh/releases/download/v0.4.0/yh-linux-amd64.zip unzip yh-linux-amd64.zip mv yh /usr/local/bin/ # Install krew curl -LO https://github.com/kubernetes-sigs/krew/releases/download/v0.4.3/krew-linux_amd64.tar.gz tar zxvf krew-linux_amd64.tar.gz ./krew-linux_amd64 install krew export PATH=\u0026#34;$PATH:/root/.krew/bin\u0026#34; echo \u0026#39;export PATH=\u0026#34;$PATH:/root/.krew/bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile # Install kube-ps1 echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;alias k=kubectl\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;complete -F __start_kubectl k\u0026#39; \u0026gt;\u0026gt; /etc/profile git clone https://github.com/jonmosco/kube-ps1.git /root/kube-ps1 cat \u0026lt;\u0026lt;\u0026#34;EOT\u0026#34; \u0026gt;\u0026gt; /root/.bash_profile source /root/kube-ps1/kube-ps1.sh KUBE_PS1_SYMBOL_ENABLE=false function get_cluster_short() { echo \u0026#34;$1\u0026#34; | cut -d . -f1 } KUBE_PS1_CLUSTER_FUNCTION=get_cluster_short KUBE_PS1_SUFFIX=\u0026#39;) \u0026#39; PS1=\u0026#39;$(kube_ps1)\u0026#39;$PS1 EOT # Install krew plugin kubectl krew install ctx ns get-all # ktop df-pv mtail tree # Install Docker amazon-linux-extras install docker -y systemctl start docker \u0026amp;\u0026amp; systemctl enable docker # Install nerdctl wget https://github.com/containerd/nerdctl/releases/download/v1.3.1/nerdctl-1.3.1-linux-amd64.tar.gz tar -xzf nerdctl-1.3.1-linux-amd64.tar.gz sudo mv nerdctl /usr/local/bin/ # CLUSTER_NAME export CLUSTER_NAME=${ClusterBaseName} echo \u0026#34;export CLUSTER_NAME=$CLUSTER_NAME\u0026#34; \u0026gt;\u0026gt; /etc/profile # Create SSH Keypair ssh-keygen -t rsa -N \u0026#34;\u0026#34; -f /root/.ssh/id_rsa ... 몇 가지 alias와 환경 설정을 추가하고, 시스템의 시간대를 Asia/Seoul로 변경한다. 필요한 패키지들(tree, jq, git, htop, lynx, kubectl, helm, eksctl, AWS CLI v2, yh, krew, docker, nerdctl, kube-ps1)을 설치한다. 클러스터 이름을 설정하고, SSH 키 페어를 생성한다. 배포는 다음과 같은 파라미터를 통해 진행하였다.\n1 aws cloudformation deploy --template-file myeks-1week.yaml --stack-name myeks --parameter-overrides KeyName=eks-terraform-key --region ap-northeast-2 배포 완료 후 다음의 명령어를 통해 베스천 서버에 접근하고 EKS 배포 과정으로 넘어가겠다.\n1 ssh -i eks-terraform-key.pem ec2-user@$(aws cloudformation describe-stacks --stack-name myeks --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text) 2. eksctl를 통한 EKS 배포 베스천 서버에 접속하여 AWS 인증 정보를 입력하자.\n1 2 3 4 5 [root@myeks-host example]# aws configure AWS Access Key ID [None]: ACCESS-KEY AWS Secret Access Key [None]: SECRET-KEY Default region name [None]: ap-northeast-2 Default output format [None]: json AWS 인증 정보 입력 후 eksctl를 통해 private cluster 구성하자. 구성 내용은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: my-eks region: ap-northeast-2 # 지역 설정(서울) version: \u0026#34;1.25\u0026#34; # 클러스터 버전 vpc: id: vpc-05a960a0837da1328 # 베스천 서버 VPC ID (아래 내용 참고) cidr: 192.168.0.0/16 # # 베스천 서버 VPC CIDR (아래 내용 참고) securityGroup: sg-0c59ddf1a9a73edc9 nat: gateway: HighlyAvailable subnets: public: public-2a: id: subnet-06391e7ab56a8ae9c # 서브넷 ID (아래 내용 참고) cidr: 192.168.1.0/24 # 서브넷 CIDR (아래 내용 참고) public-2c: id: subnet-00c193bd6e515a79b # 서브넷 ID (아래 내용 참고) cidr: 192.168.2.0/24 # 서브넷 CIDR (아래 내용 참고) private: private-2a: id: subnet-02d592518f7ae0755 # 서브넷 ID (아래 내용 참고) cidr: 192.168.3.0/24 # 서브넷 CIDR (아래 내용 참고) private-2c: id: subnet-0dcfc3b165e7b355d # 서브넷 ID (아래 내용 참고) cidr: 192.168.4.0/24 # 서브넷 CIDR (아래 내용 참고) clusterEndpoints: # 클러스터 엔드포인트 액세스 설정 부분 publicAccess: false # 공용 액세스 비활성화 privateAccess: true # 사설 액세스 활성화 nodeGroups: - name: ng-1 instanceType: m5.xlarge # 인스턴스 유형 desiredCapacity: 3 # 원하는 노드 수 privateNetworking: true # 사설 네트워크 사용 ssh: publicKeyName: ec2-key # ec2 보안 키 availabilityZones: - ap-northeast-2a - ap-northeast-2c iam: withAddonPolicies: imageBuilder: true # 이미지 빌더 정책 활성화 albIngress: true # ALB 인그레스 정책 활성화 cloudWatch: true # CloudWatch 정책 활성화 autoScaler: true # 오토 스케일러 정책 활성화 instanceName: EKS-WORKER-TEST volumeSize: 30 # 볼륨 크기 설정 구성 내용 중 VPC, Subnet ID, CIDR은 AWS Console에서 확인할 수 있다. VPC → Resource Road Map 을 통해 연결된 서브넷 및 라우팅 정보를 확인할 수 있다. 정보가 필요한 리소스를 클릭하면 바로 넘어가진다.\n보안 그룹 ID 는 EC2→ 보안 그룹에서 Name myeks Control Plane Security Group 에서 확인할 수 있다.\n구성 입력 후 eksctl 명령어를 통해 클러스터를 구축하겠다.\n1 eksctl create cluster -f private-cluster.yaml 배포는 약 20분정도 소요된다. 구성 후 클러스터 API 서버 접근을 위한 인증 정보가 필요하다. 인증 정보는 다음의 eksctl 명령어를 통해 저장할 수 있다.\n1 2 3 (terraform-eks@my-eks-2:N/A) [root@myeks-host example]# eksctl utils write-kubeconfig --name my-eks --region ap-northeast-2 Flag --name has been deprecated, use --cluster 2023-04-29 19:59:36 [✔] saved kubeconfig as \u0026#34;/root/.kube/config\u0026#34; 배포 확인을 위해 파드 리스트를 확인해보자.\n1 2 3 4 5 6 7 8 9 10 11 12 (terraform-eks@my-eks:N/A) [root@myeks-host example]# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-4kms4 1/1 Running 0 109m kube-system aws-node-8r2fq 1/1 Running 0 109m kube-system aws-node-hsgmr 1/1 Running 0 109m kube-system aws-node-nf758 1/1 Running 0 109m kube-system coredns-76b4dcc5cc-4mbw2 1/1 Running 0 127m kube-system coredns-76b4dcc5cc-vfpgp 1/1 Running 0 127m kube-system kube-proxy-68vgc 1/1 Running 0 109m kube-system kube-proxy-lgltk 1/1 Running 0 109m kube-system kube-proxy-lqngn 1/1 Running 0 109m kube-system kube-proxy-vl4tb 1/1 Running 0 109m 앞서 공유한 베스천 서버 인프라 구성 파일과 eks 구성 파일을 통해 진행하면 아무 문제 없이 구성이 완료될 것이다. 아래 내용은 필자가 private cluster 구성 중 생긴 에러로 트러블슈팅한 내용이다. 혹시 구성 중 에러가 발생한다면 다음 내용을 참고하자.\n트러블슈팅 클러스터 배포 중 timeout 발생과 워크 노드 클러스터 조인\n워크 노드 조인 중 에러가 생긴 에러이다. 에러 메세지는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 (N/A:N/A) [root@myeks-host example]# eksctl create cluster -f private-cluster.yaml 2023-04-28 12:24:04 [ℹ] eksctl version 0.138.0 2023-04-28 12:24:04 [ℹ] using region ap-northeast-2 2023-04-28 12:24:04 [!] warning, having public access disallowed will subsequently interfere with some features of eksctl. This will require running subsequent eksctl (and Kubernetes) commands/API calls from within the VPC. Running these in the VPC requires making updates to some AWS resources. See: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html for more details 2023-04-28 12:24:05 [✔] using existing VPC (vpc-06210c8560709c761) and subnets (private:map[private-2a:{subnet-0cceb4f7117a82214 ap-northeast-2a 192.168.3.0/24 0 } private-2c:{subnet-0c19b6b58320fce0a ap-northeast-2c 192.168.4.0/24 0 }] public:map[public-2a:{subnet-023a838543987d725 ap-northeast-2a 192.168.1.0/24 0 } public-2c:{subnet-09e710a10ef0fb5ef ap-northeast-2c 192.168.2.0/24 0 }]) 2023-04-28 12:24:05 [!] custom VPC/subnets will be used; if resulting cluster doesn\u0026#39;t function as expected, make sure to review the configuration of VPC/subnets 2023-04-28 12:24:05 [ℹ] nodegroup \u0026#34;ng-2\u0026#34; will use \u0026#34;ami-0fdcb707922882aef\u0026#34; [AmazonLinux2/1.25] 2023-04-28 12:24:05 [ℹ] using EC2 key pair \u0026#34;eks-terraform-key\u0026#34; 2023-04-28 12:24:05 [ℹ] using Kubernetes version 1.25 2023-04-28 12:24:05 [ℹ] creating EKS cluster \u0026#34;my-eks\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; region with un-managed nodes 2023-04-28 12:24:05 [ℹ] 1 nodegroup (ng-2) was included (based on the include/exclude rules) 2023-04-28 12:24:05 [ℹ] will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s) 2023-04-28 12:24:05 [ℹ] will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s) 2023-04-28 12:24:05 [ℹ] if you encounter any issues, check CloudFormation console or try \u0026#39;eksctl utils describe-stacks --region=ap-northeast-2 --cluster=my-eks\u0026#39; 2023-04-28 12:24:05 [ℹ] Kubernetes API endpoint access will use provided values {publicAccess=false, privateAccess=true} for cluster \u0026#34;my-eks\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; 2023-04-28 12:24:05 [ℹ] CloudWatch logging will not be enabled for cluster \u0026#34;my-eks\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; 2023-04-28 12:24:05 [ℹ] you can enable it with \u0026#39;eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-northeast-2 --cluster=my-eks\u0026#39; 2023-04-28 12:24:05 [ℹ] 2 sequential tasks: { create cluster control plane \u0026#34;my-eks\u0026#34;, 2 sequential sub-tasks: { wait for control plane to become ready, create nodegroup \u0026#34;ng-2\u0026#34;, } } 2023-04-28 12:24:05 [ℹ] building cluster stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:24:05 [ℹ] deploying stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:24:35 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:25:05 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:26:05 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:27:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:28:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:29:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:30:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:31:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:32:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:33:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:34:06 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-cluster\u0026#34; 2023-04-28 12:36:06 [ℹ] building nodegroup stack \u0026#34;eksctl-my-eks-nodegroup-ng-2\u0026#34; 2023-04-28 12:36:06 [ℹ] --nodes-min=2 was set automatically for nodegroup ng-2 2023-04-28 12:36:06 [ℹ] --nodes-max=2 was set automatically for nodegroup ng-2 2023-04-28 12:36:07 [ℹ] deploying stack \u0026#34;eksctl-my-eks-nodegroup-ng-2\u0026#34; 2023-04-28 12:36:07 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-nodegroup-ng-2\u0026#34; 2023-04-28 12:39:43 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-my-eks-nodegroup-ng-2\u0026#34; 2023-04-28 12:39:43 [ℹ] waiting for the control plane to become ready 2023-04-28 12:39:44 [✔] saved kubeconfig as \u0026#34;/root/.kube/config\u0026#34; 2023-04-28 12:39:44 [ℹ] no tasks 2023-04-28 12:39:44 [✔] all EKS cluster resources for \u0026#34;my-eks\u0026#34; have been created 2023-04-28 12:39:44 [ℹ] adding identity \u0026#34;arn:aws:iam::955963799952:role/eksctl-my-eks-nodegroup-ng-2-NodeInstanceRole-283XKKCXM9GT\u0026#34; to auth ConfigMap 2023-04-28 12:39:44 [ℹ] nodegroup \u0026#34;ng-2\u0026#34; has 0 node(s) 2023-04-28 12:39:44 [ℹ] waiting for at least 2 node(s) to become ready in \u0026#34;ng-2\u0026#34; Error: timed out waiting for at least 2 nodes to join the cluster and become ready in \u0026#34;ng-2\u0026#34;: context deadline exceeded 보통 타임에러로 표시되며 kubectl 명령어를 확인하면 워크 노드가 조인이 안되어 파드가 정상적으로 배포되지 않는 것을 확인할 수 있다.\n해당 내용은 깃 이슈에서 참고할 수 있지만 답을 찾을 수 없어 며칠을 고생했다. 원인은 EKS 보안 그룹 설정이였다. 앞서 베스천서버 구성시 EKS 통신을 위한 보안 그룹을 설정하였는데 추가를 안하면 EKS 워크 노드와 베스천 서버와의 통신이 안되어 조인이 안된다. EKS 보안 그룹을 설정하고 다시 EKS를 배포하자.\nAPI 서버 접근이 안되는 경우\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (N/A:N/A) [root@myeks-host example]# eksctl create cluster -f private-cluster.yaml 2023-04-27 19:45:28 [ℹ] eksctl version 0.138.0 2023-04-27 19:45:28 [ℹ] using region ap-northeast-2 2023-04-27 19:45:28 [!] warning, having public access disallowed will subsequently interfere with some features of eksctl. This will require running subsequent eksctl (and Kubernetes) commands/API calls from within the VPC. Running these in the VPC requires making updates to some AWS resources. See: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html for more details 2023-04-27 19:45:28 [ℹ] setting availability zones to [ap-northeast-2a ap-northeast-2b ap-northeast-2d] 2023-04-27 19:45:28 [ℹ] subnets for ap-northeast-2a - public:192.168.0.0/19 private:192.168.96.0/19 2023-04-27 19:45:28 [ℹ] subnets for ap-northeast-2b - public:192.168.32.0/19 private:192.168.128.0/19 2023-04-27 19:45:28 [ℹ] subnets for ap-northeast-2d - public:192.168.64.0/19 private:192.168.160.0/19 2023-04-27 19:45:28 [ℹ] nodegroup \u0026#34;EKS-PRIVATE-NODE\u0026#34; will use \u0026#34;ami-0fdcb707922882aef\u0026#34; [AmazonLinux2/1.25] 2023-04-27 19:45:28 [ℹ] using Kubernetes version 1.25 2023-04-27 19:45:28 [ℹ] creating EKS cluster \u0026#34;my-private-eks\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; region with un-managed nodes ... Error: getting auth ConfigMap: Get \u0026#34;https://AA1694EDA538EFE2ADC5FCCABBB4F745.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/configmaps/aws-auth\u0026#34;: dial tcp 192.168.175.32:443: i/o timeout (terraform-eks@my-private-eks:N/A) [root@myeks-host example]# kubectl get pods -A Unable to connect to the server: dial tcp 192.168.134.188:443: i/o timeout → 앞에도 다뤘지만 Timeout 의 대부분의 원인이 보안 그룹이다. 이 경우는 클러스터 구성 파일에서 베스천 서버의 보안그룹을 설정하지 않아서 생긴 문제였다. 클러스터 구성에서 베스천 서버에 대한 보안 그룹(아웃 바운드 베스천서버 443 포트)을 설정하면 된다. 클러스터 구성에서 보안 그룹 설정은 AWS 콘솔 → EKS에서 가능하다.\nAWS CLI 및 eksctl 결과 i/o timeout\n1 2 (N/A:N/A) [root@myeks-host example]# eksctl get cluster --region ap-northeast-2 Error: checking AWS STS access – cannot get role ARN for current session: operation error STS: GetCallerIdentity, https response error StatusCode: 0, RequestID: , request send failed, Post \u0026#34;https://sts.ap-northeast-2.amazonaws.com/\u0026#34;: dial tcp 52.95.192.98:443: i/o timeout 마찬가지로 보안 그룹 문제였다. 베스천 서버의 아웃바운드에 0.0.0.0/0를 추가하면 해결된다.\n콘솔에서 컴퓨팅 리소스 확인이 안되는 경우\n다음 그림과 같이 EKS 배포이후 AWS 콘솔에서 워크 노드가 표시되지 않는 경우이다.\n원인은 EKS 클러스터 사용자 정보에 AWS 사용자 정보가 없기 때문이다. 다음과 같이 추가하도록 하자.\n1 kubectl edit cm/aws-auth -n kube-system 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::000000000:role/eksctl-my-eks-nodegroup-ng-1-NodeInstanceRole-ZG7JVL5Z4IPU username: system:node:{{EC2PrivateDNSName}} mapUsers: | - userarn: arn:aws:iam:000000000:user/hanhorang # AWS 인증에서 사용한 IAM 사용자 arn를 입력하자. username: hanhorang groups: - system:masters kind: ConfigMap metadata: name: aws-auth namespace: kube-system mapUsers에서 IAM 사용자 arn을 추가하면 해결된다.\n클러스터 구성 확인 클러스터 구성 후 클러스터 정보와 인스턴스 정보를 확인하겠다.\n클러스터 구성 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 클러스터 확인 (terraform-eks@my-eks:N/A) [root@myeks-host example]# eksctl get cluster NAME REGION EKSCTL CREATED my-eks ap-northeast-2 True # 클러스터 노드 그룹 확인 (terraform-eks@my-eks:N/A) [root@myeks-host example]# eksctl get nodegroup --cluster my-eks CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAMETYPE my-eks ng-1 CREATE_COMPLETE 2023-04-29T09:05:31Z 4 4 4 m5.xlarge ami-0fdcb707922882aef eksctl-my-eks-nodegroup-ng-1-NodeGroup-IWPRDQX6J0CG unmanaged # 클러스터 접근 정보 확인 및 노드 확인 (terraform-eks@my-eks:N/A) [root@myeks-host example]# kubectl get nodes -v6 I0429 20:21:39.855710 5753 loader.go:374] Config loaded from file: /root/.kube/config I0429 20:21:40.631509 5753 round_trippers.go:553] GET https://8F53A6D3D93C1D751527688A7CB07659.yl4.ap-northeast-2.eks.amazonaws.com/api/v1/nodes?limit=500 200 OK in 769 milliseconds NAME STATUS ROLES AGE VERSION ip-192-168-3-31.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 131m v1.25.7-eks-a59e1f0 ip-192-168-3-59.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 131m v1.25.7-eks-a59e1f0 ip-192-168-4-195.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 131m v1.25.7-eks-a59e1f0 ip-192-168-4-250.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 131m v1.25.7-eks-a59e1f0 # 클러스터 정보 확인 kubectl cluster-info dump ... 베스천 서버에서 워크 노드 접근을 위한 보안 그룹 설정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # 인스턴스 IP 확인 terraform-eks@my-eks:N/A) [root@myeks-host example]# aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output table ------------------------------------------------------------------- | DescribeInstances | +-----------------+-----------------+------------------+----------+ | InstanceName | PrivateIPAdd | PublicIPAdd | Status | +-----------------+-----------------+------------------+----------+ | EKS-WORKER-TEST| 192.168.4.230 | None | running | | EKS-WORKER | 192.168.4.195 | None | running | | EKS-WORKER | 192.168.4.250 | None | running | | EKS-WORKER | 192.168.3.59 | None | running | | EKS-WORKER | 192.168.3.31 | None | running | | myeks-host | 192.168.1.100 | 43.201.102.195 | running | +-----------------+-----------------+------------------+----------+ # 워크 노드로 Ping을 하나 안된다. (terraform-eks@my-eks:N/A) [root@myeks-host example]# ping 192.168.3.31 PING 192.168.3.31 (192.168.3.31) 56(84) bytes of data. # 노드 보안그룹 ID 확인하여 베스천 서버 IP를 추가하자 (terraform-eks@my-eks:N/A) [root@myeks-host example]# aws ec2 describe-security-groups --filters Name=group-name,Values=*nodegroup* --query \u0026#34;SecurityGroups[*].[GroupId]\u0026#34; --output text sg-0bf33458f7e193841 sg-0f4cd52a07786b9fb # --group-id 에 위 보안 그룹 하나 입력 (terraform-eks@my-eks:N/A) [root@myeks-host example]# aws ec2 authorize-security-group-ingress --group-id sg-0f4cd52a07786b9fb --protocol \u0026#39;-1\u0026#39; --cidr 192.168.1.100/32 { \u0026#34;Return\u0026#34;: true, \u0026#34;SecurityGroupRules\u0026#34;: [ { \u0026#34;SecurityGroupRuleId\u0026#34;: \u0026#34;sgr-01bca8eecf69c86b6\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;sg-0f4cd52a07786b9fb\u0026#34;, \u0026#34;GroupOwnerId\u0026#34;: \u0026#34;955963799952\u0026#34;, \u0026#34;IsEgress\u0026#34;: false, \u0026#34;IpProtocol\u0026#34;: \u0026#34;-1\u0026#34;, \u0026#34;FromPort\u0026#34;: -1, \u0026#34;ToPort\u0026#34;: -1, \u0026#34;CidrIpv4\u0026#34;: \u0026#34;192.168.1.100/32\u0026#34; } ] } # 접근 확인 (terraform-eks@my-eks:N/A) [root@myeks-host example]# ping 192.168.3.31 PING 192.168.3.31 (192.168.3.31) 56(84) bytes of data. 64 bytes from 192.168.3.31: icmp_seq=1 ttl=255 time=0.176 ms 64 bytes from 192.168.3.31: icmp_seq=2 ttl=255 time=0.152 ms 64 bytes from 192.168.3.31: icmp_seq=3 ttl=255 time=0.144 ms 인스턴스 정보 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # kubelet 확인 systemctl status kubelet ● kubelet.service - Kubernetes Kubelet Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubelet-args.conf, 30-kubelet-extra-args.conf Active: active (running) since Sat 2023-04-29 09:08:56 UTC; 2h 49min ago Docs: https://github.com/kubernetes/kubernetes Process: 3176 ExecStartPre=/sbin/iptables -P FORWARD ACCEPT -w 5 (code=exited, status=0/SUCCESS) Main PID: 3178 (kubelet) Tasks: 16 Memory: 77.3M CGroup: /runtime.slice/kubelet.service └─3178 /usr/bin/kubelet --config /etc/kubernetes/kubelet/kubelet-config.json --kubeconfig /var/lib/kubelet/kubeconfig --container-runtime-endpoint unix://... Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.377073 3178 kubelet.go:2117] \u0026#34;SyncLoop ADD\u0026#34; source=\u0026#34;api\u0026#34; pods=...sncki] Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.377103 3178 topology_manager.go:205] \u0026#34;Topology Admit Handler\u0026#34; Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.513465 3178 reconciler.go:357] \u0026#34;operationExecutor.VerifyControllerAt... Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.614154 3178 reconciler.go:269] \u0026#34;operationExecutor.MountVolume starte... Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.632046 3178 operation_generator.go:730] \u0026#34;MountVolume.SetUp succeeded... Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.703987 3178 util.go:30] \u0026#34;No sandbox for pod can be found. Need...sncki\u0026#34; Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.791915 3178 provider.go:102] Refreshing cache for provider: *c...ovider Apr 29 11:52:06 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:06.792001 3178 provider.go:82] Docker config file not found: coul...t /] Apr 29 11:52:07 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:07.235202 3178 kubelet.go:2155] \u0026#34;SyncLoop (PLEG): event for pod\u0026#34; ...a6526} Apr 29 11:52:11 ip-192-168-3-31.ap-northeast-2.compute.internal kubelet[3178]: I0429 11:52:11.242075 3178 kubelet.go:2155] \u0026#34;SyncLoop (PLEG): event for pod\u0026#34; ...4351a} Hint: Some lines were ellipsized, use -l to show in full. # 볼륨 확인 lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:0 0 30G 0 disk ├─nvme0n1p1 259:1 0 30G 0 part / └─nvme0n1p128 259:2 0 1M 0 part # 컨테이너 런타임 확인 ps axf |grep /usr/bin/containerd 3013 ? Ssl 0:46 /usr/bin/containerd 4269 ? Sl 0:10 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 0ea51c82eb135fca4bdae99f89e8f5804d1e144efb857e8fae310a9d7039e21a -address /run/containerd/containerd.sock 4270 ? Sl 0:02 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id f723a90d2f436d77f37789bc29e26fcdbca82ca575bfacde43be2a0978573634 -address /run/containerd/containerd.sock 24478 ? Sl 0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id c1657f716d01283e8e7aa183f6583ec4c96af3d1d19b081004d1c565015a6526 -address /run/containerd/containerd.sock 26752 ? S+ 0:00 \\_ grep --color=auto /usr/bin/containerd 참고 https://awskoreamarketingasset.s3.amazonaws.com/2022 Summit/pdf/T14S4_Amazon EKS 마이그레이션 요점 정리.pdf\nhttps://341123.tistory.com/m/6\n","date":"Apr 29","permalink":"https://HanHoRang31.github.io/post/aews-1-eks/","tags":["KANS","eks","cloud","AWS","kubernetes"],"title":"[AEWS] EKS 아키텍처와 Private EKS 클러스터 배포하기"},{"categories":null,"contents":" 1 2 [테크 따라잡기]는 IT 기업 기술 스택 사례를 참고하여 구현하고 정리하는 시리즈입니다. 오로지 기술 성장을 위해 작성할 예정이며 스터디 과정에서 얻는 인사이트를 공유하고자 합니다. 도커 레지스트리인 Harbor에 대해 사용 사례를 조사하던 중 배울 것이 많은 테크 블로그 글을 발견하였다. 그대로 글로만 보기에는 필자가 아쉬워서 직접 구현하고 구현 과정에서 얻는 인사이트를 공유하고자 블로그 글을 작성하였다. 참고한 테크 블로그는 다음의 링크에서 확인이 가능하다.\nhttps://engineering.linecorp.com/ko/blog/harbor-for-private-docker-registry\n블로그 글을 요약하자면, Private Docker Registry로 Harbor를 선택하여 구성하였고, 오픈소스 오브젝트 스토리지인 Minio를 통해 스토리지 고가용성을 보장하였다. 또한 사내 LDAP 서버를 이용하여 직원 정보를 Harbor와 연계하여 로그인 정보를 연계하였다고 한다.\n기술 구현의 큰 틀은 같지만, 쿠버네티스 환경(AWS EKS)에서 Harbor를 배포하고 도커 이미지 백엔드 저장소로 Minio 사용하고자 한다. 또한 고가용성을 가지기 위해 다중 지역별 볼륨을 분산시키고, 백업에 대한 아키텍처를 설계하고 구현할 것이다. 아키텍처를 구현하면서 고민한 부분도 공유할 예정이다.\nHarbor Harbor는 CNCF에서의 레벨이 마지막 레벨인 Graduation(졸업) 단계인 오픈소스 도커 레지스트리이다. 도커 레지스트리 기능으로 안정성, 유용성, 커뮤니티 지원 등 여러 측면에서 충분한 성숙도를 갖춘 것으로 인정받았으며 필요 사례에 맞게 커스터 마이징이 가능하다. 대표적인 기능으로는 멀티 레파티토리 지원, 보안 스캔, 사용자 관리 권한, 이미지 사이클 관리, 마이그레이션, 저장소 복제, 스토리지 백업, 미러링 기능이 있다.\n기능면에서 미러링과 복제가 헷갈릴 수 있는데 목적이 분명 다르다. 미러링은 실시간 동기화를 통해 고가용성과 데이터 안정성을 보장하는 기능인 반면, 복제는 데이터 분산, 백업 및 로드 분산을 위해 주기적으로 또는 수동으로 데이터를 복사하는 방식이다.\nhttps://landscape.cncf.io/guide#provisioning\u0026ndash;container-registry\n아키텍처는 다음과 같다. 중앙 하늘색 네모 부분이 harbor 구성 요소이다. 크게 네트워크단, 서비스단, 데이터 액세스 계층으로 구성된다.\nhttps://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor\n네트워크 계층(Proxy) : Nginx 서버에 의해 형성된 리버스 프록시이다. 하버의 서비스 구성 요소는 모두 이 역방향 프록시 뒤에 있으며 Proxy를 통해 전달된다. 이를 통해 로드 밸런싱, 보안, SSL/TLS 부하 관리, 캐싱 부하 개선, 압축 개선등의 이점을 얻을 수 있다. 서비스 계층(Core) : Harbor Core는 프로젝트 관리, 사용자 인증 및 권한 관리, 시스템 설정 관리, 오케스트레이션 및 이벤트 처리와 같은 다양한 핵심 기능을 담당하는 중심 컴포넌트이다. 세부 기능은 다음과 같다. 프로젝트 관리: Harbor Core는 프로젝트의 생성, 수정, 삭제 및 구성을 관리한다. 프로젝트는 사용자가 컨테이너 이미지와 Helm 차트를 구성 및 관리할 수 있는 논리적 상위 레벨의 단위이다 사용자 인증 및 권한 관리: Harbor Core는 사용자 인증 및 권한 관리를 수행한다. 이를 통해 특정 프로젝트에 대한 접근 권한을 제어할 수 있으며, 사용자는 자신의 권한 범위 내에서 이미지 및 차트를 관리할 수 있다. Harbor는 여러 인증 방식을 지원하며, 외부 인증 시스템과의 통합도 가능하다. 시스템 설정: Harbor Core는 전체 시스템에 대한 다양한 설정을 관리한다. 이에는 공통 설정, 네트워크 설정, 스토리지 설정, 보안 설정, 로깅 및 모니터링 설정 등이 포함된다. 오케스트레이션: Harbor Core는 다른 서비스 컴포넌트와의 상호 작용을 조정한다. 예를 들어 이미지 스캔 요청이 들어오면, 해당 작업을 Job Service에 전달하고, 결과를 사용자에게 전달한다. 이벤트 처리: Harbor Core는 시스템 내부의 이벤트를 처리하며, 필요한 경우 다른 컴포넌트와 상호 작용하여 작업을 수행한다. 이러한 이벤트에는 이미지 및 차트의 생성, 수정, 삭제 등이 포함된다. 데이터 액세스 계층(Data Access Layer) : 3가지로 구성되며 kv-storage(Redis 작업 데이터에 대한 캐싱 스토리지), Local / Remote Storage(도커 레지스트리 저장소) , SQL Database(PostgreSQL 프로젝트, 사용자, 역할, 복제 정책, 태그 보존 정책, 스캐너, 차트 및 이미지와 같은 Harbor 모델의 관련 메타데이터를 저장)을 수행한다. 공식문서에 따르면 Harbor는 기본적으로 상태 비저장 상태이며 파드간 복제가 가능하여 파드 HA를 제공한다. 하지만 스토리지 계층 레벨에서는 사용자가 고가용성 PostgreSQL, 애플리케이션 데이터 및 PVC를 위한 Redis 클러스터 또는 이미지 및 차트를 저장하기 위한 스토리지를 구축해야 한다.\nhttps://goharbor.io/docs/1.10/install-config/harbor-ha-helm/\n이에 따라 스토리지 계층에서의 고가용성을 구성하고자 한다. Harbor 헬름 차트를 확인하면 스토리지 관련 설정부분이 6개이다. 각 스토리지은 다음과 같다.\nRegistry: Docker 레지스트리 스토리지이다. 컨테이너 이미지 및 Helm 차트와 같은 아티팩트를 저장하고 관리한다. ChartMuseum: Helm 차트 레포지토리 스토리지이다. Helm 차트를 저장하고 제공하는 데 사용된다. JobService: JobService의 로그 및 작업 데이터를 저장한다. JobService는 이미지 레플리케이션, 가비지 수집, 스캔 작업 등의 작업을 수행한다. Database: Harbor의 메타데이터를 저장하는 PostgreSQL 데이터베이스이다. 프로젝트, 사용자, 권한 및 구성 정보와 같은 메타데이터를 저장한다 Redis: Harbor에서 사용하는 캐싱 및 메시지 큐 서비스이다. Trivy: 컨테이너 이미지의 취약점 스캔을 수행하는 오픈 소스 스캐너이다. 이 구성 요소의 볼륨은 취약점 데이터베이스 및 스캔 결과를 저장한다. 6개의 스토리지 중 고가용성 구성을 위한 스토리지는 3개이다.\nRegistry : 도커 이미지 저장소 ChartMuseum : helm 차트 저장소 Database : Harbor 메타데이터 저장소 Harbor에서는 스토리지로 block storage, file storage, object stroage 로 설정이 가능하다. 다만, 저장소의 성능 및 활용 특성으로 저장소별 스토리지를 설정해야 한다. 헬름 차트를 확인하면 도커 이미지 저장소는 object 스토리지로 나머지 저장소는 블록 스토리지로 설정 추천하는데 이유는 다음과 같다.\n도커 이미지 저장소를 object storage(예: Amazon S3)로 사용하는 이유:\n확장성: Object storage는 거의 무제한의 저장 용량을 제공하므로, 많은 도커 이미지를 저장할 수 있다. 내구성: Object storage는 데이터를 여러 물리적 위치에 자동으로 복제하므로, 데이터 손실의 위험이 낮다. 비용 효율: 일반적으로 object storage는 용량 당 비용이 낮고, 저장된 데이터에 따라 비용이 증가한다. 기타 저장소를 블록 스토리지(예: AWS EBS)로 저장하는 이유:\n높은 IOPS: EBS는 높은 IOPS(Input/Output Operations Per Second) 성능을 제공하므로, 데이터베이스 작업에 적합하다. 지연 시간 최소화: 블록 스토리지는 데이터베이스 작업에 필요한 빠른 읽기/쓰기 작업이 가능하다. 일관된 성능: EBS는 일관된 성능을 제공하여 데이터베이스 작업의 안정성을 보장한다. 스냅샷 및 백업: EBS 볼륨의 스냅샷을 쉽게 생성할 수 있으며, 데이터베이스 백업 및 복구를 용이하게 한다. 추천 스토리지에 따라 스토리지 구성할 것인데 도커 이미지 저장소로는 Minio 오브젝트 스토리지를 사용할 것이다. 그리고 나머지 저장소는 AWS EBS를 사용할 것이다. 또한 백업을 위해 오픈소스 백업 솔루션인 Velero를 사용할 것이다. 종합하여 AWS 아키텍처를 구성하자면 다음과 같다.\n통신 시나리오에 따라 색깔을 달리 표현했다.이어서 통신 시나리오 및 세부 컴포넌트(MiniO)을 알아보겠다.\nusers (보라선) : Harbor에 접속해서 도커 이미지를 확인할 수 있는 개발자 및 레지스트리 관리자이다. Harbor 도메인을 통해 ELB를 거쳐 harbor ingress 로 통신한다. Storage(파랑선) : 스토리지 볼륨 연계도이다. 설계 목적과 같이 Harbor Registry 저장소는 Minio로 나머지 저장소는 PV(EBS) 로 적재했다. Backup(초록선) : 백업 툴인 Velero를 통해 S3 에 데이터를 백업시킨다. Admin(빨강선) : 인프라 관리자는 Bestion server에 접속하여 쿠버네티스 및 인프라를 관리한다. Minio MinIO 는 AWS의 S3 SDK와 호환되는 오픈소스 Object Storage이다. Minio 모드 중 Distributed Mode는 Minio의 분산 모드로, 여러 서버 또는 노드 간에 데이터를 분산 저장하고 관리할 수 있다. 분산 모드는 높은 가용성, 내구성, 그리고 스케일 아웃 확장성을 제공한다.\n구성시 중요한 점으로 Distributed MinIO 의 디스크이다. 분산 모드에서는 최소 4개의 디스크가 필요하다. 분산스토리지의 데이터 복구 기능으로 Erasure Coding 알고리즘을 채택하기 때문인데 원본 데이터를 여러 조각으로 나누고, 각 조각에 대해 패리티 정보를 생성하여 분산 저장한다. 이에따라 데이터 복구에 N/2 데이터 블록과 N/2 패리티 블록 조건을 만족해야 하며 최소 4개 이상의 디스크를 사용하는 것을 권장하기 때문이다. 이렇게 구성하면 최대 패리티에서 MinIO는 삭제 세트당 최대 절반의 드라이브 손실을 허용할 수 있다.\n배포 설치 환경 : EKS 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스 솔루션 최소 요구 사항 Harbor (CPU 2 core , 4GB, 40 GB) Distributed MinIO ( CPU 1~2 core, 2~4GB, 분산 Minio) Velero ( CPU 1 core, 1GB, 디스크 공간 알아서..) 원활한 테스트를 위해 약 CPU 5 core 이상, 메모리 8기가 이상이 필요하다. 필자의 경우 워크노드를 c5a.2xlarge 로 3개($0.344 *2) 설성하여 테스트를 진행하였다.\nDistributed MinIO 배포 설치는 minio helm 참고하여 구성하였다.\n1 2 3 4 helm repo add minio https://charts.bitnami.com/bitnami helm repo update helm fetch minio/minio --untar --version 12.2.1 구성 옵션별로 수정은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # values-minio.yaml mode: distributed auth: rootUser: admin rootPassword: \u0026#34;admin1234\u0026#34; statefulset: replicaCount: 2 zones: 2 drivesPerNode: 1 provisioning: config: - name: region options: name: ap-northeast-2 ingress: enabled: true hostname: minio.hanhorang.link path: /* annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}, {\u0026#34;HTTPS\u0026#34;:9090}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: {ACM arn 입력} persistence: storageClass: \u0026#34;kops-csi-1-21\u0026#34; 분산 모드에서 인스턴스 설정은 statuefulset에서 설정한다. replicaCount: 각 zones당 실행되는 실행되는 파드의 수를 설정한다. zones: AWS의 가용영역이라 생각하면 된다. 분산스토리지로 사용할 영역 개수를 지정한다. drivesPerNode: 각 워커 노드에 연결된 디스크 수를 1개로 설정한다. 이 설정에 따라 각 MinIO 파드는 하나의 디스크를 사용하게 된다. 배포\n1 2 kubectl create ns minio helm install minio minio/minio -f values-minio.yaml -n minio --version 12.2.1 배포 완료후, 도메인으로 접속하여 확인해보자. 도메인은 헬름 차트의 ingress.hostname에 입력한 값이다.\n어드민 계정은 차트에서 admin / admin1234 로 설정되어 있다. 로그인을 하자.\n로그인이 완료되면 다음과 같은 화면을 확인할 수 있다.\nHarbor 연동을 위하여 접속한 Minio 에서 버킷 생성과 Access Key를 발급받아야 한다. 버킷 이름과 Access keys는 Harbor 구성 헬름 차트에서 필요하다.\n버킷 생성\n초기 화면 Object Brower 에서 버킷 생성이 가능하다.\nAccess Key 발급\nAccess Keys 에서 발급받자.\nCreate 버튼 잊지말고 클릭하자.\nAccess Key 발급 후 MINIO 동작 권한을 등록해야 한다. 생성한 키를 클릭하면 정책 입력 칸이 나온다. 아래 정책을 입력하도록 하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;admin:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::*\u0026#34; ] } ] } Harbor 배포 Harbor 구성도 마찬가지로 helm로 배포할 것이다. 먼저 헬름 차트를 가져오겠다.\n1 2 3 4 helm repo add harbor https://helm.goharbor.io helm repo update # 차트 얻기 helm show values **harbor/harbor** \u0026gt; values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #values-harbor.yaml # network 설정 expose: tls: certSource: none ingress: hosts: core: harbor.hanhorang.link notary: notary.hanhorang.link controller: alb className: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: {ACM arn 입력} externalURL: https://harbor.hanhorang.link # Storage 설정 persistence: persistentVolumeClaim: registry: storageClass: \u0026#34;kops-csi-1-21\u0026#34; chartmuseum: storageClass: \u0026#34;kops-csi-1-21\u0026#34; database: storageClass: \u0026#34;kops-csi-1-21\u0026#34; imageChartStorage: disableredirect: true type: s3 s3: region: ap-northeast-2 bucket: registry accesskey: \u0026lt;minio-access-key\u0026gt; secretkey: \u0026lt;minio-secret-key\u0026gt; regionendpoint: http://minio.minio.svc.cluster.local:9000 # HA core: replicas: 3 스토리지 연동은 persistence에서 진행한다. registry 부분 연동은 imageChartStorage에서 설정한다. imageChartStorage.s3 에서 minio 에서 발급받은 키를 입력한다. imageChartStorage.s3.regionpoint 는 minio 의 DNS 정보를 입력했다. 인그래스로 설정이 가능하나, 다음과 같은 에러로 DNS 로 설정하였다. 네트워크 설정으로 오류 발생시 다음의 메세지를 확인하자. 🧐 minio 네트워크 설정 관련 트러블슈팅\nregionendpoint 설정에 따라 에러가 발생하여 정리한다.\n1 kubectl logs harbor-core-5b7864c575-rnfm9 -n harbor regionendpoint: minio.hanhorang.link 의 경우 도메인은 minio의 ALB 도메인 주소이다. Minio 는 S3와 호환되는 SDK를 가지고 있다. 아래 로그 메세지의 경우 포트를 지정하지 않아 생긴 오류다.\n1 2 3 4 5 2023-03-21T16:59:40Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;2236a0c4-55cd-4e5c-989d-1b9f941efb25\u0026#34;]: failed to populate properties for project library, error: get chart count of project 1 failed: http error: code 500, message InvalidArgument: S3 API Requests must be made to API port. status code: 400, request id: , host id: 2023-03-21T16:59:40Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;2236a0c4-55cd-4e5c-989d-1b9f941efb25\u0026#34;]: failed to populate properties for project test, error: get chart count of project 2 failed: http error: code 500, message InvalidArgument: S3 API Requests must be made to API port. status code: 400, request id: , host id: 2023-03-21T16:59:42Z [ERROR] [/lib/http/error.go:56]: {\u0026#34;errors\u0026#34;:[{\u0026#34;code\u0026#34;:\u0026#34;UNKNOWN\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;get chart count of project 2 failed: http error: code 500, message InvalidArgument: S3 API Requests must be made to API port.\\n\\tstatus code: 400, request id: , host id: \u0026#34;}]} regionendpoint: minio.hanhorang.link:9000 경우 포트 9000을 추가하니 Timeout이 발생한다. harbor ALB에서 다시 minio ALB로 거치는 과정에서 시간이 초과된 것으로 예상된다.\n1 2 3 4 5 2023-03-21T16:55:40Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;a0aed46e-4d11-4459-af04-2efa9f94cbf3\u0026#34;]: failed to populate properties for project library, error: get chart count of project 1 failed: get content failed: send request GET /library/index.yaml failed: Get \u0026#34;http://harbor-chartmuseum/library/index.yaml\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) 2023-03-21T16:55:40Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;a0aed46e-4d11-4459-af04-2efa9f94cbf3\u0026#34;]: failed to populate properties for project test, error: get chart count of project 2 failed: get content failed: send request GET /test/index.yaml failed: Get \u0026#34;http://harbor-chartmuseum/test/index.yaml\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) 2023-03-21T16:55:56Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;8e0c7248-9425-4862-b066-9b4b29151105\u0026#34;]: failed to populate properties for project test, error: get chart count of project 2 failed: get content failed: send request GET /test/index.yaml failed: Get \u0026#34;http://harbor-chartmuseum/test/index.yaml\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) 2023-03-21T16:55:56Z [ERROR] [/server/v2.0/handler/project.go:509][requestID=\u0026#34;8e0c7248-9425-4862-b066-9b4b29151105\u0026#34;]: failed to populate properties for project library, error: get chart count of project 1 failed: get content failed: send request GET /library/index.yaml failed: Get \u0026#34;http://harbor-chartmuseum/library/index.yaml\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) 2023-03-21T16:56:29Z [ERROR] [/lib/http/error.go:56]: {\u0026#34;errors\u0026#34;:[{\u0026#34;code\u0026#34;:\u0026#34;UNKNOWN\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;get chart count of project 2 failed: get content failed: send request GET /test/index.yaml failed: Get \\\u0026#34;http://harbor-chartmuseum/test/index.yaml\\\u0026#34;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)\u0026#34;}]} regionendpoint: minio.minio.svc.cluster.local:9000 경우\n도메인 연결의 최소로 하기 위해 CoreDNS를 사용했다. 그러나 HTTP 설정으로 에러가 발생했다.\n1 2023-03-21T16:35:45Z [ERROR] [/lib/http/error.go:56]: {\u0026#34;errors\u0026#34;:[{\u0026#34;code\u0026#34;:\u0026#34;UNKNOWN\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;get chart count of project 2 failed: http error: code 500, message RequestError: send request failed\\ncaused by: Get \\\u0026#34;https://minio.minio.svc.cluster.local:9000/minio-test?prefix=test\\\u0026#34;: http: server gave HTTP response to HTTPS client\u0026#34;}]} regionendpoint: http://minio.hanhorang.link:9000\nHTTP를 붙이니 정상적으로 연동이 확인된다. 쿠버네티스 내부에서 연결하는 것이라 HTTPS 트래픽 부하를 최소화하는 목적으로 HTTP를 사용하여 설정했다.\n1 2 3 kubectl create ns harbor helm install harbor harbor/harbor -f values-harbor.yaml --namespace harbor --version 1.11.0 배포 확인 헬름차트에서 입력 harbor 도메인으로 접근하면 로그인 화면이 나온다. 초기 admin 유저의 접속 정보는 admin / Harbor12345 이다. 로그인하면 레지스트리 정보가 나온다.\nNew Project 버튼을 눌러 레지스트리 프로젝트에 버킷을 생성하자\n연동 테스트 임의의 도커 이미지 PULL \u0026amp; PUSH 하여 Harbor 레지스트리, Minio 버킷에 데이터가 저장되는 지 확인해보자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 docker pull nginx \u0026amp;\u0026amp; docker pull busybox \u0026amp;\u0026amp; docker images docker tag busybox harbor.$KOPS_CLUSTER_NAME/test/busybox:0.1 echo \u0026#39;Harbor12345\u0026#39; \u0026gt; harborpw.txt cat harborpw.txt | docker login harbor.$KOPS_CLUSTER_NAME -u admin --password-stdin WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded docker push harbor.$KOPS_CLUSTER_NAME/test/busybox:0.1 The push refers to repository [harbor.hanhorang.link/test/busybox] baacf561cfff: Pushed 0.1: digest: sha256:acaddd9ed544f7baf3373064064a51250b14cfe3ec604d65765a53da5958e5f5 size: 528 Harbor에 이미지가 저장되고 이어서 minio 버킷에 하버 데이터가 저장된 것을 확인할 수 있다!\n이어서 이번 블로그 글에서는 Harbor와 minio를 통해 EKS에서 고가용성 Private Docker Registry(Harbor) 구축을 하였다. 아키텍처 설계를 하면서 스토리지 특성에 대해 딥하게 다룰 수 있는 계기가 되었고, CNCF의 졸업 레벨의 시스템(Harbor)의 아키텍처를 구체적으로 분석할 수 있었다. 느끼는 거지만 간단하면서도 각 컴포넌트가 세부적으로 분산되어 처리된다는 점과 스토리지 연동 부분에서 사례 참고가 되었다.\n아키텍처 구축이 끝이 아니다. 운영 레벨의 기능들이 남았다. 다음 글에서는 Velero를 통한 백업과 LDAP 서버와의 연동, Harbor의 기능들에 대해 살펴보겠다.\n","date":"Apr 21","permalink":"https://HanHoRang31.github.io/post/tech-private-docker/","tags":["eks","cloud","AWS","kubernetes","harbor","minio"],"title":"[테크 따라잡기] EKS에서 고가용성 Private Docker Registry(Harbor) 구축하기"},{"categories":null,"contents":"전글 Loki 최신 버전 설치하기에서 인덱스 DB를 Dynamodb로 연동하지 못했다. 미련이 남아 스터디가 끝난 이후에도 여러 테스트를 해보았고, 마침내 연동을 성공하여 관련 경험을 공유한다.\n먼저, Loki에서 DynamoDB로 연동해야 하는 목적부터 알고 가자. 배경 지식 설명을 위해 Loki 의 지원 스토리지를 다시 확인하겠다.\nhttps://grafana.com/docs/loki/latest/operations/storage/\n스토리지 연동 목록을 살펴보면 Loki 2.0 이상에서 싱글 스토어(boltdb-shipper)가 추가되었고 연동으로 추천하고 있다. (기본 스토리지도 싱글 스토어이다.) 이유를 살펴보니 비용 감소인데 로컬에서 인덱스를 저장하기 때문에 외부 스토리지에 대한 종속성이 줄기 때문이라 한다. 다만, 원격 저장소로 백업하고 복원하는데 추가 작업이 필요하다. (앞 글에서는 S3로 연동했으나 시간 텀이 15분 정도 있었음)\n비용은 발생하지만, DyanmoDB 연동 목적은 다음과 같다.\n복원력 : 실시간으로 데이터가 적재되어 로키 파드가 죽어도 데이터가 유지된다. 확장성 : 멀티 클러스터에서의 실시간 로깅 확인 (로컬인 경우 S3에 적재가 가능하지만 시간 텀이 있다.) 결론적으로, 소규모 프로젝트나 초기 설정에 초점을 맞추는 경우 BoltDB \u0026amp; S3 옵션을 고려할 수 있다. 반면, 대규모 프로젝트나 장기적인 운영에 초점을 맞추는 경우 S3 \u0026amp; DynamoDB 옵션이 더 적합하다.\n연동 트러블슈팅 DynamoDB 연동을 위해 해결했던 문제점은 두가지였다. 항목별로 살펴보겠다.\nHelm Value Override 문제 Loki 스토리지 연동 설정 문제 Helm Value Override 문제 지난 글에서 스토리지 설정을 S3 및 기타 스토리지로 설정했음에도 기본 스토리지 (boltdb \u0026amp; snipper) 가 설정되어 관련 문제를 확인하였다. helm template 명령어를 통해서 스토리지 차트 랜더링을 확인하니 Values 오버라이드 값이 아닌 기본 값(boltdb \u0026amp; snipper)이 입력되어 있는 것을 확인할 수 있다.\n1 helm template loki grafana/loki -f loki.yaml -n loki --version 4.8.0 \u0026gt; result.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # Source: loki/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: loki labels: helm.sh/chart: loki-4.8.0 app.kubernetes.io/name: loki app.kubernetes.io/instance: loki app.kubernetes.io/version: \u0026#34;2.7.3\u0026#34; app.kubernetes.io/managed-by: Helm data: # override 적용 안된다! config.yaml: | auth_enabled: false common: compactor_address: \u0026#39;loki-read\u0026#39; path_prefix: /var/loki replication_factor: 3 storage: s3: bucketnames: chunks insecure: false s3forcepathstyle: false limits_config: enforce_metric_name: false max_cache_freshness_per_query: 10m reject_old_samples: true reject_old_samples_max_age: 168h split_queries_by_interval: 15m memberlist: join_members: - loki-memberlist query_range: align_queries_with_step: true ruler: storage: s3: bucketnames: ruler insecure: false s3forcepathstyle: false type: s3 runtime_config: file: /etc/loki/runtime-config/runtime-config.yaml schema_config: configs: - from: \u0026#34;2022-01-11\u0026#34; index: period: 24h prefix: loki_index_ object_store: s3 schema: v12 store: boltdb-shipper server: grpc_listen_port: 9095 http_listen_port: 3100 storage_config: hedging: at: 250ms max_per_second: 20 up_to: 3 table_manager: retention_deletes_enabled: false retention_period: 0 --- 스토리지 연동 부분인 data.config.yaml.schema_config 와 data.config.yaml.storage_config 에 기본 값이 적용되어 있는 것을 확인할 수 있다. 원인은 Loki 헬름 차트에서 if 문법과 override 값에서 생긴 랜더링 문제로 확인된다. 관리성 문제로 overrider 파일의 값을 적용시키려 했지만, 적용되지 않았다. (헬름 차트에 정확한 순서 및 문법을 검사했음에도..)\n우회적 해결 방법으로 기본 차트 vaules.yaml 에 값을 직접 입력하였다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # vaule.yaml Loki: ... config: | .... table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 chunk_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 ... # vaule.yaml Loki: ... # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas schemaConfig: configs: - from: \u0026#34;2022-01-11\u0026#34; store: aws object_store: s3 schema: v12 index: prefix: loki_ # -- Check https://grafana.com/docs/loki/latest/configuration/#ruler for more info on configuring ruler rulerConfig: {} # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig` structuredConfig: {} # -- Additional query scheduler config query_scheduler: {} # -- Additional storage config storage_config: hedging: at: \u0026#34;250ms\u0026#34; max_per_second: 20 up_to: 3 aws: s3: s3.ap-northeast-2.amazonaws.com bucketnames: han-loki region: ap-northeast-2 access_key_id: \u0026lt;access-key\u0026gt; secret_access_key: \u0026lt;aws-secret-key\u0026gt; dynamodb: dynamodb_url: dynamodb.ap-northeast-2.amazonaws.com ... config: | .... table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 chunk_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 ... ## Loki 스토리지 연동 설정 문제 DynamoDB 스토리지로 선택이 되나, loki-read Pod에서 다음의 연동 오류가 발생한다. ```bash level=info ts=2023-04-16T13:31:40.414190216Z caller=http.go:279 org_id=fake msg=\u0026#34;ended tailing logs\u0026#34; tenant=fake selectors=\u0026#34;{stream=\\\u0026#34;stdout\\\u0026#34;,pod=\\\u0026#34;loki-canary-c8s22\\\u0026#34;}\u0026#34; level=info ts=2023-04-16T13:31:50.419151688Z caller=http.go:276 org_id=fake msg=\u0026#34;starting to tail logs\u0026#34; tenant=fake selectors=\u0026#34;{stream=\\\u0026#34;stdout\\\u0026#34;,pod=\\\u0026#34;loki-canary-c8s22\\\u0026#34;}\u0026#34; level=error ts=2023-04-16T13:31:50.422289139Z caller=series_index_store.go:583 org_id=fake msg=\u0026#34;error querying storage\u0026#34; err=\u0026#34;QueryPages error: table=loki_: MissingRegion: could not find region configuration\u0026#34; level=error ts=2023-04-16T13:31:50.422642663Z caller=series_index_store.go:583 org_id=fake msg=\u0026#34;error querying storage\u0026#34; err=\u0026#34;QueryPages error: table=loki_: MissingRegion: could not find region configuration\u0026#34; level=info ts=2023-04-16T13:31:50.422757665Z caller=http.go:279 org_id=fake msg=\u0026#34;ended tailing logs\u0026#34; tenant=fake selectors=\u0026#34;{stream=\\\u0026#34;stdout\\\u0026#34;,pod=\\\u0026#34;loki-canary-c8s22\\\u0026#34;}\u0026#34; level=info ts=2023-04-16T13:32:00.112792136Z caller=http.go:276 org_id=fake msg=\u0026#34;starting to tail logs\u0026#34; tenant=fake selectors=\u0026#34;{stream=\\\u0026#34;stdout\\\u0026#34;,pod=\\\u0026#34;loki-canary-q52t8\\\u0026#34;}\u0026#34; level=error ts=2023-04-16T13:32:00.11538504Z caller=series_index_store.go:583 org_id=fake msg=\u0026#34;error querying storage\u0026#34; err=\u0026#34;QueryPages error: table=loki_: MissingRegion: could not find region configuration\u0026#34; level=error ts=2023-04-16T13:32:00.115479121Z caller=series_index_store.go:583 org_id=fake msg=\u0026#34;error querying storage\u0026#34; err=\u0026#34;QueryPages error: table=loki_: MissingRegion: could not find region configuration\u0026#34; 해당 이슈는 Loki 깃허브에서 확인이 가능했으나 답변이 없었다.\nhttps://github.com/grafana/loki/issues/1957\n원인은 Loki DynamoDB 스토리지 설정으로 생긴 문제였다. 깃 이슈를 확인하면 Loki 공식문서에서 제공하는 파라미터를 통해 설정한 것을 확인할 수 있었는데, 그대로 따라하면 오류가 발생한다.\nhttps://grafana.com/docs/loki/latest/configuration/#aws_storage_config\n필자의 경우 에러 메세지를 기반으로 소스 코드를 분석했다. 결론적으로는 DynamoDB Struct에 Region 설정값이 없어서 생긴 오류였다. 코드 분석 과정은 다음과 같다.\n에러 메세지을 찾아 확인하니 ValidateEndpointHandler 함수에서 발생하는 에러였다. 해당 함수는 AWS SDK의 일부로 AWS 서비스 요청에 대한 엔드포인트와 리전 정보를 검증하는 함수이다.\n해당 함수의 호출은 AWS 클라이언트를 생성하고, 이를 사용하여 서비스 요청을 시작할 때 발생한다. 각 스토리지 S3, DyanmoDB 클라이언트 구조체는 다음과 같은데 S3에는 region이 있지만, Dynamodb 에서는 해당 변수가 없었다.\n아래와 같이 DynamoDB는 변수 dynamdb.dynamodb_url 이 Endpoint 및 시크릿 키를 입력받도록 되어 있다. 이는 Loki 버전 업데이트 전(Version 2.0 이하)의 변수 입력과 동일하다. 필자가 추정하건데 차트와 연동하여 스토리지 변수 설정 부분은 S3, GCS, Azure 만 업데이트되어 있는 것 같다.\n오류 해결은 간단하다. Loki 스토리지 예제를 참고하여 DynamoDB_URL에 Region 과 AWS 키 값 형식을 확인하며 차트에 반영하면 된다. Loki 스토리지 예제는 다음과 같이 확인할 수 있었다.\nhttps://grafana.com/docs/loki/latest/configuration/examples/\n이를 반영하여 최종적으로 수정한 차트는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # values.yaml Loki: ... ... config: | .... table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 chunk_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 ... # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas schemaConfig: configs: - from: \u0026#34;2022-01-11\u0026#34; store: aws object_store: s3 schema: v12 index: prefix: loki_ ... # -- Check https://grafana.com/docs/loki/latest/configuration/#ruler for more info on configuring ruler rulerConfig: {} # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig` structuredConfig: {} # -- Additional query scheduler config query_scheduler: {} # -- Additional storage config storage_config: hedging: at: \u0026#34;250ms\u0026#34; max_per_second: 20 up_to: 3 aws: s3: s3://\u0026lt;AWS-ACCESS-KEY\u0026gt;:\u0026lt;AWS-SECRET-KEY\u0026gt;@ap-northeast-2/han-loki dynamodb: dynamodb_url: dynamodb:///\u0026lt;AWS-ACCESS-KEY\u0026gt;:\u0026lt;AWS-SECRET-KEY\u0026gt;@ap-northeast-2 앞선 깃 이슈에서 dynamodb_url에 inmemory:/// 로 변수를 설정하는 것이 있었는데 이는 가상의 인메모리로 구현시 사용된다고 한다. 연동시 헷갈리지 말자 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # values-loki.yaml tableManager: enabled: true test: enabled: true monitoring: lokiCanary: enabled: true selfMonitoring: enabled: true loki: auth_enabled: false Loki Config 부분을 제외하고는 오버라이드가 가능하여 나머지 설정 부분은 다음과 같이 설정하였다. 배포 확인 차트 배포를 통해 연동을 해보자.\n1 2 3 4 kubectl create ns loki helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm install loki grafana/loki -f values.yaml -f values-loki.yaml -n loki --version 4.8.0 배포 후 AWS 콘솔에서 확인하면 S3에는 청크가 DynamoDB에는 인덱스가 저장되는 것을 확인할 수 있다.\nS3 Chunk 확인\nDynamoDB 인덱스 조회 확인\n성공이다..! 돌고 돌아 공식 문서에서의 예제로 연동에 성공하였다. 헬름 차트 랜더링과ㅊ 파라미터 변수의 예제를 꼭 확인하도록 하자!\n운영시 고려사항 배포 과정에서 고려사항이 있어 추가로 남겨둔다.\n스토리지 사용 비용 첫 쨰로 비용이다. DynamoDB 연동시 기본 읽기용량이 100, 쓰기용량이 25로 설정되어 있다. 비용 계산를 확인 하자면 다음과 같다.\n한국 DynamoDB 비용\n쓰기 요청 유닛(WRU): 1,000,000 건당 $1.3556 읽기 요청 유닛(RRU): 1,000,000 건당 $0.271 기본 읽기용량이 100, 쓰기용량이 25로 설정된 경우, 용량 단위에 대한 실제 요청 수를 알아야 한다.\n예를 들어, 시간당 10,000건의 쓰기 요청과 50,000건의 읽기 요청이 있다고 가정하겠다.\n쓰기 요청 비용: 10,000 WRU x ($1.3556 / 1,000,000) = $0.013556 읽기 요청 비용: 50,000 RRU x ($0.271 / 1,000,000) = $0.01355 총 비용은 $0.013556 + $0.01355 = $0.027106 (1h) 이다.\n운영 환경에서 배포시 시스템 규모를 파악하여 쓰기 및 읽기 요청에 따라 용량을 설정하도록 하자. 용량 설정은 위 차트에서의 table_manager 에서 설정이 가능하다. 필자는 5로 설정했었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # values.yaml Loki: ... ... config: | .... table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 chunk_tables_provisioning: provisioned_write_throughput: 5 provisioned_read_throughput: 5 ... ","date":"Apr 14","permalink":"https://HanHoRang31.github.io/post/loki-2-8-0/","tags":["KANS","kops","cloud","AWS","kubernetes","PLG","loki","promtail"],"title":"Loki 최신버전(V2.8.0) DyanmoDB 연동하기"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 5주차 시간에는 모임장님께서 쿠버네티스 보안을 주제로 학습 내용을 공유해 주셨다. 이번 블로그 글에서는 쿠버네티스 보안에 대해 스터디한 내용을 공유하고자 한다.\nKubernetes 4C Layer 쿠버네티스 공식문서에 따르면 쿠버네티스 보안은 4계층(클라우드/ 클러스터 / 컨테이너 / 코드)으로 구성되며, 각 계층에 대해 보안 관점이 필요하다고 한다.\nhttps://kubernetes.io/docs/concepts/security/overview/\nCloud, Infra : 클라우드 계층에서는 쿠버네티스 클러스터가 실행되는 기반 인프라에 초점을 맞춘다. 이 계층에서는 가상 머신, 네트워크, 스토리지 및 기타 자원에 대한 보안을 강화하고, 클라우드 제공 업체의 보안 도구 및 기능을 활용하는 것으로 초점이 맞춰져 있다. 인프라 보호 대상과 클라우드 제공 업체(AWS) 의 제공 보안은 다음의 표를 참고하자. 보호 대상 AWS EKS에서의 보안 기능 API 서버에 대한 네트워크 액세스 클러스터 구축시 액세스 지점에 대해 Elastic Load Balancer를 구성한다. 이를 통해 API 서버에 대한 액세스를 VPC 내부로 제한하거나 인터넷 게이트웨이를 통해 인터넷에서도 접근이 가능하도록 설정할 수 있다. 또한, AWS Identity and Access Management (IAM)을 사용하여 사용자와 역할에 대한 접근 제어를 구성할 수 있다. 노드에 대한 네트워크 액세스 VPC 서비스를 이용하여 노드에 대한 액세스를 VPC 내부로 제한하거나 인터넷 게이트웨이를 통해 인터넷에서도 접근이 가능하도록 설정할 수 있다. 또한 보안 그룹을 통해 IP 주소 범위에 대한 인바운드 및 아운바운드 트래픽을 제어할 수 있다. 클라우드 제공 업체 API 에 대한 쿠버네티스 액세스 IAM 권한의 접근 키를 다룬다. 접근 키가 탈취되면 해당 리소스에 대한 제어가 탈취된다. 공식 문서에서는 최소 권한 원칙을 액세스 권한을 부여할 것을 권고한다. etcd에 대한 액세스 etcd 데이터베이스가 EKS 완전 관리형 컨트롤 플레인 내부에 숨겨져 있으며, 사용자는 etcd에 직접 액세스할 수 없다. etcd 암호화 기본적으로 EKS 클러스터를 생성할 때, etcd 데이터를 암호화하는 옵션을 선택할 수 있다. 이렇게 설정하면, 클러스터의 etcd 데이터는 AWS Key Management Service (KMS)에서 제공하는 고객 관리형 키 (CMK)를 사용하여 암호화되어 기밀성이 보장된다. Cluster: 클러스터 계층에서는 쿠버네티스 클러스터 자체의 보안에 중점을 둔다. 클러스터 상의 중요한 서비스 A와 보안이 취약한 서비스 B에 대해 격리를 위한 계층이라고 보면 된다. 쿠버네티스에서는 격리에 방법으로 다양한 방법을 제안한다. 대표적으로 RBAC 인증, 네트워크 정책, 파드 보안 표준, 인그래스용 TLS 등이 있다. 항목 설명 RBAC 인증 역할(Role) 및 클러스터 역할(ClusterRole)을 사용하여 쿠버네티스 사용자와 서비스 계정에 권한을 부여하는 Role-Based Access Control 방식이다. 이를 통해 세분화된 권한 제어로 클러스터의 보안을 강화할 수 있다. 네트워크 정책 쿠버네티스 네트워크 정책은 특정 파드, 네임스페이스, 또는 IP 범위와 같은 소스로부터 들어오거나 나가는 트래픽을 허용하거나 차단하는 규칙을 정의한다. 이를 통해 네트워크 보안을 강화하고, 민감한 데이터를 처리하는 파드에 대한 접근을 제한할 수 있다. 파드 보안 표준 파드 보안 표준은 컨테이너와 파드가 안전하게 실행되도록 하는데 도움이 되는 일련의 보안 지침 및 구성이다. 예를 들어, 보안 컨텍스트(Security Context), 네트워크 폴리시, 리소스 제한, 파드 안티-어피니티 등을 사용하여 파드의 보안을 강화할 수 있다. 인그래스용 TLS 쿠버네티스 인그래스를 사용하여 클러스터 외부에서 내부 서비스로의 요청을 중앙 집중식으로 관리할 때, TLS(Transport Layer Security)를 사용하여 클라이언트와 서버 간 통신을 암호화하여 데이터를 보호할 수 있다. 인증서와 개인 키를 제공하고 호스트 이름과 인증서의 일치 여부를 확인해야 한다. Container: 컨테이너 계층에서는 실행되는 워크로드에 직접적으로 영향을 주는 컨테이너에 초점을 맞춘다. 이 계층에서는 컨테이너 이미지 보안, 리소스 격리, 시크릿 관리 및 네트워크 정책을 포함된다. 여기에서는 보안 컨텍스트, 네트워크 폴리시, 컨테이너 런타임의 보안 기능(이미지 스캔) 등을 사용하여 컨테이너의 보안을 강화한다. 항목 설명 보안 컨텍스트 (Security Context) 쿠버네티스에서 컨테이너 또는 파드에 적용되는 보안 설정을 정의하는데 사용된다. 보안 컨텍스트를 사용하여 컨테이너의 파일 시스템에 대한 액세스 권한, 프로세스 ID, 사용자 ID, 그룹 ID 등을 제어할 수 있다. 이를 통해 컨테이너와 파드가 안전하게 실행되도록 할 수 있으며, 호스트 시스템과 다른 컨테이너로부터 분리되어 보안을 강화할 수 있다. 컨테이너 런타임의 보안 기능 컨테이너 런타임(예: Docker, containerd, CRI-O 등)은 컨테이너를 실행하고 관리하는데 사용되는 소프트웨어이다. 컨테이너 런타임은 다양한 보안 기능을 제공하여 컨테이너의 보안을 강화할 수 있다. 예를 들어, 컨테이너 런타임은 네임스페이스를 사용하여 컨테이너 프로세스를 격리할 수 있으며, cgroups을 사용하여 리소스 사용량을 제한하고, seccomp, AppArmor, SELinux와 같은 보안 프로파일을 적용하여 컨테이너의 시스템 호출을 제한할 수 있다. Code: 코드 계층에서는 애플리케이션의 소스 코드 및 구성에 초점을 맞춘다. 이 계층에서는 애플리케이션의 취약점 및 보안 결함을 찾고 수정하여 워크로드의 보안을 향상시켜야 한다. 방법으로는 TLS 액세스, 통신 포트 제한, 종속성 보안, 정적 및 동적 소스 코드 분석, 의존성 스캔 및 안전한 코딩 기법 등이 있다. 항목 설명 통신 포트 제한 통신 포트 제한은 서버, 컨테이너, 애플리케이션 등이 사용하는 네트워크 포트를 제한하여 보안을 강화하는 방법이다. 불필요한 포트를 차단하고, 필요한 포트에 대해서만 허용하면 악의적인 공격자가 시스템에 액세스하는 것을 방지할 수 있다. 종속성 보안 종속성 보안은 애플리케이션에서 사용하는 외부 라이브러리 및 패키지에 대한 보안을 관리하는 방법이다. 취약한 종속성을 사용하면 시스템이 공격에 노출될 수 있다. 따라서, 종속성을 최신 상태로 유지하고, 보안 취약점이 발견될 경우 적절한 조치를 취하는 것이 중요하다. 정적 및 동적 소스 코드 분석 정적 소스 코드 분석은 코드를 실행하지 않고 소스 코드를 검사하여 보안 취약점을 찾는 방법이다. 동적 소스 코드 분석은 애플리케이션을 실행하면서 코드를 분석하여 보안 취약점을 찾는 방법이다. 이러한 분석 방법들을 사용하여 애플리케이션 코드의 보안 취약점을 발견하고 수정할 수 있다. 의존성 스캔 및 안전한 코딩 기법 의존성 스캔은 애플리케이션의 종속성에 대한 보안 취약점을 찾기 위한 도구를 사용하는 것이다. 안전한 코딩 기법은 개발자가 코드를 작성할 때 고려해야 하는 보안 지침 및 원칙이다. 이러한 기법을 사용하면 애플리케이션의 보안을 강화하고, 취약점을 줄일 수 있다. 보안적으로 신경써야할 요소가 많다. 위의 설명한 보안 기능들을 전부 다루면 좋겠지만 내용이 방대하다. 이번 블로그 글에서는 보안 툴인 kubescape를 이용하여 앞서 4C에서 Cluster, Container, Code 계층의 보안 검증 과정을 다뤄보겠다.\nKubescape 쿠버네티스 클러스터 보안 설정을 평가하고 검증하는 오픈 소스이다. NSA와 MITRE의 Kubernetes Hardening Guidance를 기반으로 작동하며 웹 대시보드를 통해 검증 및 취약점 점검을 할 수 있다. 또한, 도커 레지스트리, 이미지 취약점 스캔이 가능하다. 23년 4월 기준으로 업데이트가 계속 진행 중이며 공식 문서 또한 정리가 잘 되어 있다.\n그렇지만 제한적인 요소도 존재한다. 클러스터를 ARMO 웹 대시보드로 연결해야한다는 점으로 온프레미스에서 도입시 고려해야 한다. 또한 프리티어 기준 워크 노드가 10개로 제한된다.\n보안 기능을 전부 사용하려면 Operator 설치가 동반된다. 아키텍처는 다음과 같이 구성된다.\nhttps://github.com/kubescape/helm-charts/blob/master/charts/kubescape-cloud-operator/README.md\nMaster Gateway: ARMO 백엔드에서 실행 중인 마스터 게이트웨이이며 사용자가 등록한 모든 게이트웨이에 메시지를 브로드캐스트하여 모든 클러스터 게이트웨이에 런타임 작업을 전달한다. In-cluster Gateway: 클러스터 내에서 다른 구성 요소와 통신하기 위해 마스터 게이트웨이와 연결되며, 웹소켓(Websocket)을 사용하여 등록된다. 브로드캐스트 메세지를 전달받아 다른 컴포넌트에 전달한다. Operator : 트리거 엔진으로 게이트웨이에서 잔달받은 작업을 실행하거나 스케줄링하는 역할을 담당한다. Kubevuln : 컨테이너 이미지 취약점을 스캔하는 컴포넌트이다. 취약점 스캔은 grype을 통해 진행한다. Kubescape : 클러스터 내부 검증을 스캔하는 컴포넌트이다. Kollector : kubernetes API Server와 통신하여 클러스터 정보와 변경 정보를 확인하며 정보를 백엔드 CloudEndpoint로 전달한다. 설치 배포 환경 : kops 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스\n먼저 https://cloud.armosec.io/ 회원가입이 필요하다. 회원가입 이후 회원 ID 값에 맞게 helm 설치 명령어가 나온다. 복사하여 클러스터에 설치하자.\n설치 이후 Verfiy installation 버튼을 누르면 아래와 같이 연결이 안된다고 나오나 kops 클러스터에서 설치했을 때의 버그같다.\n아래와 같이 로그 확인 및 대시보드를 확인하면 정상적으로 등록이 되어 있다.\n1 kubectl -n kubescape logs -f $(kubectl -n kubescape get pods | grep kollector | awk \u0026#39;{print $1}\u0026#39;) 기능 확인 대시보드를 접속하면 왼쪽의 메뉴를 통해서 보안 검증이 가능하다.\nCompliance : 쿠버네티스 준수 정책 검증 Vulnerabilities : 컨테이너 파드 취약점 점검 RBAC Visualizer : RBAC 시각화 Repository Scanning : 레파지토리 스캐닝 Registry Scanning : 도커 레지스트리 스캐닝 Compliance 쿠버네티스 모범 사례를 체계화하여 수백 개의 항목들을 통해 클러스터 검증시켜주는 기능이다. 검증 항목은 MITRA 와 NSA 에서 제시하는 보안 가이드라인이며 웹 대시보드에서 프레임워크별 제어가 가능하다.\n또한, 보안이 필요한 항목에는 fix버튼을 통해 구성적으로 확인이 가능하다.\n아래는 필자 클러스터 환경에서 high 레벨 항목이며 몇 가지 항목 원인은 다음과 같다.\nC-0057(Privileged container) : 호스트 시스템의 모든 기능을 포함하는 컨테이너가 있어서 발생한 항목이다. 파드 구성 중 spec.container.securityContext.privileged == true 일 때 발생한다. 필자의 경우 볼륨 관리 파드(ebs-csi-node) 에서 발생했다. C-0045(Writable hostPath mount) : 쓰기 가능한 hostPath 볼륨으로 컨테이너를 생성했을 때 발생하는 알람이다. 설명에서는 이를 통해 호스트 볼륨의 정보를 얻을 수 있다 한다. 파드 구성 중 mount.readOnly == false 이 됐을 때 발생한다. C-0015 (kubernetes secret list) : 쿠버네티스 사용자가 시크릿에 대해 접근이 가능할 때 발생하는 알람이다. C-0041(HostNetwork access) : 호스트 네트워크에 연결할 경우 발생하는 알람이다. 필자의 경우 external-dns 파드에서 발생했다. 항목을 살펴보니 대부분 add-on 파드들에 대해 발생한 경고이다. 이러한 경우 ignore 로 알람 무시가 가능하다.\n각 모범 사례 기준이 엄격하고 서드 파티 레벨의 애플리케이션의 구성 정보 확인시 검증 과정에서 유용하게 사용할 수 있을 것 같다.\nVulnerabilities 취약점 점검으로 배포 컨테이너들에 대해 취약점 점검을 진행한다. 취약점 점검은 grype 엔진을 통해 진행되며 CVE(Common Vulnerabilities and Exposures, 미국 NSA, CISA에서 제공하는 보안 가이드라인) 식별자로 검사가 진행된다.\n위와 마찬가지로 필자의 클러스터에서 CRITICAL한 레벨의 취약점을 살펴보겠다.\nGHSA-r48q-9g5r-8q2h(CVE-2022-1996) : emicklei/go-restful의 버전 3.8.0 이전에서 발견된 취약점이다. 사용자가 제어할 수 있는 키를 통해 권한을 우회할 수 있는 보안 문제가 있다고 한다. 한 가지 의문은 kubescape 내 파드인 kollector 에서 발생하는 것인데, 깃허브 이슈와 커밋에도 없는 사항이라 깃허브 이슈로 등록했다.\n그 외에도 metrics-server, coredns 파드에서도 발생했으며 관련 깃 이슈를 공유한다. 대부분 해당 패키지 버전 업데이트로 피드백하며 업데이트하는 것 같다.\nRBAC Visualizer 쿠버네티스 role 에 부여된 오브젝트 접근 제어 권한에 대해 시각화 기능을 제공해준다. 운영적으로 매력적인 기능이다. 아래 그림처럼 쿠버네티스 ServiceAccount 별로 접근 제어 목록을 한 눈에 확인할 수 있어 불필요한 권한 및 접근 제어에 대해 검증이 가능할 것 같다.\nRepository scanning 코드 관리 저장소인 깃허브, 깃랩 등의 레파지토리에 스캐닝 검사를 시켜준다.\n레파지토리 등록은 공식문서를 참고하자. 필자는 mac 환경에서 진행했다.\n깃허브 토큰을 환경 변수로 등록하고, kubescape CLI 를 통해 취약점 검사 및 웹 대시보드로 정보를 전달한다.\n1 export GITHUB_TOKEN=my-access-token Location 은 저장소 URL 이다. kubescape CLI 를 통해 검사하면 웹 대시보드에서 확인이 가능하다. 아래 화면은 필자의 깃허브 레파지토리를 등록해서 검사하였다.\n레파지토리의 코드와 헬름 차트의 value 값들에도 취약점 검사가 진행된다.\n필자의 레파지토리의 경우 C-0009(Resource limits) 항목이 많아 확인해보니 resource limit 설정 문제였다. 테스트 환경에서 진행함으로 리소스 제한을 풀었지만 운영 환경에서 도입시 파드 구성 환경에 따라 설정해야겠다.\nRegistry Scanning 이미지 레파지토리에도 검사가 가능하다. 하버에서도 이미지 취약점 점검이 가능하는 것으로 알고 있는데 차이점을 확인해보겠다. 기능 확인을 위해 하버를 클러스터에 설치하여 스캐닝 기능을 테스트하였다. 등록시 스캐닝 주기와 태그 개수를 설정할 수 있다.\n시간별로 이미지 레파지토리에 취약 점검이 가능하다. 마찬가지로 grype 엔진을 통해 이미지 취약점 점검을 진행한다.\nINTEGRATIONS kubescape 툴은 Code, CI / CD 에도 활용이 가능하다.\nhttps://github.com/kubescape/kubescape\n공식문서를 확인하니 jenkins, gitlab CI / CD 에서도 job 스케쥴링으로 취약점 점검이 가능하다. CI / CD 파이프라인 구축 이후 고려해보도록 하자.\n3rd party 애플리케이션 연동을 통해 알람 구성도 쉽게 가능하다. 슬랙에서도 알람 설정이 쉽게 가능한데, 스캐닝과 점검 레벨에 따라 알람 설정이 가능하다.\n마치며 이번 블로그 글에서는 쿠버네티스 보안 계층과 보안 툴인 kubescape를 살펴보았다. 개인적으로 쿠버네티스의 보안 생태계를 경험하여 역량 향상에 도움이 되었다. 필자가 생각하기에 kubescape 툴은 미국 가이드라인의 사례나 취약점 점검과 grype 엔진을 통합해서 평가해주는 operator 툴 느낌이 강했다. 비즈니스 모델에 따라 달라지겠지만 비슷한 operator 툴을 대안으로 찾으면 좋을 것 같다.\n","date":"Apr 07","permalink":"https://HanHoRang31.github.io/post/pkos-5-security/","tags":["KANS","kops","cloud","AWS","kubernetes","security","kubescape"],"title":"[PKOS] 쿠버네티스 보안과 Kubescape DeepDive"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 스터디 4주차 시간에는 쿠버네티스 모니터링과 로깅 시스템을 구축하여 기능들을 살펴보았다. 이번 블로그 글에서는 모니터링 시스템에 대해 심화 학습한 내용들을 공유하고자 한다.\n모니터링은 어떤 대상을 감시, 감찰한다는 뜻으로 모니터링의 목적은 지속적인 감시, 감찰을 통해 대상의 상태나 가용성, 변화 등을 확인하고 대비하는 것이다. 모니터링의 개념처럼 쿠버네티스 모니터링도 똑같다. 쿠버네티스에서 특정 기간에 측정한 일련의 숫자(메트릭)에 대해 감시와 감찰을 통해 대상의 상태나 가용성 변화를 확인하고 대비한다고 보면 되겠다. 쿠버네티스 모니터링 시스템으로는 Prometheus, InfluxDB, DataDog, 클라우드 프로바이더 등이 있으나 이번 블로그 글에서는 오픈소스 모니터링 시스템인 Prometheus와 기능 확장 시스템인 Thanos를 다루겠다.\nPrometheus 오픈 소스 모니터링 시스템이다. 시계열 데이터 수집, 저장 및 쿼리 기능을 제공하고 다양한 경고 기능을 제공한다. 오픈소스 진영에서 가장 많이 사용하는 모니터링 시스템으로 사실상 거의 표준처럼 사용하고 있다. 아키텍처는 다음과 같다. (공식문서)\nhttps://prometheus.io/docs/introduction/overview/\n빨간 네모로 표시된 것이 프로메테우스 구성 컴포넌트이다.\nPrometheus Server : Prometheus 서버는 메트릭 수집, 저장, 처리 및 쿼리 기능을 수행한다. 메트릭 수집 방식으로 Pull 방식을 기본적으로 사용한다. 해당 서버가 대상 서비스로부터 메트릭을 주기적으로 수집하고, 시계열 데이터베이스(TSDB, HDD/SDD)에 저장한다. 데이터베이스에 저장한 데이터는 쿼리 언어 PromQL을 통해 데이터를 필터링, 집계 시각화하는데 사용한다. Pushgateway : Pushgateway는 Push 방식을 사용하는 일부 유형의 메트릭을 Prometheus에서 수집하기 위한 중간 서버이다. 주로 일회성 작업(예: 배치 작업)으로부터 메트릭을 수집하는 데 사용된다. 작업이 종료되더라도 메트릭이 보존되어 Prometheus 서버가 해당 메트릭을 수집할 수 있게 한다. Alertmanager: Alertmanager는 Prometheus 서버에서 발생한 경고를 관리하고, 사용자에게 알림을 전달하는 컴포넌트이다. Prometheus UI : 내장된 웹 인터페이스로, 사용자가 Prometheus 서버에서 메트릭을 쿼리하고, 시각화된 그래프를 확인할 수 있다. 사용자는 PromQL을 사용하여 원하는 메트릭을 검색하고 분석할 수 있으며, 기본적인 대시보드 및 경고 설정을 관리할 수 있다. 아키텍처를 살펴보았는데 프로메테우스는 단일 노드 시스템으로 설계되어 있어 클러스터링 구조를 직접 지원하지 않는다. 이로인해 확장성과 고가용성에 일부 보완이 필요하다.\n확장성 문제 단일 노드에서 모든 메트릭을 처리하려 할 때 노드의 자원이 고갈되어 성능 저하를 초래할 수 있다. 대규모 인프라에서 많은 수의 메트릭을 수집하고 처리하는 데 있어 성능 저하와 저장소 부족 문제가 발생할 수 있다. 외부 스토리지 연결이 필요하다. 고가용성 문제 단일 노드에서 발생하는 장애나 다운타임이 생겨 프로메테우스 서버가 내려가면 그 시간 동안에는 메트릭을 수집할 수 없다. 볼륨이 AWS EBS 를 사용해도 단일 노드에서만 연결이 가능하다. 연결 노드에 다운 타임이 발생하면 메트릭을 가져올 수 없다. 이러한 문제를 해결하기 위한 도구로 Thanos를 사용할 것이다.\nThanos 프로메테우스의 확장성과 고가용성을 개선하기 위한 시스템이다. 사이트 정문에 대놓고 프로메테우스를 저격하고 있다. 타노스 아키텍처를 통해 어떻게 개선할 수 있는 확인해보겠다.\nhttps://thanos.io/v0.6/thanos/getting-started.md/\n파란 네모가 타노스 구성 컴포넌트이다. 설계 디자인은 공식 문서에서도 참고가 가능하다.\nThanos Sidecar : Prometheus에 연결되어 메트릭 데이터를 쿼리하고 클라우드 스토리지에 업로드한다. 노드마다 사이드카가 연결되며 외부 스토리지 저장을 통해 확장성을 개선시키는 역할의 컴포넌트이다. Thanos Store Gateway : 외부 스토리지에 메트릭 데이터를 읽어 Thanos Query로 전달한다. 해당 컴포넌트를 통해 외부 스토리지에서 과거 데이터도 쿼리할 수 있게 된다. Thanos Query : 사용자 쿼리를 요청 처리하며 짧은 시간의 데이터는 타노스 사이드카에서 가져오며, 오래된 데이터는 스토어 게이트웨이를 통해 외부 스토리지에서 가져온다. Prometheus Query API를 구현하여 사용자가 기존의 Prometheus 쿼리를 그대로 사용할 수 있게 한다. 프로메테우스단에서 고가용성을 제공해주는 컴포넌트이다. 통합 데이터간의 중복 제거 (de-duplication) 기능을 기본으로 제공하여 여러 프로메테우스 및 원격 스토리지의 메트릭 데이터를 통합하여 쿼리할 수 있게 해준다. 한 가지 주의할 점은 Thanos query 도 고가용성을 보장해줘야 한다. 공식 문서에 따르면 타노스 구성 파드들은 샤딩 수단을 제공하지 않아, 모두 수평적 확장이 가능하다. 타노스 배포시 쿼리 파드 개수를 2개 이상으로 조절하여 가용성을 보장시키자.\nhttps://observability.thomasriley.co.uk/prometheus/using-thanos/high-availability/\nhttps://thanos.io/tip/thanos/design.md/#metric-sources\nThanos Compactor : 타노스 쿼리와는 별개의 프로세스로, 객체 스토리지 버킷만 가리키며 여러 개의 작은 블록을 더 큰 블록으로 지속적으로 통합시켜주는 컴포넌트이다. 블록을 통합시키면 데이터가 압축하게 되므로 버킷의 총 스토리지 크기, 스토어 노드의 로드 및 버킷에서 쿼리 데이터를 가져오는 데 필요한 요청 수가 크게 줄어든다. Thanos Ruler : 프로메테우스 인스턴스들로부터 알림 규칙 정보를 가져와 통합하고, 프로메테우스와 함께 작동하는 외부 알림 시스템에게 알림을 전송하는 역할의 컴포넌트이다. 연계 배포 배포 환경 : kops 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스\n프로메테우스 \u0026amp; 타노스를 연계하여 배포한다. 배포를 위해 헬름 차트를 사용할 예정이며 프로메테우스 배포는 kube-prometheus-stack(그라파나, 추가 메트릭 자동 구성) 차트를 사용할 것이다. 또한 타노스는 bitnami/thanos 차트를 사용할 것이며 타노스 외부스토리지는 MinIO를 배포하여 연결할 것이다. 전체 배포 순서는 다음과 같다. 구성 차트는 필자의 깃허브에서 참고가 가능하다.\nMInIO 배포 kube-promethes-statck 설정 \u0026amp; 배포 타노스 설정 \u0026amp; 배포 그라파나 설정 및 대시보드 확인 1. MinIO 배포 타노스 외부 스토리지로 MinIO를 설정할 것이다. 이를 위한 사전 작업으로 MinIO를 먼저 배포하겠다.\n차트 가져오기\n1 2 3 helm repo add minio https://charts.bitnami.com/bitnami helm repo update helm fetch minio/minio --untar --version 12.2.1 차트 설정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # values-minio.yaml mode: distributed auth: rootUser: admin rootPassword: \u0026#34;admin1234\u0026#34; statefulset: replicaCount: 4 zones: 1 drivesPerNode: 1 provisioning: config: - name: region options: name: ap-northeast-2 ingress: enabled: true hostname: minio.hanhorang.link path: /* annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}, {\u0026#34;HTTPS\u0026#34;:9090}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: \u0026#34;$ACM arn \u0026#34; persistence: storageClass: \u0026#34;kops-csi-1-21\u0026#34; 분산스토리지 모드로 설정 (테스트환경 노드 4개) 노드당 파드 하나를 할당 Ingress(네트워크) : AWS ALB 설정 persistence(볼륨) : AWS gp2 기본 스토리지 클래스 설정 배포\n1 2 kubectl create ns minio helm install minio minio/minio -f values-minio.yaml -n minio --version 12.2.1 버킷 생성 및 접근 키 발급\n타노스에서 minio 버킷에 접근하기 위한 접근 키를 발급받자\nminio 도메인 접속\n어드민 계정은 차트에서 admin / admin1234 로 설정되어 있다. 로그인을 하자.\n로그인이 완료되면 다음과 같은 화면을 확인할 수 있다.\n버킷 생성 후 버킷 접근을 위한 액세스 키 발급이 필요하다. 왼쪽 메뉴 [Access Keys] 에서 키를 발급받자.\nAccess Key 발급 후 MINIO 동작 권한을 등록해야 한다. 생성한 키를 클릭하면 정책 입력 칸이 나온다. 아래 정책을 입력하도록 하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;admin:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::*\u0026#34; ] } ] } 필자의 경우 접근 키는 다음과 같이 생성되었다.\naccess_key : aajl91wFPCRVmfWR\nsecret_key: SfP4woqjY3fcyh1cuwF1CNQFEe6hs4X6\n발급받은 키를 기반으로 Secret을 생성하자.\n1 2 3 4 5 6 7 #minio-key.yaml type: s3 config: bucket: thanos endpoint: minio.hanhorang.link access_key: aajl91wFPCRVmfWR secret_key: SfP4woqjY3fcyh1cuwF1CNQFEe6hs4X6 1 2 kubectl create ns monitoring kubectl create secret generic thanos-minio-secret -n monitoring --from-file=minio-key.yaml 2. kube-promethes-statck 설정 \u0026amp; 배포 프로메테우스 배포 및 사이드 카에 타노스 연동을 위한 설정을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm fetch prometheus-community/kube-prometheus-stack --untar --version 45.7.1 # 사용 리전의 인증서 ARN 확인 CERT_ARN=`aws acm list-certificates --query \u0026#39;CertificateSummaryList[].CertificateArn[]\u0026#39; --output text` echo \u0026#34;alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN\u0026#34; KOPS_CLUSTER_NAME=\u0026#34;hanhorang.link\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # values-kube-prometheus-stack.yaml cat \u0026lt;\u0026lt;EOT \u0026gt; ./values-kube-prometheus-stack.yaml alertmanager: enabled: false grafana: defaultDashboardsTimezone: Asia/Seoul adminPassword: admin1234 ingress: enabled: true ingressClassName: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; hosts: - grafana.$KOPS_CLUSTER_NAME paths: - /* prometheus: # 사이드카 노출 서비스 설정 thanosService: enabled: true ingress: enabled: true ingressClassName: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; hosts: - prometheus.$KOPS_CLUSTER_NAME paths: - /* prometheusSpec: podMonitorSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false retention: 5d retentionSize: \u0026#34;10GiB\u0026#34; scrapeInterval: \u0026#34;15s\u0026#34; # alert 관련 설정으로 주석 처리 # evaluationInterval: 15s # 가용성 설정 replicas: 3 # 타노스 설정 thanos: image: \u0026#34;quay.io/thanos/thanos:v0.27.0\u0026#34; objectStorageConfig: key: minio-key.yaml name: thanos-minio-secret version: v0.27.0 # 볼륨 설정 storageSpec: {} ## Using PersistentVolumeClaim ## # volumeClaimTemplate: # spec: # storageClassName: gluster # accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] # resources: # requests: # storage: 50Gi # selector: {} EOT 알람을 사용하지 않음으로 alertmanager false로 설정하였다. 헬름 차트를 보면 prometheus.thanos 에 설정하는 부분이 있는데 여기서 설정하는 것이 아니다! 원격 스토리지 접근에 대한 오류가 발생하므로 prometheus.prometheusSpec.thanos 에 앞서 생성한 시크릿 키를 입력하자. (위에 차트 그대로 입력하면 문제없습니다.) 1 2 3 4 kubectl create ns monitoring helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.7.1 \\ -f values-kube-prometheus-stack.yaml --namespace monitoring 배포 이후 타노스 사이드카 연동을 확인하자.\n1 kubectl describe pods prometheus-kube-prometheus-stack-prometheus-0 -n monitoring 성공이다!\n3. 타노스 설정 \u0026amp; 배포 타노스 사이드카를 제외한 컴포넌트를 설치하고 thnaos query 가 프로메테우스 사이드카로, store gateway가 원격 스토리지인 minio 로 연동할 수 있도록 설정해야 한다. 차트부터 가져오도록 하자.\n1 2 3 helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm fetch bitnami/thanos --untar --version 12.3.2 타노스 연동을 위해 설정을 진행한다. 메트릭을 가져오기 위해 버킷 정보와 프로메테우스 배포시 같이 배포된 타노스 사이드카 서비스 주소를 입력한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 objstoreConfig: |- type: s3 config: bucket: monitoring endpoint: minio.minio.svc.cluster.local:9000 access_key: aajl91wFPCRVmfWR secret_key: SfP4woqjY3fcyh1cuwF1CNQFEe6hs4X6 insecure: true querier: stores: - kube-prometheus-stack-thanos-discovery.monitoring.svc.cluster.local:10901 - thanos-storegateway.monitoring.svc.cluster.local:10901 replicaCount: 2 ingress: enabled: true hostname: thanos.hanhorang.link ingressClassName: \u0026#34;alb\u0026#34; annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:955963799952:certificate/7569648c-bfd5-4860-b2c1-16ef02acbb58 alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;monitoring\u0026#34; path : /* bucketweb: enabled: true compactor: enabled: true storegateway: enabled: true ruler: enabled: false objstoreConfig.config.endpoint 를 서비스 DNS로 대체했다. ALB 도메인 입력시 Timeout 으로 파드가 올라가지 않기 때문이다. querier.store 에 쿼리할 대상을 등록한다. 대상으로 타노스 사이드카의 서비스 주소와 스토어게이트웨이를 등록한다. 배포\n1 2 3 helm install thanos bitnami/thanos --version 12.3.2 \\ -f values-thanos.yaml --namespace monitoring 배포 완료 후 타노스 쿼리 호스트 도메인을 통해 접속하자. Store와 Status/Target를 확인하여 사이드카 연동을 확인한다.\n타노스가 정상적으로 배포된 것을 확인하였다. 배포 이후에는 프로메테우스 서버를 2개 이상 띄어서 프로메테우스 서버가 고가용성을 갖도록 구성하자. (앞서 프로메테우스 배포시 프로메테우스 서버를 3개를 배포하였다)\n3개의 프로메테우스 서버가 서로 독립적으로 메트릭을 수집한다. 타노스 쿼리는 프로메테우스에 등록된 사이드카를 통해 메트릭을 통합 수집한다. 이 때 고가용성이 보장되는데 하나의 프로메테우스가 다운타임이 가진다한들 다른 프로메테우스 서버에서 메트릭 수집 및 집계를 수행할 수 있기때문이다. 물론 중복 중복된 메트릭에 대해선 타노스 내 Use Deduplication 기능을 통해 소거가 가능하다. 중복 메트릭 설정은 프로메테우스 라벨 설정을 통해 가능하나 자동으로 설정이 되어 생략하겠다.\n4. 그라파나 설정 그라파나는 시각화 대시보드이다. 앞서 구축한 모니터링 시스템을 기반으로 메트릭 수집 파이프라인을 구성하고 대시보드를 확인하겠다. 그라파나 도메인에 접속하여 로그인을 진행한다. (초기 아이디: admin, 비밀번호: admin1234)\n먼저, 수집 메트릭 URL을 프로메테우스에서 타노스 쿼리로 수정할 것이다. 왼쪽 하단의 톱니바퀴 메뉴에서 Configuration에 들어간 다음 프로메테우스 설정 URL을 thanos-query:9090 으로 수정하자.\n바꾸고 나서 대시보드를 확인하면 정상적으로 작동하는 것을 확인할 수 있다.\n마치며 kube-prometheus-stack 자체적으로도 프로메테우스 고가용성을 보장한다. 하지만 이렇게 구성한 프로메테우스 HA는 여전히 중복 데이터와 쿼리 집계, 확장성에 대한 보완 요소가 있다. 이를 해결하기 위해 Thanos을 소개하였고 연동 방법과 구성 요소를 확인하였다.\n참고 https://aws.amazon.com/ko/blogs/opensource/improving-ha-and-long-term-storage-for-prometheus-using-thanos-on-eks-with-s3/\nhttps://velog.io/@seokbin/Kube-Prometheus-Thanos-구성#4-프로메테우스-ha-구성\n","date":"Apr 01","permalink":"https://HanHoRang31.github.io/post/pkos2-4-monitoring/","tags":["KANS","kops","cloud","AWS","kubernetes","monitoring","prometheus","thanos"],"title":"[PKOS] Thanos를 통한 고가용성 모니터링(프로메테우스) 시스템 구축하기"},{"categories":null,"contents":" 로깅 (Loki \u0026amp; Promtail ) 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. Logging? 애플리케이션 실행 중 발생하는 이벤트, 작업, 오류 등의 정보를 기록하는 프로세스이다. 로깅의 주요 목적은 프로그램의 실행 상태를 추적하고, 문제 발생 시 원인을 찾기고, 내부 감사를 기록하기 위함이다. 로그 파일은 시간 순서대로 저장되며, 대부분의 경우 텍스트 파일 또는 데이터베이스에 저장된다.\n컨테이너화된 애플리케이션에 가장 쉽고 가장 널리 사용되는 로깅 방법은 표준 출력(stdout)과 표준 에러(stderr) 스트림에 작성하는 것이다. 이를 이용하면 로깅 명령어를 통해 조회가 가능하다.\n1 2 3 4 5 6 # 로그 확인 예 kubectl logs metrics-server-5f65d889cd-znqw5 -n kube-system I0328 00:14:31.072509 1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key) I0328 00:14:31.477085 1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController .. 쿠버네티스 환경에서도 컨테이너 엔진이나 런타임이 제공하는 기본적인 로깅 기능이 있으나 충분하지 않다. 컨테이너가 크래시되거나, 파드가 축출되거나, 노드가 종료된 경우에도 애플리케이션의 로그에 접근할 수 없기 때문이다.\n따라서, 쿠버네티스에서 로그는 노드, 파드 또는 컨테이너와는 독립적으로 별도의 스토리지와 라이프사이클을 가져야 한다. 이 개념을 클러스터-레벨-로깅 이라 하며 이를 위해 별도의 벡엔드 솔루션이 필요하다. 쿠버네티스에 사용할 수 있는 로깅 솔루션은 3가가지 오픈소스 프로젝트를 결합한 PLG 스택(Promtail, Loki, Grafana) 또는 ELK(Elasticsearch, Logstash, Kibana)있다. 이번 블로그 글에서는 PLG 스택을 알아볼 것이며 로깅 시스템인 Loki 와 로그 수집 에이전트인 Promtail 을 설치하하여 클러스터-레벨-로깅을 테스트해보겠다.\nLoki Loki는 Grafana Labs에서 개발한 경량 로깅 시스템으로, 쿠버네티스 환경에서 메타데이터를 기반으로 로그를 수집하고 빠르게 처리할 수 있다. 그리고 Prometheus와 호환되는 레이블 기반 질의 및 필터링 기능을 제공하며, Grafana와 통합을 통해 로깅 데이터를 대시보드에서 확인이 가능하다. 로깅 수집 에이전트인 Promtail을 사용하여 로깅을 수집하며 이를 통해 쿠버네티스 리소스(노드, 파드 또는 컨테이너)와는 독립적으로 별도의 스토리지와 라이프사이클을 가진다.\nhttps://grafana.com/blog/2018/12/12/loki-prometheus-inspired-open-source-logging-for-cloud-natives/\n메타데이터 인덱싱?\nhttps://grafana.com/oss/loki/\n로키는 메타데이터를 인덱싱을 통해 경량화 및 빠른 쿼리 성능을 가진다. 메타인덱싱 원리는 위의 그림과 같은데 로그 전체 텍스트를 저장하는 것이 아니라 타임스탬프와 라벨을 묶어 인덱스(index) 로 그 외 나머지 텍스트를 청크(chunk)로 나눠 저장된다.\n인덱스: 로그 시간과 레이블을 묶어 해싱을 통해 고유한 식별자를 만든다. 이 식별자를 통해 로그 스트림을 참조하고 검색하는데 사용된다. 인덱스는 일반적으로 NoSQL DB에 저장하는데 Key 값에는 인덱스를 values 값에는 해당 청크의 데이터를 저장한다. 청크: 청크는 실제 로그 데이터이다. 청크는 데이터를 압축 및 저장하기 위해 여러 압축 알고리즘을 사용할 수 있으며, 기본적으로 압축이 적용되어 저장 공간을 최적화합니다. 일반적으로 오브젝트 스토리지에 저장한다. 아키텍처 https://grafana.com/blog/2018/12/12/loki-prometheus-inspired-open-source-logging-for-cloud-natives/\n그림에서 화살표 빨강은 로그 Write, 파랑은 Read를 의미한다. 또한, 각 구성 컴포넌트들은 HA를 지원하여 컴포넌트 내부 구성 하나에 장애가 발생하더라도 서비스가 중단되지 않는다.\nYour Jobs : 로그 수집 에이전트로 사용자 정의에 맞게 로그를 수집하여 로키 서버(Distributor)에 전달한다. 로그 수집 에이전트로 Promtail를 사용하나 fluent, fluentbit 과 호환이 가능하다. Distributor(디스트리뷰터): Distributor는 로그 데이터를 수신하고, 해당 데이터를 인제스터(Ingester)에 분산시키는 역할을 한다. 또한, 레이블의 해시 값을 사용하여 데이터를 적절한 인제스터에 전달하며 로드밸런싱을 통해 로그 데이터를 여러 인제스터에 고르게 분산시켜준다. Ingester(인제스터): Ingester는 Distributor로부터 로그 데이터를 받아서, 로그 스트림을 청크로 나누고 압축한 후 저장시킨다. 인제스터는 메모리 또는 영구 스토리지에 청크를 저장할 수 있으며, 쿼리어(Querier)에게 저장된 청크에 대한 질의 결과를 제공한다. 청크가 일정 시간 또는 크기에 도달하면 인제스터는 이를 영구 스토리지에 저장시킨다. Querier(쿼리어): Querier는 사용자로부터 질의를 받아 처리한다. 질의를 처리할 때, 쿼리어는 인덱스를 사용하여 관련된 청크를 찾고, 인제스터 및 영구 스토리지에서 해당 청크를 가져와 질의 결과를 반환한다. loki 버전 3.0 이상부터는 loki, loki-distributed가 통합되었다.\nhttps://grafana.com/docs/loki/latest/getting-started/\nLoki Write component : 로그 데이터를 수신하고 저장시켜주는 컴포넌트이다. 앞서 아키텍처의 빨간 flow를 담당하는 Distributor와 Ingester 로 구성되어 있다. Loki Read component: 로그 데이터를 조회하고 처리하는데 사용된다. 앞서 아키텍처의 파랑 flow를 담당하는 Querier와 Query Frontend 로 구성되어 있다. gateway : Loki 구성요소에 대한 프록시 서버이다. 로그를 까보면 NGINX 게이트웨이가 설치되며 로드밸런싱 기능을 수행하여 각 컴포넌트에 트래픽을 분산시킨다. 설치 설치 환경 : kops 클러스터 (k8s 1.24), AWS Ubuntu 인스턴스\n설치는 다음과 같이 진행할 예정이다.\nLoki (helm grafana/loki 4.8.0) Promtail (helm grafana/promtail 6.9.2) 사족이지만, 로그 저장소로 mongoDB를 활용하려 했으나 안 된다! 로키 호환 저장소가 정해져있기 때문이다. 온프로미스에서 구성시 참고하자.\n본 블로그에서는 S3에 오브젝트 데이터를 DynamoDB에 인덱스 데이터를 저장시키겠다. (230402. DynamoDB 인덱스 연동문제로 S3에 인덱스, 오브젝트 데이터 저장)\nhttps://grafana.com/docs/loki/latest/operations/storage/\nLoki 설치 Loki 저장소로 S3 와 DyanmoDB 스토리지 생성과 IAM role 권한 연결이 필요하다. 본 테스트에서는 S3 이름을 han-loki , DyanmoDB 이름은 loki_ 로 생성하였다. DynamoDB 사용시 주의해야할 점은 다음과 같다.\nDynamoDB 테이블 이름을 헬름 차트의 schema_config.config DynamoDB 파티션 키 \u0026amp; 정렬 키를 (문자열, 바이너리)로 지정해야 한다. IAM 권한은 S3, DyanmoDB에 대한 정책을 부여했다.\nAmazonS3FullAccess AmazonDynamoDBFullAccess 필자는 사용자에 IAM 권한를 부여 후 access-key 와 secret-key를 가져왔다. 해당 키는 밑의 헬름 차트 버킷 연동에서 사용된다. 우선 헬름을 통해 로키 차트를 가져오겠다.\n1 2 3 4 kubectl create ns loki helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm fetch grafana/loki --untar --version 4.8.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 #values-loki.yaml schema_config: configs: - from: 2020-05-15 store: aws object_store: s3 schema: v11 index: prefix: loki period: 0 storage_config: aws: s3: s3://ap-northeast-2/han-loki dynamodb: dynamodb_url: dynamodb://ap-northeast-2 table_manager: retention_deletes_enabled: true retention_period: 336h index_tables_provisioning: write_scale: enabled: false read_scale: enabled: false chunk_tables_provisioning: write_scale: enabled: false read_scale: enabled: false table_prefix: \u0026#34;loki\u0026#34; tableManager: enabled: true monitoring: lokiCanary: enabled: true selfMonitoring: enabled: false loki: auth_enabled: false storage: bucketNames: chunks: han-loki ruler: han-loki admin: han-loki type: s3 s3: s3: han-loki endpoint: s3.ap-northeast-2.amazonaws.com region: ap-northeast-2 secretAccessKey: {SECRET KEY} accessKeyId: {ACCESS KEY} s3ForcePathStyle: false insecure: false http_config: {} access_key 와 Secret_access_key 노출에 주의하자! schema_config : 인덱스와 청크 데이터에 대한 저장 스키마를 정의한다. storage_config: 데이터 저장할 스토리지 정보를 입력한다. table_manager, tableManager : 테이블 기반 데이터 저장소에 인덱스 및 청크를 지원하는데 버전 호환 문제로 작동이 안되어 dynamodb 에 인덱스가 저장이 안된다. (현재 S3에 저장됨) monitoring.lokicanary: 시스템 검증에 사용된다. true 설정시 별도의 카나리 파드가 생성되어 일정시간마다 테스트 로그를 전달한다. monitoring.selfMonitoring : 대시보드에 Loki 관련 대시보드가 업로드된다하지만, loki: 로키 서버 설정을 정의한다. 최신 버전에는 스토리지 연동을 여기서 하는데 dynamodb에 대한 설정 예시가 없고 테스트가 안되서 s3 만 정의하였다. 1 helm install loki grafana/loki -f values-loki.yaml -n loki --version 4.8.0 파드 배포는 완료되었으나, 로키 스토리지 연동으로 안정화 작업이 필요하다.\n안정화 작업는 다음과 같다.\nstorage_config 스토리지 연동 문제\n1 2 level=error ts=2023-04-01T03:18:30.427197283Z caller=flush.go:144 org_id=self-monitoring msg=\u0026#34;failed to flush\u0026#34; err=\u0026#34;failed to flush chunks: store put chunk: NoCredentialProviders: no valid providers in chain. Deprecated.\\n\\tFor verbose messaging see aws.Config.CredentialsChainVerboseErrors, num_chunks: 1, labels: {app_kubernetes_io_component=\\\u0026#34;read\\\u0026#34;, app_kubernetes_io_instance=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_name=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_part_of=\\\u0026#34;memberlist\\\u0026#34;, cluster=\\\u0026#34;loki\\\u0026#34;, container=\\\u0026#34;loki\\\u0026#34;, controller_revision_hash=\\\u0026#34;loki-read-7749df4969\\\u0026#34;, filename=\\\u0026#34;/var/log/pods/loki_loki-read-2_a8fab5a2-e78b-4cff-9f8d-ba0ee5952a3c/loki/0.log\\\u0026#34;, job=\\\u0026#34;loki/loki-read\\\u0026#34;, namespace=\\\u0026#34;loki\\\u0026#34;, pod=\\\u0026#34;loki-read-2\\\u0026#34;, statefulset_kubernetes_io_pod_name=\\\u0026#34;loki-read-2\\\u0026#34;, stream=\\\u0026#34;stderr\\\u0026#34;}\u0026#34; level=info ts=2023-04-01T03:18:30.427244754Z caller=flush.go:168 msg=\u0026#34;flushing stream\u0026#34; user=self-monitoring fp=33d14d11bfd98f55 immediate=false num_chunks=1 labels=\u0026#34;{app_kubernetes_io_component=\\\u0026#34;read\\\u0026#34;, app_kubernetes_io_instance=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_name=\\\u0026#34;loki\\\u0026#34;, app_kubernetes_io_part_of=\\\u0026#34;memberlist\\\u0026#34;, cluster=\\\u0026#34;loki\\\u0026#34;, container=\\\u0026#34;loki\\\u0026#34;, controller_revision_hash=\\\u0026#34;loki-read-7749df4969\\\u0026#34;, filename=\\\u0026#34;/var/log/pods/loki_loki-read-2_a8fab5a2-e78b-4cff-9f8d-ba0ee5952a3c/loki/0.log\\\u0026#34;, job=\\\u0026#34;loki/loki-read\\\u0026#34;, namespace=\\\u0026#34;loki\\\u0026#34;, pod=\\\u0026#34;loki-read-2\\\u0026#34;, statefulset_kubernetes_io_pod_name=\\\u0026#34;loki-read-2\\\u0026#34;, stream=\\\u0026#34;stderr\\\u0026#34;}\u0026#34; 공식문서 예제 storage_config 가 최신 기준으로 업데이트된 것 같지 않다. 필자의 경우 배포 yaml로 구성을 설정하니 정상적으로 작동했다.\nS3 디렉토리에 chunk 폴더가 없다? 이슈 에 따르면 멀티테넌트 구성에서 loki를 실행할때 인증이 비활성화하면 기본적으로 fake 폴더에 저장된다고 한다. fake폴더 안에는 정상적으로 chunk가 들어가있는 것을 확인할 수 있으나 다중 클러스터에서 배포시 loki 별로 인증이 필요할 것 같다.\ndynamoDB에 인덱스를 저장하고 싶어요.\n해결해야할 문제다. 현재 S3에 index값이 들어가는데 dynamodb에 옮겨야 한다. 헬름차트에 table-manager 설정이 두개(table-manager, tableManager)여서 설정이 안 먹힌다. 추가 원인으로는 깃허브 이슈( https://github.com/grafana/loki/issues/5070) 설정값( extraArgs)인데 적용이 안된다. 추후 해결시 업데이트하겠다.\n배포를 완료하면 로키 파드가 정상적으로 작동하는 것을 알 수 있다.\nhttp://loki-read-headless.loki.svc.cluster.local:3100\nPromtail 설치 1 2 3 helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm fetch grafana/promtail --untar --version 6.9.2 Promtail 차트에서 따로 설정할 것은 없지만, 로키 연결과 로그 라인 파이프라인 구성 확인을 위해 집어넣는다.\nconfig.clients.url : 로그 수집 후 전달한 로그 서버를 입력한다. 보통 로키 서버 설치시 설정되는 url로 지정된다. config.snippets: 로그 라인 분석과 추출, 필터링하는 스테이지들의 작업을 정의한다. 원하는 로그 데이터 형식을 정의해서 입력하면 된다. scraping 구문은 프로메테우스와 동일하다. 1 helm install promtail grafana/promtail -n loki --version 6.9.2 로그 수집 테스트 Promtail-Loki-Grafana 까지의 로그 수집을 테스트하겠다. 테스트를 위해 nginx 를 배포하고 파드 로그 확인과 그라파나(로키)에서 로그를 확인한다.\n1 2 helm repo add bitnami https://charts.bitnami.com/bitnami helm install nginx bitnami/nginx --version 13.2.23 -f nginx-values.yaml 파드 로그는 파드가 올라간 노드 /var/log/pods 에 저장되어 있다. 파드의 로그를 확인하고 그라파나 대시보드에서 로키가 해당 로그를 긁어오는지 확인하자.\n[Explorer] → Job = default/nginx 설정 후 로그 확인 잘 들어온다! 이어서 파드 라이프사이클과 독립적으로 로그가 관리되는 지 확인하겠다. nginx 파드를 삭제하고 파드 로그와 로키를 확인하겠다.\n1 helm uninstall nginx 아래 디렉토리를 확인하면 nginx 파드가 삭제되어 로그 디렉토리가 삭제된 것을 확인할 수 있다.\n파드가 삭제되었지만 그라파나(로키) 에서 nginx 로그를 확인할 수 있다.\n마치며 그라파나 공식 문서를 참고하니 엔터프라이즈에 대한 지원만 활발한 느낌이다. 오픈소스로 설치시 연동 부분과 최신 버전 호환 문제로 테스트하는 데도 오랜 시간이 걸렸다. 특히 인덱스 데이터를 dynamodb 에 연동해야 했지만 설정 문제로 실패했고 S3에 인덱스, 청크 데이터를 저장시켰다. 이 부분은 공식 문서를 최신 버전으로 업데이트를 하거나 예가 나오면 업데이트하겠다.\n","date":"Apr 01","permalink":"https://HanHoRang31.github.io/post/pkos2-4-logging/","tags":["KANS","kops","cloud","AWS","kubernetes","monitoring","PLG","loki","grafana","promtail"],"title":"[PKOS] 로깅 PLG 스택, 최신 버전(Loki v2.8.0) 배포하기"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 3주차 시간에는 Gitlab 과 ArgoCD를 배포하여 Gitops 시스템을 구축하였다. 이번 블로그 글에서는 GitOPS 시스템에 대한 실습 내용들을 정리하고 공유하고자 한다.\nGitOps는 GIt을 진실의 원천(SSOT, Single Source of Truth) 으로 사용하는 인프라와 애플리케이션 배포 관리 방식이다. 진실의 원천이라는 말은 Git에서만 소스를 관리할 수 있게 하여 단일 진실 원천을 구현한다는 말이다. 쿠버네티스에서는 GItOps를 ArgoCD를 이용하여 깃 저장소에 있는 소스를 정의된 클러스터 환경에 자동으로 반영시켜 준다.\nGitOps 시스템을 구축하면 얻을 수 있는 이점은 다음과 같다.\n버전 관리: Git을 사용하므로 인프라 및 애플리케이션의 모든 변경 사항이 추적되고 버전이 관리된다. 이를 통해 문제 발생 시 이전 상태로 쉽게 되돌릴 수 있다. 디커플링: 코드와 인프라를 분리함으로써 개발자와 운영팀 간의 협업이 쉬워진다. 자동화: 변경 사항이 자동으로 적용되므로 수동 인프라 관리 작업이 줄어들고, 실수를 방지할 수 있다. 보안: Git 저장소에 접근 권한을 제어함으로써 인프라 변경에 대한 보안을 강화할 수 있다. 신속한 피드백 루프: 문제가 발생하면 소스 코드에 대한 변경을 통해 빠르게 해결하고 적용할 수 있다. 이점만 존재하는 것은 아니다, 단점도 존재한다.\n학습 곡선: GitOps 및 관련 도구를 사용하려면 Git, 선언적 인프라 도구 및 오케스트레이션 플랫폼에 대한 지식이 필요하다. 복잡성: GitOps를 사용하면 초기 설정과 관리가 복잡할 수 있다. 적절한 도구와 프로세스를 구축하고 유지 관리하는 데 시간과 노력이 필요할 수 있다. 높은 의존성: GitOps는 Git 저장소에 대한 높은 의존성을 가지며, 저장소 접근이 불가능한 경우 인프라 변경이 제한될 수 있다. 간단하게 예제 시나리오를 구성하여 GitOps 시스템을 구축하고자 한다. 먼저 쿠버네티스 환경에 Gitlab과 ArgoCD를 배포할 것이고, 관리 대상을 지난 시간에 배운 Harbor 배포 차트로 지정할 것이다.\nGitlab 배포 Gitlab은 소스 원격 저장소이다. 흔히 쓰는 깃허브로 생각하면 이해가 빠르다. Gitlab의 차별점은 사설(공식문서에는 offline이라 칭함) 깃랩을 구축할 수 있다는 점인데 직접 헬름 차트를 구성하여 배포하겠다.\n깃랩 차트는 다음과 같이 불러올 수 있다.\n1 2 3 helm repo add gitlab https://charts.gitlab.io/ helm repo update helm fetch gitlab/gitlab --untar --version 6.8.1 깃랩 차트에 부가 옵션이 많다.. 공식 문서를 참고하자. 다음은 필자가 차트를 보고 간략히 정리한 내용이다.\n유료 버전 무료 제공 : 쿠버네티스에서 오프라인 깃랩 설치시 enterprise edtion (유료, 이하 EE라 칭함) 을 무료로 사용할 수 있다. 이유를 찾아보니 고객 유치 전략이라 한다. EE 사용시 고급 보안, 인증, 권한 관리, 진행률 보고, 고급 CI/CD 기능, 멀티 프로젝트 파이프라인 등의 고급 기능을 제공한다. 스토리지 관련 가용성 제공 : Gitlab 자체적으로 도커 레지스트리를 제공할 수 있다. 또한,레지스트리 이미지 및 깃랩 페이지, 등의 데이터등에 대한 저장소 스토리지로 Minio 를 사용한다. 앞선 블로그 글에서 harbor 레지스트리에 대한 고가용성 구축을 다루었는데, 깃랩에서는 자동으로 연동해주는 것 같다. 네트워크 최적화 기능 제공 : gitlab 서버(Gitlay)에 대한 로드밸런싱(Prafect) 기능을 제공하며, Workhorse라는 컴포넌트를 통해 중앙 프록시 및 파일 처리를 관리한다. 보안 : Oauth, 인증 및 권한 관리(gitlab shell) 을 제공한다. Observability 제공 : 그라파나 연동 기능, Tracing 기능을 제공한다. CI / CD 제공 : gitlab-runner 라는 컴포넌트를 통해 CI / CD 기능을 제공한다. 뭐지..? 단순히 깃 저장소를 확장하여 대부분의 addon를 자동으로 연계시켜 제공하다니, 심지어 고가용성, 최적화에 대한 구성도 자동으로 제공해준다. 기능별로 세세히 보고 싶은 마음이 굴뚝같지만, 이번 블로그글에서는 gitops 시스템 대한 내용만 다룬다. 차트에서 다음과 같은 부분을 수정하였다.\nvalues-gitops.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 global: hosts: domain: {도메인 입력} ingress: configureCertmanager: false provider: aws class: alb annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: {ACM arn 입력} alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/group.name: \u0026#34;gitlab\u0026#34; tls: enabled: false certmanager: install: false nginx-ingress: enabled: false prometheus: install: false gitlab-runner: install: false 헬름 차트에서는 차트 오버라이드가 가능하다. 위의 차트에서 ACM값만 수정해서 설치를 진행하자.\n1 helm install gitlab gitlab/gitlab -f values-gitops.yaml --namespace gitlab --version 6.8.4 설치 후 도메인을 통해 로그인을 진행한다. 초기 admin 계정의 아이디는 root 이며, 비밀번호는 다음의 명령어에서 확인한다.\n1 2 # 웹 root 계정 암호 확인 kubectl get secrets -n gitlab gitlab-gitlab-initial-root-password --template={{.data.password}} | base64 -d ;echo 접속하면 깃랩 프로젝트 화면이 보인다. 이어서 PKOS 스터디에서 진행한 실습 내용을 테스트하겠다. 사용자 계정을 생성하여 토큰을 발급받고, 신규 프로젝트를 파일을 업로드해보자.\nArgoCD 배포 ArgoCD는 Git 리포지토리에 저장된 쿠버네티스 매니페스트와 실제 클러스터 상태를 동기화시켜주는 지속적인 배포(Continuous Delivery, CD) 툴이다. Argo CD를 사용하면 Git 리포지토리를 기반으로 인프라 구성을 코드로 관리할 수 있게 된다. 이는 단일 진실 원천(SSOT)로 자동화, 보안, 버전 관리 측면에서 유용하다.\nArgoCD 배포는 Helm 차트로 진행한다.\n1 2 3 helm repo add argo https://argoproj.github.io/argo-helm helm repo update helm fetch argo/argo-cd --untar --version 5.19.14 ArgoCD 차트 분량도 상당하다. 아키텍처를 보니 이해가 빠른 것 같아서 먼저 공유한다.\nhttps://blog.searce.com/argocd-gitops-continuous-delivery-approach-on-google-kubernetes-engine-2a6b3f6813c0\nargo-cd-server: Argo CD API 서버와 웹 UI를 제공하는 컴포넌트이다. 사용자 인증 및 권한 관리를 처리하며, 클러스터와 Git 리포지토리 간의 동기화를 관리한다. argo-cd-repo-server: Git 리포지토리와 통신하여 사용자가 관리하는 쿠버네티스 매니페스트 파일을 가져오는 컴포넌트이다. 또한, 리포지토리 내의 Helm 차트와 Kustomize 구성을 처리한다. argo-cd-application-controller: Argo CD의 핵심 컴포넌트로, 쿠버네티스 클러스터 상태와 Git 리포지토리 상태를 비교하고 동기화를 수행한다. 클러스터의 실제 상태와 원하는 상태를 일치시키는 작업을 담당한다. argo-cd-dex-server: 인증 프록시 역할을 하는 컴포넌트로, 다양한 OAuth 및 OIDC 프로바이더와 통합하여 Argo CD 인증을 처리한다. argo-cd-redis: 캐싱 및 세션 관리를 위한 Redis 데이터베이스이다. Argo CD는 Redis를 사용하여 성능 향상과 빠른 응답 시간을 제공한다. Kustomize ?\n쿠버네티스 리소스 구성을 커스터마이징을 위한 도구이다. 동일한 구성을 가진 매니패스트에 수정할 부분만 추가해서 오버라이드가 가능하다.\n아래 예는 원본 베이스 앱(my-app) 에 dev, ops 환경별 필요 값을 오버라이드하는 예제이다.\n먼저 베이스가 되는 deployment를 선언하고 Kustomize 구성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # base/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-app image: my-app-image:latest ports: - containerPort: 80 1 2 3 4 5 # base/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml 이제 dev, Ops 환경에 대한 kustomization.yaml 파일을 생성하고 다음 내용을 추가하자.\nDev 환경 설정\n1 2 3 4 5 6 7 # overlays/dev/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - ../../base patchesStrategicMerge: - deployment-patch.yaml 1 2 3 4 5 6 7 8 9 10 11 12 # verlays/dev/deployment-patch.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 2 template: spec: containers: - name: my-app image: my-app-image:dev Ops 환경 설정\n1 2 3 4 5 6 7 # overlays/ops/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - ../../base patchesStrategicMerge: - deployment-patch.yaml 1 2 3 4 5 6 7 8 9 10 11 12 # overlays/ops/kustomization.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 4 template: spec: containers: - name: my-app image: my-app-image:ops 눈치 챘는가? Ops와 dev의 replicas 개수와 이미지 설정만 달랐고 그 부분만 추가했다. 배포는 다음 식으로 진행한다.\n1 kubectl kustomize overlays/dev | kubectl apply -f - 위의 컴포넌트별 차트에서 설정이 가능하다. 추가 설정은 아래 config.param 에서 오버라이드하는 것을 추천한다. 사용 예는 깃허브에 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 # Argo CD configuration parameters ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/argocd-cmd-params-cm.yaml params: ## Controller Properties # -- Number of application status processors controller.status.processors: 20 # -- Number of application operation processors controller.operation.processors: 10 # -- Specifies timeout between application self heal attempts controller.self.heal.timeout.seconds: 5 # -- Repo server RPC call timeout seconds. controller.repo.server.timeout.seconds: 60 깃허브의 사용 예에는 중요한 reSyncPreiod 설정(저장소 동기화 시간) 이 없는 것 같다. 이럴 때는 직접 차트를 확인하여 구성 값을 확인하고 config.params에 추가하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # argo-cd/templates/argocd-application-controller - command: - argocd-application-controller - --metrics-port={{ .Values.controller.containerPorts.metrics }} {{- if .Values.controller.metrics.applicationLabels.enabled }} {{- range .Values.controller.metrics.applicationLabels.labels }} - --metrics-application-labels - {{ . }} {{- end }} {{- end }} {{- with .Values.controller.args.statusProcessors }} - --status-processors - {{ . | quote }} {{- end }} {{- with .Values.controller.args.operationProcessors }} - --operation-processors - {{ . | quote }} {{- end }} {{- with .Values.controller.args.appResyncPeriod }} # config.params 추가 - --app-resync - {{ . | quote }} {{- end }} {{- with .Values.controller.args.appHardResyncPeriod }} - --app-hard-resync - {{ . | quote }} {{- end }} {{- with .Values.controller.args.selfHealTimeout }} - --self-heal-timeout-seconds - {{ . | quote }} {{- end }} 실제 배포는 CLB에 externalDNS 로 진행하였다.\n1 2 3 4 5 6 # values-argocd.yaml server: service: type: LoadBalancer annotations: external-dns.alpha.kubernetes.io/hostname: argocd.\u0026lt;도메인 입력\u0026gt; 배포\n1 2 kubectl create ns argocd helm install argocd argo/argo-cd -f values-argocd.yaml --namespace argocd --version 5.19.14 실제로 배포할 시 고려할 점은 접네트워크 대역이다. ArgoCD는 클러스터를 직접적으로 관리할 수 있기 때문이다. 실제 Devops 팀이나 Admin 사용자가 사용할 것이라 예상한다. 이를 위해 허용된 네트워크 대역에서만 로드밸런서 접근이 가능하도록 설정하는 것이 중요할 것 같다. 로드밸런스 설정 후 적절한 보안 그룹을 생성하여 접근을 제어하도록 하자.\n초기 admin 로그인 정보는 다음의 명령어로 확인이 가능하다.\n1 2 3 #비밀번호 ARGOPW=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d) echo $ARGOPW 로그인 이후 ArgoCD에 클러스터와 깃 저장소 등록이 필요하다. ArgoCD UI 나 CLI 를 통해 확인 및 등록이 가능하다. 클러스터는 구축한 클러스터 정보가 기본으로 등록되어 있다. 향후 생산성을 위해 CLI를 통해 확인해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 최신버전 설치 curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 install -m 555 argocd-linux-amd64 /usr/local/bin/argocd chmod +x /usr/local/bin/argocd # 버전 확인 argocd version --short # argocd 서버 로그인 argocd login argocd.$KOPS_CLUSTER_NAME --username admin --password $ARGOPW WARNING: server certificate had error: x509: certificate is valid for localhost, argocd-server, argocd-server.argocd, argocd-server.argocd.svc, argocd-server.argocd.svc.cluster.local, not argocd.hanhorang.link. Proceed insecurely (y/n)? y \u0026#39;admin:login\u0026#39; logged in successfully Context \u0026#39;argocd.hanhorang.link\u0026#39; updated # argocd repo 등록 argocd repo add https://gitlab.hanhorang.link/Horang/test-stg.git --username horang --password PASSWORDa! Repository \u0026#39;https://gitlab.hanhorang.link/Horang/test-stg.git\u0026#39; added # argocd 확인 argocd repo list TYPE NAME REPO INSECURE OCI LFS CREDS STATUS MESSAGE PROJECT git https://gitlab.hanhorang.link/Horang/test-stg.git false false false true Successful argocd cluster list SERVER NAME VERSION STATUS MESSAGE PROJECT https://kubernetes.default.svc in-cluster Unknown Cluster has no applications and is not being monitored. GitOps 구축 구축한 Gilab, ArgoCD 를 통해서 GitOps 시스템을 구축해보겠다. gitops 정의대로 깃 저장소에 있는 헬름 차트가 쿠버네티스 환경에 실시간으로 동기화되는 지 테스트해보겠다. 사용 헬름 차트는 앞서 스토리지 테스트를 위해 구축한 minIO 를 대상으로 진행하겠다. 해당 차트는 필자의 깃허브에서 확인이 가능하다.\n1 2 3 helm repo add minio https://charts.bitnami.com/bitnami helm repo update helm fetch minio/minio --untar --version 12.2.1 먼저, gitlab 저장소에 헬름 차트를 PUSH한다.\n다음은 ArgoCD를 통해 동기화를 진행한다. 동기화 구성은 ArgoCD CRD로 작성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: minio-helm namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: minio server: https://kubernetes.default.svc project: default source: repoURL: https://gitlab.hanhorang.link/Horang/test-stg path: minio/ targetRevision: HEAD helm: valueFiles: - values-minio.yaml syncPolicy: syncOptions: - CreateNamespace=true spec.destination : 동기화 클러스터 대상을 나타낸다 spec.source : 깃허브 저장소를 입력한다. spec. syncpolicy : 동기화 정책을 구성한다. 옵션은 공식 문서를 참고바란다. CreateNamespace=true 옵션은 대상 네임스페이스가 없으면 생성시킨다는 옵션이다. 배포 후 ArgoCD UI에서 확인하자.\n1 kubectl apply -f minio-helm-argo.yaml 배포시 OutofSync 의 상태가 되는데 상단의 Sync App눌러 동기화를 진행한다.\n동기화시 옵션에 따라 세부 동작이 가능하다.\nPRUNE: 리포지토리에 없는 리소스를 삭제함 DRY RUN : 테스트로 실제 변경하지 않음 APPLY ONLY: 리소스 생성 및 수정만 수행하고 삭제하지 않음 FORCE : 강제 적용\nSKIP SCHEMA VALIDATION: 리소스 매니페스트의 JSON 스키마 검증을 건너뛰는 옵션이다. 이 옵션은 매니페스트에 포함된 스키마가 유효하지 않거나 검증되지 않아도 배포를 진행하고자 할 때 사용한다. AUTO-CREATE NAMESPACE: ArgoCD가 리소스를 배포할 네임스페이스가 존재하지 않는 경우 자동으로 해당 네임스페이스를 생성하는 옵션이다. PRUNE LAST: 클러스터에 존재하지 않아야 하는 리소스를 자동으로 제거하여 깔끔한 상태를 유지할 수 있도록 돕는다. APPLY OUT OF SYNC ONLY: ArgoCD가 오직 동기화되지 않은 리소스에 대해서만 kubectl apply 명령을 실행하는 옵션이다. 이렇게 하면 이미 동기화된 리소스는 건드리지 않고, 변경된 리소스에 대해서만 업데이트를 진행한다.\nRESPECT IGNORE DIFFERENCES: ArgoCD가 리소스를 비교할 때, 무시해야 하는 차이점을 존중하도록 설정하는 옵션이다. 이렇게 하면 사용자가 지정한 특정 필드의 변경 사항을 무시하고 동기화 여부를 결정할 수 있다.\nSERVER-SIDE APPLY: ArgoCD가 서버 측에서 리소스를 적용하도록 설정하는 옵션이다. 이 옵션을 사용하면, 리소스의 변경 사항이 서버 측에서 자동으로 병합되어 관리자가 수동으로 병합할 필요가 없다. 이 방식은 클라이언트 측에서 **kubectl apply**를 사용하는 것보다 더 효율적인 리소스 관리를 가능하게 한다.\nREPLACE : 리소스 변경시 기존 리소스를 삭제하고 새로운 리소스를 생성하여 대체한다.\nRETRY : 동기화 실패시 재시도\nSync 후 배포까지 모니터링 후 정상적으로 작동하는 것을 확인할 수 있다.\nArgoCD는 기본적으로 수동적으로 Sync 작업이 필요하다. 자동으로 깃 저장소에 내용으로만 동기화시키려면 self-healing 옵션이 필요하다. App Detail의 Policy 설정에서 활성화하자.\n마치며 이번 글에서는 Gitlab 와 ArgoCD 차트를 분석하여 구성하였고 GitOps 시스템을 구축해보았다. 인프라 관리자 측면에서 SSOT를 구성하면 코드 구성 관리 측면에서 편리해질 것이 느껴진다. 그리고 Gitlab와 ArgoCD 메뉴얼이 잘 정리되어 있다. 확장 기능(메트릭, 알람, 보안) 필요시 메뉴얼을 참고하자!\n","date":"Mar 25","permalink":"https://HanHoRang31.github.io/post/pkos2-3-gitops/","tags":["KANS","kops","cloud","AWS","kubernetes","GitOps","Gitlab","ArgoCD","CI/CD"],"title":"[PKOS] GitOps와 ArgoCD DeepDive"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 2주차 스터디에서는 쿠버네티스의 네트워크와 스토리지를 중점적으로 공부하였다. 분량이 많아 네트워크와 스토리지를 나눠서 블로그를 작성할 예정이다. 이번 블로그 글에서는 로컬 스토리지에 대해 공유하겠다. 일반적으로 로컬 스토리지는 IOPS 성능이 특화되어 있지만 노드에 종속되어 있어 고가용성이나 스토리지 기능에 제한이 있다. 이러한 제한을 없애기 위한 과정으로 로컬 스토리지의 Hostpath, local 볼륨을 마운트하여 테스트를 진행할 것이고, 마지막으로는 로컬 볼륨에서 고가용성과 스토리지 기능(백업)을 가진 Mysql 데이터베이스를 구성하겠다. 추가로 스토리지 성능 측정과 모니터링 과정, QnA를 준비하였다. 본론으로 들어가서, 쿠버네티스에서 스토리지를 사용하는 이유는 무엇일까? 스토리지가 데이터를 저장하는 용도인 것처럼 데이터 저장을 위해서이다. 예를 들어, 데이터베이스같은 애플리케이션을 파드로 운영한다고 가정해보자, 파드 라이프사이클과 별개로 데이터가 보존되어야 한다. 이를 위해 쿠버네티스에서는 PV(Persistent Volume)과 PVC(Persistent Volume Claim) 리소스를 제공한다. 또한, 데이터 관리 방법(데이터 저장 위치, 데이터 공유, 확장성)에 따라 여러 스토리지 볼륨과 기능을 제공한다. 이처럼 데이터를 보존해야 하는 애플리케이션을 상태있는(Stateful) 애플리케이션이라 칭하며 스토리지를 통해 클러스터 내의 컨테이너에 안정적이고 지속적인 데이터를 제공할 수 있다.\n로컬 스토리지 로컬 스토리지는 말 그대로 파드의 스토리지로 서버 내부 볼륨의 스토리지를 사용하는 것이다. AWS EC2는 내부 볼륨인 인스토어 스토어를 사용한다. 로컬 스토리지 구현은 쿠버네티스에서 HostPath, Local 볼륨 마운트로 나뉘어 사용이 가능하다. HostPath 볼륨과 Local 볼륨의 차이는 쿠버네티스 볼륨 리소스(PV) 사용 유무에 따라 구분한다.\n일반적으로 내부 볼륨의 스토리지를 사용하는 만큼, 다른 원격 스토리지와 비교했을때 IOPS 성능이 뛰어나다. cncf 공식사이트에서 IOPS 기준 약 2~3배의 차이가 난다고 하니 성능 필요의 애플리케이션에서는 도입을 고려할 만하다. 그리고 로컬스토리지는 EC2 에 종속되어 있어 고가용성 구성과 백업같은 기능 사용에 추가 구성이 필요하다. 필자는 이를 해결하기 위해 볼륨프로비저닝 플러그인인 Local-path-provisioner 와 백업 솔루션인 Velero를 사용하였다.\n먼저 Local-path-provisioner 플러그인 설치 과정을 살펴볼 것이고, 로컬스토리지 이해를 위해 3가지의 케이스로 나뉘어 테스트를 진행하겠다.\nLocal-path-provisioner PV를 통한 고가용성 테스트 Hospath PV를 통한 고가용성 테스트 파드간 데이터 동기화 구성 local-path-provisioner 볼륨 프로비저닝 플러그인 중 하나로, 로컬 노드의 파일 시스템 경로를 사용하여 PVC(Persistent Volume Claim)를 만들어주는 역할을 한다. PVC를 통해 볼륨을 요청하면 PV가 자동으로 생성되어 연결된다고 보면 된다.\n설치시, 로컬 노드에 대한 볼륨 설정이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 curl -s -O https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml vim local-path-storage.yaml ---------------------------- apiVersion: apps/v1 kind: Deployment metadata: name: local-path-provisioner namespace: local-path-storage spec: replicas: 1 selector: matchLabels: app: local-path-provisioner template: metadata: labels: app: local-path-provisioner spec: # 추가 부분 nodeSelector: kubernetes.io/hostname: \u0026#34;마스터 노드 이름 입력 \u0026#34; tolerations: - effect: NoSchedule key: node-role.kubernetes.io/control-plane operator: Exists ... kind: ConfigMap apiVersion: v1 metadata: name: local-path-config namespace: local-path-storage data: config.json: |- { \u0026#34;nodePathMap\u0026#34;:[ { \u0026#34;node\u0026#34;:\u0026#34;DEFAULT_PATH_FOR_NON_LISTED_NODES\u0026#34;, \u0026#34;paths\u0026#34;:[\u0026#34;/data/local-path\u0026#34;] # 추가 부분 } ] } ---------------------------- Deployment / spec.spec 에서 nodeselector 와 tolerations 로 볼륨 배치 노드 설정(마스터 노드로 파드 배치) Configmap / data config.json에서 로컬 볼륨 PATH 설정 설치 확인\n1 2 3 4 kubectl get sc local-path ---------------------------- NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path rancher.io/local-path Delete WaitForFirstConsumer false 29m Case 1. Local-path-provisioner PV를 통한 고가용성 테스트 HostPath 볼륨이 IOPS 성능이 좋은 것을 앞서 확인하였다. 성능적으로 사용하기 좋은 볼륨이라 할 수 있으나 고가용성 구성이 필요하다.\nHostpath 볼륨의 문제점으로 노드간 마이그레이션에 자유롭지 못하기 때문이다. 이해를 위해 직접 예제 파드를 배포해보고 고가용성을 테스트 해보겠다.\nLocal-path-provisioner PV를 통한 고가용성 테스트\n앞서 배포한 Local-path-provisioner 를 통해 PV를 생성하여 파드를 배포하고 고가용성 구성을 테스트해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # 파드 예제 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: localpath-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi storageClassName: \u0026#34;local-path\u0026#34; --- apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: pod-persistent-volume mountPath: /data volumes: - name: pod-persistent-volume persistentVolumeClaim: claimName: localpath-claim 파드 배포 후 노드 드레인을 진행해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 배포 파드의 노드 확인 PODNODE=$(kubectl get pod -l app=date -o jsonpath={.items[0].spec.nodeName}) echo $PODNODE # 노드 드레인과 파드 모니터링 kubectl drain $PODNODE --force --ignore-daemonsets --delete-emptydir-data \u0026amp;\u0026amp; kubectl get pod -w --------------------------------- node/i-0c41dc0f6eeb01730 cordoned Warning: ignoring DaemonSet-managed Pods: kube-system/aws-node-w8bxm, kube-system/ebs-csi-node-thndq, kube-system/node-local-dns-v2hhc evicting pod default/date-pod-d95d6b8f-q9skb evicting pod kube-system/metrics-server-5f65d889cd-9btc7 pod/metrics-server-5f65d889cd-9btc7 evicted pod/date-pod-d95d6b8f-q9skb evicted node/i-0c41dc0f6eeb01730 drained NAME READY STATUS RESTARTS AGE date-pod-d95d6b8f-x5vrb 0/1 Pending 0 2m 노드 드레인시, 상태가 Pending 인 것을 확인할 수 있다. 이는 다른 노드에 PV볼륨이 없기 때문이다.\n마찬가지로 파드를 5개로 추가해도 볼륨이 있는 노드에만 파드가 올라온 것을 확인할 수 있다.\n1 2 # 예제 파드 개수 5개로 증가 kubectl scale deployment date-pod --replicas=5 Case2. HostPath를 통한 고가용성 테스트 볼륨 Hostpath로 노드 PATH를 직접 지정하여 고가용성을 테스트해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: log mountPath: /data volumes: - name: log hostPath: path: \u0026#34;/data\u0026#34; type: DirectoryOrCreate 파드 드레인과 개수 조절시 노드에 상관없이 배포가 진행된다.\n하지만, 볼륨별로 데이터가 쌓이는 것이 다르다. 각 노드에 들어가서 로그를 확인해보면 찍히는 것이 다른 것을 확인할 수 있다.\n고가용성이라 할 수 있지만 적재되어 있는 데이터가 달라 stateful 애플리케이션(ex. MySQL)을 운영하기엔 한계가 있다.\n참고 책에서는 이러한 제약사항을 해결하기 위해서는 애플리케이션 단에서 다른 노드의 파드와 데이터를 동기화해서 해결할 수 있다고 한다.\n동기화 방법을 찾아보니 NFS 볼륨(ex. AWS EFS)을 구성하여 HostPath 를 연결하거나, 볼륨간 rsync를 사용하라 나오지만, 성능(로컬SSD가 아님)이 떨어져 해결 방법은 아닌 것 같다.\nCase3. 파드간 데이터 동기화 구성 그렇다면 성능 좋고, 고가용성도 보장되고, 데이터 동기화를 보장할 수 있는 Stateful 애플리케이션을 구성할 수 있을까?\n쿠버네티스 공식문서 예제에서 이를 확인할 수 있는데 해당 예제를 구성해보고 테스트해보겠다. 해당 예제에서는 데이터베이스 리소스로 StatefulSet를 사용한다.\nStatefulSet 리소스는 이름처럼 Stateful한 애플리케이션을 위해 만든 리소스이다. StatefulSet 리소스의 특징은 다음과 같다.\nStatefulSet 리소스의 특징\nPod 이름\nStatefulSet에 의해 생성된 파드들은 {Pod 이름}-{순번} 식으로 이름이 정해진다. 이는 클러스터 내부 환경에서 데이터베이스에 접근할 때 사용하기 위해서이다.\n파드 순차적 배포\nPod 생성시 모든 Pod 가 동시에 생성되지 않고 순서대로 하나씩 생성된다. 이는 데이터베이스에서 마스터 파드 → 슬레이브 파드로 기동해야 하는 조건등에서 유용하게 사용 될 수 있다.\n파드별 볼륨 마운트\n일반적으로 PVC \u0026amp; PV에 중복적으로 Pod를 사용할 수 없다. 연결된 Pod가 존재하면 그 다음 파드들은 PVC를 얻지 못해 볼륨을 사용하지 못한다. 반면, Statefulset에서 PVC를 템플릿 형태로 정의하여 Pod마다 PVC, PV를 생성하여 파드별로 볼륨을 마운트할 수 있게 된다.\n다시 돌아가서 쿠버네티스 공식문서의 예제는 클러스터에 MySQL 스테이트풀셋이 배포되고 각 레플리카에 순서대로 배포되는 예제이다. 중요하게 볼 점은 스테이트 풀셋의 매니페스트이다.\n해당 StatefulSet 매니페스트는 3개의 replica를 가진 MySQL을 생성한다. init 컨테이너는 두개 배포되며, init-container는 MySQL init 설정을 수행하고, xtrabackup init-container는 MySQL 클러스터 복제를 위해 데이터를 클론하여 동기화를 진행한다. init 컨테이너 이후 MySQL 컨테이너는 데이터베이스 작업을 수행하며, xtrabackup 컨테이너는 클론 작업을 진행한다.\n공식 문서의 예제를 그대로 배포하면 스토리지 클래스가 default로 EBS 볼륨(외부)에 연결된다. 이대로 진행하면 local-path-provisioner에서 배포한 로컬 볼륨에서 마운트되지 않는다.\n로컬 볼륨에 마운트하기 위해서는 추가 작업이 필요하다.\nlocal-path-provisioner 배포 파일 수정\nlocal-path-provisioner 파드를 워크 노드에만 배포하도록 설정한다. 해당 설정을 통해 워크 노드에만 로컬 볼륨을 생성하고 파드에 연결할 수 있도록 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim local-path-storage.yaml ---------------------------- apiVersion: apps/v1 kind: Deployment metadata: name: local-path-provisioner namespace: local-path-storage spec: replicas: 1 selector: matchLabels: app: local-path-provisioner template: metadata: labels: app: local-path-provisioner spec: spec: # 아래 부분 추가 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/node operator: Exists 스테이트 풀셋의 매니페스트 내 스토리지클래스 지정 1 2 3 4 5 6 7 8 9 volumeClaimTemplates: - metadata: name: data spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: local-path # 로컬 볼륨 추가 resources: requests: storage: 10Gi 스테이트 풀셋의 매니페스트 replica count 를 워크노드(2) 개수 만큼 수정 1 2 3 serviceName: mysql replicas: 2 # 수정 template: 배포 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl apply -f ./ ---------------------------- configmap/mysql created service/mysql created service/mysql-read created statefulset.apps/mysql created kubectl get pods ---------------------------- NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 31m mysql-1 2/2 Running 0 31m 동기화 테스트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # 파드 0에서 Mysql Data 생성 kubectl exec -it pod/mysql-0 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, init-mysql (init), clone-mysql (init) --------------------------------- bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 140 Server version: 5.7.41-log MySQL Community Server (GPL) Copyright (c) 2000, 2023, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. --------------------------------- mysql\u0026gt; create database testdb; --------------------------------- Query OK, 1 row affected (0.01 sec) --------------------------------- mysql\u0026gt; use testdb; --------------------------------- Database changed --------------------------------- mysql\u0026gt; create table test(name varchar(10), testdata varchar(50)); --------------------------------- Query OK, 0 rows affected (0.02 sec) --------------------------------- mysql\u0026gt; insert into test values(\u0026#39;han\u0026#39;, \u0026#39;mysql example test\u0026#39;); --------------------------------- Query OK, 1 row affected (0.01 sec) --------------------------------- mysql\u0026gt; select * from test; --------------------------------- +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.00 sec) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 파드 1에서 확인 kubectl exec -it pod/mysql-1 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, init-mysql (init), clone-mysql (init) --------------------------------- bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 306 Server version: 5.7.41 MySQL Community Server (GPL) Copyright (c) 2000, 2023, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. --------------------------------- mysql\u0026gt; use testdb --------------------------------- Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed --------------------------------- mysql\u0026gt; select * from test; --------------------------------- +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.00 sec) 됐다! 로컬 볼륨 기반으로 Mysql 예제를 배포하였고 고가용성 구성을 위해 동기화까지 진행을 완료했다!\n로컬 볼륨 백업과 복원 쿠버네티스에서는 HostPath 볼륨의 대한 백업과 복원 기능은 지원하지 않는다. 노드 별로 백업 스크립트를 작성하거나 써드파티 솔루션을 사용해야 한다. 이번 절에서는 local-path-provisioner 을 사용해서 볼륨을 프로비저닝한만큼, 해당 볼륨에 맞게 백업을 할 수 있는 써드파트 솔루션을 찾아보았다.\nlocal-path-provisioner 깃허브 이슈를 찾아보니 Velero 솔루션을 이용해서 백업을 할 수 있다고 하여 테스트를 진행해보고자 한다.\nVelero? 는 쿠버네티스 클러스터의 리소스와 퍼시스턴트 볼륨을 백업하고 복원하는 데 사용되는 오픈 소스 툴이다.\nVelero 을 사용하기전 local-path-provisioner 볼륨 타입을 Local로 수정해야 한다. Hostpath 볼륨을 지원하지 않으나 Local 볼륨은 Restic 과 연계하여 백업을 지원하기 때문이다. (공식문서)\n이는 Local 볼륨이 쿠버네티스의 자원으로 관리되며, 스토리지 클래스와 퍼시스턴트 볼륨 클레임(PVC)을 사용할 수 있기 때문으로 보인다.\nlocal-path-provisioner 사전 작업\nLocal 볼륨을 수정하기 위해서는 local-path-provisioner 파드의 수정 작업이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 volumeClaimTemplates: - metadata: name: data annotations: volumeType: local # 추가 spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: local-path resources: requests: storage: 10Gi 로컬 볼륨 확인\n1 2 3 4 5 6 kubectl get pv \u0026lt;pv-name\u0026gt; -o yaml --------------------------------- spec: local: # local or HostPath 볼륨 path: /mnt/local-storage/ssd/vol1 ... Velero 설치\n필자는 Velero 백업 버킷을 AWS S3 설정하여 설치를 진행하였다.\n설치는 S3 버켓 생성 및 설정 / Veleo CLI 로 나뉜다.\nS3 버켓 지정 및 IAM 설정\nVelero 에서 S3 버킷을 접근하기 위한 IAM USER ID와 KEY 생성\n1 aws s3 mb s3://\u0026lt;bucket-name\u0026gt; --region ap-northeast-2 IAM Policy 생성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 # 버킷 변수 설정 export BUCKET=\u0026lt;bucket-name\u0026gt; # IAM Policy 생성 cat \u0026gt; velero-policy.json \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:DescribeSnapshots\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateVolume\u0026#34;, \u0026#34;ec2:CreateSnapshot\u0026#34;, \u0026#34;ec2:DeleteSnapshot\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${BUCKET}/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${BUCKET}\u0026#34; ] } ] } EOF # IAM Policy Attach aws iam put-user-policy \\ --user-name velero \\ --policy-name velero \\ --policy-document file://velero-policy.json # IAM user 정보 가져오기 aws iam create-access-key --user-name velero --------------------------------- { \u0026#34;AccessKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;velero\u0026#34;, \u0026#34;AccessKeyId\u0026#34;: \u0026#34;{ID}\u0026#34;, # 밑의 credentials-velero ID에 저장 \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;{KEY}\u0026#34;, # 밑의 credentials-velero KEY에 저장 \u0026#34;CreateDate\u0026#34;: \u0026#34;2023-03-16T04:31:23+00:00\u0026#34; } } # credentials-velero 생성 및 IAM 정보 저장 cat \u0026lt;\u0026lt; EOF \u0026gt; credentials-velero [default] aws_access_key_id=\u0026lt;AWS_ACCESS_KEY_ID\u0026gt; aws_secret_access_key=\u0026lt;AWS_SECRET_ACCESS_KEY\u0026gt; EOF Velero CLI 설치 후 서버 설치\nrestic은 버전 velero 1.10(최신버전) 이상에서 더 이상 지원되지 않는다. 버전을 1.9.6으로 맞춰서 다운받아야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # arch 확인 uname -m --------------------------------- x86_64 # velero CLI 설치 wget https://github.com/vmware-tanzu/velero/releases/download/v1.9.6/velero-v1.9.6-linux-amd64.tar.gz tar xzvf velero-v1.9.6-linux-amd64.tar.gz cp velero-v1.9.6-linux-amd64/velero ~/bin # CLI 확인 velero --------------------------------- Velero is a tool for managing disaster recovery, specifically for Kubernetes cluster resources. It provides a simple, configurable, and operationally robust way to back up your application state and associated data. If you\u0026#39;re familiar with kubectl, Velero supports a similar model, allowing you to execute commands such as \u0026#39;velero get backup\u0026#39; and \u0026#39;velero create schedule\u0026#39;. The same operations can also be performed as \u0026#39;velero backup get\u0026#39; and \u0026#39;velero schedule create\u0026#39;. ... Velero 설치\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 export BUCKET=\u0026lt;bucket-name\u0026gt; export REGION=ap-northeast-2 velero install \\ --provider aws \\ --bucket $BUCKET \\ --secret-file ./credentials-velero \\ --backup-location-config region=$REGION \\ --use-volume-snapshots=false \\ --plugins velero/velero-plugin-for-aws:v1.3.0 \\ --use-restic --------------------------------- ... Deployment/velero: created DaemonSet/restic: attempting to create resource DaemonSet/restic: attempting to create resource client DaemonSet/restic: created Velero is installed! ⛵ Use \u0026#39;kubectl logs deployment/velero -n velero\u0026#39; to view the status. # Velero 확인 kubectl get all -n velero NAME READY STATUS RESTARTS AGE pod/restic-f5ngz 1/1 Running 0 38s pod/restic-x9sk9 1/1 Running 0 37s pod/velero-5f6657d4c8-jttxv 1/1 Running 0 38s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/restic 2 2 2 2 2 \u0026lt;none\u0026gt; 38s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/velero 1/1 1 1 38s NAME DESIRED CURRENT READY AGE replicaset.apps/velero-5f6657d4c8 1 1 1 38s Velero 백업\nVelero는 Restic을 사용하여 PV 볼륨에 대해 백업하는 방법에 두 가지 접근 방식을 지원한다. (공식문서)\n옵트인 접근 방식(default): Restic을 사용하여 백업할 볼륨이 포함된 모든 포드에 annotation을 달아야 한다. 옵트아웃 접근 방식: 모든 포드 볼륨이 Restic을 사용하여 백업되고 백업되지 않아야 하는 볼륨을 옵트아웃할 수 있는 기능이 있다. 이번 절에서는 옵트인 접근 방식을 택할 것이고 Case3 의 Mysql 볼륨을 백업할 예정이다. 백업할 볼륨에 대해 backup.velero.io/backup-volumesannotation 달고 백업을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 주석 추가 pod에 볼륨 정보를 추가 kubectl annotate pod/mysql-0 backup.velero.io/backup-volumes=data # 백업 velero backup create mysql --include-namespaces default --wait --------------------------------- Backup request \u0026#34;mysql\u0026#34; submitted successfully. Waiting for backup to complete. You may safely press ctrl-c to stop waiting - your backup will continue in the background. .................. Backup completed with status: Completed. You may check for more information using the commands `velero backup describe mysql` and `velero backup logs mysql`. # 백업 목록 확인 velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-16 14:22:39 +0900 KST 29d default \u0026lt;none\u0026gt; S3 버킷 조회할 수 있다.\nS3 조회시, backups(쿠버네티스 리소스 저장 경로) 와 restic(PV 볼륨 데이터 저장 경로)에 데이터를 확인할 수 있다.\nVelero 복원\nmysql 배포 파일과 PV를 지우고 복원을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #mysql 지우기 kubectl delete -f ./ kubectl delete pvc/\u0026lt;PVC 볼륨\u0026gt; #velero 복원 velero restore create --from-backup mysql --wait --------------------------------- Restore request \u0026#34;mysql-20230316155542\u0026#34; submitted successfully. Waiting for restore to complete. You may safely press ctrl-c to stop waiting - your restore will continue in the background. ........... Restore completed with status: Completed. You may check for more information using the commands `velero restore describe mysql-20230316155542` and `velero restore logs mysql-20230316155542`. # 쿠버네티스 리소스 복원 확인 kubectl get all --------------------------------- NAME READY STATUS RESTARTS AGE pod/mysql-0 2/2 Running 0 39s pod/mysql-1 0/2 Init:CrashLoopBackOff 2 (18s ago) 39s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 4h39m service/mysql ClusterIP None \u0026lt;none\u0026gt; 3306/TCP 39s service/mysql-read ClusterIP 100.69.52.194 \u0026lt;none\u0026gt; 3306/TCP 39s kubectl get pv --------------------------------- NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-601b919a-cf20-4478-9f28-10d541c66844 10Gi RWO Delete Bound default/data-mysql-0 local-path 71s pvc-b8a766a6-411f-47df-a548-d6b0ee091ea1 10Gi RWO Delete Bound default/data-mysql-1 local-path 70s # Mysql data 확인 kubectl exec -it pod/mysql-0 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, restic-wait (init), init-mysql (init), clone-mysql (init) bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 73 Server version: 5.7.41-log MySQL Community Server (GPL) mysql\u0026gt; use testdb; --------------------------------- Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed --------------------------------- mysql\u0026gt; select * from test; +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.01 sec) 데이터가 그대로 보존되어 있다!\nVelero 스케쥴 백업\n크론탭처럼 백업도 Velero가 가능하다. 다음은 5분마다 백업을 진행하는 예제이다.\n백업된 오브젝트별 데이터 변화를 위해 데이터베이스의 데이터를 수정 후 복원을 해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 velero schedule create mysql-crontab --include-namespaces default --schedule=\u0026#34;*/5 * * * *\u0026#34; velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-16 15:51:44 +0900 KST 29d default \u0026lt;none\u0026gt; mysql-crontab-20230316072517 Completed 0 0 2023-03-16 16:25:17 +0900 KST 29d default \u0026lt;none\u0026gt; mysql-crontab-20230316072017 Completed 0 0 2023-03-16 16:20:17 +0900 KST 29d default \u0026lt;none\u0026gt; #DB 접속 후 데이터 삭제 select * from test; +------+---------------------+ | name | testdata | +------+---------------------+ | han | mysql example test | | han | mysql example test2 | | han | mysql example test3 | +------+---------------------+ 3 rows in set (0.00 sec) mysql\u0026gt; delete from test; Query OK, 3 rows affected (0.00 sec) mysql\u0026gt; select * from test; Empty set (0.00 sec) #velero 복원 velero restore create --from-backup mysql-crontab-20230316072017 --wait --------------------------------- Restore request \u0026#34;mysql-crontab-20230316072017-20230316163030\u0026#34; submitted successfully. Waiting for restore to complete. You may safely press ctrl-c to stop waiting - your restore will continue in the background. Restore completed with status: Completed. You may check for more information using the commands `velero restore describe mysql-crontab-20230316072017-20230316163030` and `velero restore logs mysql-crontab-20230316072017-20230316163030`. # Mysql DATA 확인 mysql\u0026gt; select * from test; Empty set (0.00 sec) Mysql DATA 확인 의 결과가 예상과 다르다. 백업 진행 이후 백업 데이터인 행3개가 있어야 하나, 최신 데이터가 조회된다.\n이는 Mysql 파드가 2개 있어 파드간 데이터 무결성이 보장되어 백업 데이터 파일을 옮긴다 한들 수정이 안되기 때문이다. 따라서 백업본의 결과를 얻기 위해서는 PV 볼륨과 Mysql 리소스를 지우고 다시 복원을 해야한다.\nVelero 클러스터 마이그레이션\n클러스터간 마이그레이션 방법으로 Velero를 활용할 수 있다. 공식문서를 참고하여 테스트를 진행해보겠다.\n진행 전 Velero 에서 클러스터 마이그레이션시 제약사항이 있으니 확인이 필요하다.\nVelero는 기본적으로 클라우드 공급자 간에 PV 스냅샷의 마이그레이션을 지원하지 않는다. 이를 위한 방안으로 restic을 이용하여 파일시스템 레벨의 마이그레이션을 진행해야 한다. Velero는 백업이 수행된 위치보다 낮은 Kubernetes 버전이 있는 클러스터로의 복원을 지원하지 않는다. 동일한 버전의 Kubernetes를 실행하지 않는 클러스터 간에 워크로드를 마이그레이션하는 것이 가능할 수 있지만 마이그레이션 전에 각 사용자 정의 리소스에 대한 클러스터 간 API 그룹의 호환성을 포함하여 몇 가지 요인을 고려해야 한다. AWS 및 Azure용 Velero 플러그인은 리전 간 데이터 마이그레이션을 지원하지 않는다. 이를 위한 방안으로 restic을 사용해야 한다. 우리는 restic을 사용하니 제약사항에 자유롭다. 클러스터 마이그레이션의 예에 대한 일환으로 클러스터를 재 구축하여 앞서 생성한 Mysql 애플리케이션을 복원해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # velero 설치 export BUCKET=hanhorang-velero-s3 export REGION=ap-northeast-2 velero install \\ --provider aws \\ --bucket $BUCKET \\ --secret-file ./credentials-velero \\ --backup-location-config region=$REGION \\ --use-volume-snapshots=false \\ --plugins velero/velero-plugin-for-aws:v1.3.0 \\ --use-restic --------------------------------- velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-17 00:31:21 +0900 KST 29d default \u0026lt;none\u0026gt; velero 백업 개체가 그대로 있다. 복원 과정은 앞 과정과 동일하기에 생략하였다. 클러스터를 재구축해도 Velero 백업 객체가 남아있는 이유가 무엇일까?\nVelero 리소스는 오브젝트 스토리지의 백업 파일과 동기화되기 때문이다. 설치 과정에서 삭제한 클러스터와 새로운 클러스터의 Velero 버킷이 동일하므로 백업 객체가 그대로 있음을 확인하였다. 클러스터1과 클러스터2가 병행 운영시에도 버킷 데이터에 따라 Velero 리소스가 동기화가 이루어진다는데 기본 동기화 간격이 1분으로 이 부분을 확인하여 마이그레이션을 진행하면 될 것 같다.\n로컬 볼륨 모니터링 PV 볼륨 성능 확인할 수 있는 krew df-pv 도구가 있으나, HostPath 볼륨은 인스토어스토어라서 확인되지 않는다. 하지만 Local 볼륨은 확인이 가능하다.\n1 2 3 4 5 kubectl krew df-pv \u0026amp;\u0026amp; kubectl df-pv --------------------------------- PV NAME PVC NAME NAMESPACE NODE NAME POD NAME VOLUME MOUNT NAME SIZE USED AVAILABLE %USED IUSED IFREE %IUSED pvc-678e7407-9f76-4fd1-a9ad-8c2581b8df36 data-mysql-0 default i-0314088c74eee3276 mysql-0 data 123Gi 4Gi 119Gi 3.68 119172 16395900 0.72 pvc-d2c3cbcb-13ec-46de-81e9-1178d25dd4ad data-mysql-1 default i-0ab66ac834dc8710d mysql-1 data 123Gi 4Gi 119Gi 3.74 121203 16393869 0.73 성능 측정 로컬 스토리지의 성능 측정 방법으로 iostat 명령어와 krew 툴인 kubestr을 사용하여 성능을 측정하겠다.\nkubestr 스토리지 IOPS 측정 툴이다. 스토리지 사용에 따른 검증용으로 사용하기 좋은 툴인 것 같다. 예제도 많으니 링크를 통해 확인하자. 이번 예제에서는 실습 참고용 책에서 제공해주신 예제 스크립트를 통해 성능을 측정할 것이다.\nfio-read.fio\nfio를 사용하여 4KB 블록 크기를 가지는 랜덤 읽기 및 쓰기\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [global] ioengine=libaio direct=1 bs=4k runtime=120 time_based=1 iodepth=16 numjobs=4 # numjobs=16 size=1g group_reporting rw=randrw rwmixread=100 rwmixwrite=0 [read] ioengine=libaio : Asynchronous I/O를 수행하기 위해 libaio 라이브러리를 사용합니다. direct=1 : Direct I/O를 사용합니다. bs=4k : I/O 요청에 사용되는 블록 크기는 4KB입니다. runtime=120 : 120초 동안 작업을 실행합니다. time_based=1 : 시간 기반으로 작업을 수행합니다. iodepth=16 : 각 작업에 대한 I/O 요청 수를 16개로 설정합니다. numjobs=4 : 4개의 작업을 수행합니다. size=1g : 각 작업에 대한 데이터 크기는 1GB입니다. group_reporting : 모든 작업 결과를 통합하여 보고합니다. rw=randrw : 랜덤 읽기 및 쓰기 작업을 수행합니다. rwmixread=100 : 작업 중 읽기 작업의 비율은 100%입니다. rwmixwrite=0 : 작업 중 쓰기 작업의 비율은 0%입니다. fio-write.fio\n루트 디렉토리에서 4KB 블록 크기로 16개의 job이 16개의 i/o depth로 실행되며, 실행 시간이 120초인 1GB 파일에 대해 100% 쓰기 랜덤 테스트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [global] ioengine=libaio numjobs=16 iodepth=16 direct=1 bs=4k runtime=120 time_based=1 size=1g group_reporting rw=randrw rwmixread=0 rwmixwrite=100 directory=/ [read] rwmixwrite=100: 100% 쓰기 테스트를 수행합니다. directory=/: 테스트할 디렉토리를 루트 디렉토리로 설정합니다. 성능 측정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubestr fio -f fio-write.fio -s local-path --size 10G --------------------------------- PVC created kubestr-fio-pvc-hp69m Pod created kubestr-fio-pod-fcvcp Running FIO test (fio-write.fio) on StorageClass (local-path) with a PVC of Size (10G) Elapsed time- 4m11.664545514s FIO test results: FIO version - fio-3.30 Global options - ioengine=libaio verify= direct=1 gtod_reduce= JobName: blocksize= filesize= iodepth= rw= write: IOPS=3023.577881 BW(KiB/s)=12094 iops: min=992 max=8640 avg=3023.464355 bw(KiB/s): min=3968 max=34564 avg=12093.908203 Disk stats (read/write): nvme0n1: ios=0/362587 merge=0/173 ticks=0/6330627 in_queue=6330627, util=99.954132% fio-write 실행 결과, 쓰기 평균 iops가 3034인 것을 확인할 수 있다.\nQ. 테스트가 안될 경우, PV 상태 Pending?\n해당 경우는 PVC 요청에 맞는 볼륨의 PV가 없을때 발생한다. PVC 요청에 맞는 볼륨이 있는지 또는 Local-path-provisioner 설정을 확인하자. 필자의 경우 Local-path-provisioner 설정으로 Pending 이 발생했다.\n1 2 3 4 kubectl get pvc -A --------------------------------- NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default kubestr-fio-pvc-dl7vx Pending local-path 4m34 노드 볼륨 IO 성능 측정 iostat 명령어를 통해 실시간으로 스토리지의 성능과 사용량에 관한 정보를 확인할 수 있다. 클러스터 운영시 스토리지 트러블슈팅의 일환으로 사용하자.\n아래 예제는 fio-write 스크립트 실행 중 iostat 를 실행하여 스토리지 정보를 확인한 예제이다.\n1 2 3 4 5 6 7 # iostat 패키지 설치 sudo apt install -y sysstat iostat -xmdz 1 -p nvme2n1 --------------------------------- Device r/s rMB/s rrqm/s %rrqm r_await rareq-sz w/s wMB/s wrqm/s %wrqm w_await wareq-sz d/s dMB/s drqm/s %drqm d_await dareq-sz aqu-sz %util nvme2n1 0.00 0.00 0.00 0.00 0.00 0.00 24.00 0.10 2.00 7.69 0.75 4.33 0.00 0.00 0.00 0.00 0.00 0.00 0.02 4.00 w/s: 초당 쓰기 요청 수 wMB/s: 초당 쓴 데이터 양 (메가바이트/초) wrqm/s: 초당 쓰기 요청 큐에 들어간 요청 수 %wrqm: 쓰기 요청 큐에 들어간 요청 비율 w_await: 쓰기 요청 대기 시간, 드라이버 요청 대기열에서 기다린 시간과 장치의 I/O 응답시간을 모두 포함한다. (밀리초) wareq-sz: 평균 쓰기 요청 크기 (섹터) aqu-sz: 요청 대기열의 평균 길이 %util: 디스크 사용률 (0 ~ 100%) 로컬 스토리지 QnA Q1. AWS 인스토어 스토어에 대한 볼륨 스토리지 조절이 가능한가?\n인스턴스 스토어는 EC2 인스턴스의 로컬 디스크를 사용하는 것이기 때문에 크기 조정이 불가능하다. 인스턴스 스토어를 사용하는 EC2 인스턴스를 변경하거나 새로운 인스턴스를 시작하여 크기를 조정해야 한다. 일반적으로 DB(Mysql) 볼륨 사용량으로 10G~100G을 설정한다고 하지만, 애플리케이션 규모와 기간에 따라 사용량 예측이 힘들다. 필요시 백업을 통해 인스토어 스토어 볼륨을 변경할 수 있도록 하자. EC2 인스턴스에 따른 볼륨(SSD) 는 링크에서 확인이 가능하다.\nQ2. hostpath 볼륨을 여러개의 파드가 동시에 사용할 수 있을까?\n노드의 파일 시스템은 여러 개의 프로세스가 동시에 접근할 수 있는 공유 리소스이기 때문에 여러 개의 파드가 하나의 hostpath를 사용할 수 있다. 하지만, 데이터 손상이나 권한 오류가 발생할 수 있다. 하나의 파드가 파일을 쓴 후에 다른 파드가 동일한 파일을 읽을 경우나, 여러 개의 파드가 동시에 동일한 파일을 쓰는 경우가 있기 때문이다. 이를 위해 적절한 동기화 및 락 메커니즘을 구현이 필요하다.\n테스트로 스터디에서 공유해주신 localpath-fail.yaml 를 수정해서 로그를 확인해봤다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 0.01; done\u0026#34;] # 0.01 로 수정 volumeMounts: - name: pod-persistent-volume mountPath: /data volumes: - name: pod-persistent-volume persistentVolumeClaim: claimName: localpath-claim 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 파드 10개로 증가후 테스트 kubectl scale deployment date-pod --replicas=10 -------------------------------------- # 로그 확인 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:05 | wc -l 355 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:05 | wc -l 355 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:06 | wc -l 457 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:07 | wc -l 511 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:08 | wc -l 513 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:09 | wc -l 530 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:10 | wc -l 506 0.01초 x 10 개 파드로 1초당 약 1000개의 결과가 나와야 하지만 350~530 개의 결과가 나오는 것을 확인하였다. 앞서 kubestr 측정에서 쓰기 IOPS를 측정하여 약 3000이 나왔음에도 턱없이 부족한 것을 알 수 있다.\nQ3. Velero 의 백업 용량은 몇 인가?\n사용 버킷에 따라 달라진다. S3 버킷 기준으로는 해당 버킷의 용량에 따라가는데 최대 5테라까지 지원이 가능하다.\n","date":"Mar 18","permalink":"https://HanHoRang31.github.io/post/pkos2-2-localstorage/","tags":["KANS","kops","cloud","AWS","kubernetes","Volume","velero","local-path-provisioner"],"title":"[PKOS] 쿠버네티스 로컬스토리지와 Velero를 통한 백업 테스트"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. PKOS 1주차 스터디 내용과 느낌점을 정리하고자 한다. 스터디하면서 매번 느끼는 거지만 정말 괴수분들 너무 많고, 배울 점이 정말 많다… 특히 모임장님이신 가시다님의 스터디 내용은 볼 때마다 감탄만 나온다. 쿠버네티스에 대한 원리와 세부 컴포넌트에 대한 명령어까지 배운 점이 많다. 본 글에서는 필자가 배운 내용을 정리함과 동시에 개념에 대해 몰랐던 부분이나, 트러블슈팅에 대한 내용을 중점적으로 정리하였다.\n추가로, 몰랐던 부분은 ChatGPT를 활용하여 작성하였다. ChatGPT가 주는 답변은 대체로 만족하지만, 공식 문서에 대한 내용과 비교하여 다른 내용이 일부 존재한다. ChatGPT 활용시, 공식 문서와 이중 검증이 필요하다고 본다. 본 글에서도 답변 내용을 일부 수정하여 작성한다.\nkops? kops는 쿠버네티스 클러스터를 생성, 업그레이드, 관리하는 데 사용되는 오픈 소스 도구이다. 명령어 툴로 쉽게 쿠버네티스 클러스터를 구성하고 관리할 수 있는 툴이라고 이해하면 되겠다. 쿠버네티스 클러스터의 인프라를 코드로 정의하고 관리하는데 사용할 수 있어 IaC 이며, 같은 IaC 툴인 Terraform(테라폼)과의 특징을 비교하면 다음과 같다.\nkops과 Terraform 특징 비교 표\n특징 kops Terraform 지원하는 클라우드 플랫폼(Provider) AWS, GCP, OpenStack, DigitalOcean 등 AWS, GCP, Azure, Oracle Cloud, Alibaba Cloud, VMware, OpenStack 등 관리 대상 Kubernetes 클러스터 인프라스트럭처 (서버, 네트워크, 데이터베이스 등) 코드 작성 방식 YAML 파일 HCL (HashiCorp Configuration Language) 상태 관리 상태를 지정된 저장소(state)에 저장 상태를 지정된 저장소(backend)에 저장 장점 간단하고 직관적인 클러스터 구성 AWS뿐만 아니라 다른 클라우드 프로바이더도 지원 단점 Kubernetes 클러스터만 지원 Kops에 비해 배우기가 어려움 그렇다면 kops는 언제 써야 할까?\n모임장님 의견과 동일하게 교육용이 적합하다고 본다. 쿠버네티스 클러스터 구축이 간단하고 배우기 쉽다. 또한, 배포 속도도 빠르다. 쿠버네티스 관리형 서비스인 EKS 와 비교했을 때, 마스터 노드들을 세부적으로 알 수 있어 세부 원리 이해에 좋고, 비용도 저렴하다. kops 공식문서도 정리가 잘 되어 있다. 눈여겨 볼 점은 공식문서의 addon과 Operation 부분이다. 클러스터 관리를 및 addon 배포를 간단하게 테스트할 수 있고 배포 yaml를 확인할 수 있기 때문이다.\n실습 스터디 내용으로 kops를 통해 AWS에 쿠버네티스 클러스터를 구축하고, 게임 마리오를 예제로 배포하였다. 과정은 크게 3가지로 진행하였다.\n베스천 서버(kops-ec2) 구성 kops 를 통한 클러스터 구축 및 확인 External DNS 와 게임, 슈퍼마리오 배포 실습 과정 전 사전 작업으로 퍼블릭 도메인 구입, 키 페어 생성, S3 버킷 생성, AWS IAM 자격 증명을 진행하였다. 사전 작업 내용은 공식 문서에서 참고가 가능하다.\nQ. 퍼블릭 도메인 구입 이유 ?\n퍼블릭 도메인은 쿠버네티스 클러스터 이름으로 사용하기 위하여 구입하였다. 클러스터 이름을 도메인으로 설정하면 외부에서 서비스 디스커버리 및 클러스터 액세스가 쉽게 가능해지기 때문이다. 스터디에서는 해당 도메인을 통해 클러스터 구성 확인과 게임 배포 후 접근을 위해 사용하였다.\n1. 베스천 서버(kops-ec2) 구성 베스천 서버 구성은 모임장님이 공유해주신 CloudFormation 템플릿을 통해 구성하였다. 템플릿 구성은 VPC 와 igw 구성같은 AWS 네트워크 구성과 EC2 서버 설정으로 되어있다. 그 중 EC2 서버 설정 스크립트를 확인할 필요가 있는데 kops와 필요 패키지를 같이 설치해주기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash # 호스트 이름 변경 hostnamectl --static set-hostname kops-ec2 # EC2 서버 시간을 서울로 변경 ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # Install Packages cd /root yum -y install tree jq git htop ## kubectl 설치 curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl ## kops 설치 curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d \u0026#39;\u0026#34;\u0026#39; -f 4)/kops-linux-amd64 chmod +x kops mv kops /usr/local/bin/kops ## awscli 설치 curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install ## 환경 변수 설정 export PATH=/usr/local/bin:$PATH source ~/.bash_profile ## aws cli 자동 완성 설정 complete -C \u0026#39;/usr/local/bin/aws_completer\u0026#39; aws ## SSH 설정 ssh-keygen -t rsa -N \u0026#34;\u0026#34; -f /root/.ssh/id_rsa ## vi -\u0026gt; vim 으로 설정 echo \u0026#39;alias vi=vim\u0026#39; \u0026gt;\u0026gt; /etc/profile ## root 계정 변환 echo \u0026#39;sudo su -\u0026#39; \u0026gt;\u0026gt; /home/ec2-user/.bashrc ## helm 설치 curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash wget https://github.com/andreazorzetto/yh/releases/download/v0.4.0/yh-linux-amd64.zip unzip yh-linux-amd64.zip mv yh /usr/local/bin/ 베스천 서버 구성 과정 중 에러 발생 시, AWS Cloudformation에서 로그를 확인할 수 있다. 필자는 VPC가 최대여서 베스천 서버 구성이 안되었다. 해결을 위해 필요없는 VPC를 삭제하고, 템플릿으로 생성한 Cloudformtation 스택을 삭제하고 재실행하였다.\n2. kops를 통한 클러스터 구축 및 확인 쿠버네티스 클러스터 구축은 kops 명령어로 구축하였다. 클러스터 생성에 대한 옵션은 공식 문서에서 참고할 수 있었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 클러스터 생성 bash ## export KOPS_STATE_STORE=s3://( 클러스터 관리 저장소로 활용할 버킷 이름) ## export KOPS_CLUSTER_NAME=\u0026lt;도메인 메인 주소\u0026gt; ## export REGION=ap-northeast-2 지역 kops create cluster \\ --cloud aws \\ --name $KOPS_CLUSTER_NAME \\ --state s3://$KOPS_STATE_STORE \\ --zones \u0026#34;$REGION\u0026#34;a,\u0026#34;$REGION\u0026#34;c \\ --networking amazonvpc \\ --network-cidr 172.30.0.0/16 \\ --kubernetes-version \u0026#34;1.24.10\u0026#34; \\ --node-count 2 \\ --node-size t3.medium \\ --master-size t3.medium \\ --ssh-public-key ~/.ssh/id_rsa.pub \\ -y --state : 클러스터 관련 설정 파일들이 저장된다. 이렇게 저장된 설정 파일들은 나중에 클러스터를 업데이트하거나 삭제할 때 사용된다. 실습에서는 외부 스토리지인 S3를 이용하였다. —state 플래그를 사용하지 않으면 로컬 저장소인 ~/.kops 에 클러스터 설정 파일이 저장된다. 하지만 로컬 시스템에 대한 의존성이 높아지고 협업시에 대한 공유성이 떨어져 외부 스토리지를 사용하는 것을 권장한다. 필자는 클러스터 생성시 VPC 개수 이유(error creating VPC: VpcLimitExceeded) 로 클러스터 생성이 되지 않았다.\n재설치를 위해서는 기존 클러스터 삭제가 필요하다. 실습 내용의 구성 단계에 따라 삭제하면 된다.\n1 2 3 4 5 1. EC2 Auto Scaling 그룹 삭제 2. EC2 시작 템플릿 Launch Templates 삭제 3. S3 버킷 비우기 4. Route53에 추가된 A 레코드 3개 삭제 5. CloudFormation 삭제 VPC 문제는 S3 버킷만 지우면 됐었다. Cloudformtation 스택 생성시 에러가 발생하여 AWS 리소스들은 생성되지 않았기 때문이다. 아래는 kops 를 통한 클러스터 구성 과정을 정리하였다\n1 2 3 4 5 6 7 8 1. kops create cluster 명령어 실행 2. 클러스터 구성 정보를 S3에 저장 3. 클러스터 구성 정보를 기반으로 CloudFormation 스택 생성 4. VPC 및 관련 리소스 생성 5. 마스터 노드 EC2 인스턴스 생성 6. 노드 그룹 EC2 인스턴스 생성 7. 노드 그룹 EC2 인스턴스가 마스터 노드를 참조하여 클러스터에 가입 8. 클러스터가 실행되고 kubectl을 통해 액세스 가능해짐 클러스터 구성 확인 확인\n스터디 실습 내용으로 클러스터 구성 확인에 대한 것도 시간을 할당하여 확인하였다. 쿠버네티스 클러스터가 복잡한 만큼 확인할 것이 많았는데, 실습 내용을 참고로 하여 명령어를 정리해보았다.\n클러스터 도메인 확인\n클러스터 구성시 클러스터 이름을 퍼블릭 도메인으로 입력하였다. 필자는 퍼블릭 도메인을 AWS Route53 에서 구매하였는데, 이 같은 경우 route53에서 A레코드도메인이 추가된 것을 확인할 수 있다.\n클러스터 구성 정보 확인\n클러스터 구성 정보는 kubectl 와 kops 툴로 확인이 가능하다. kops 툴을 통해 클러스터 정보 뿐만 아니라 이미지 확인(assets), 보안 정보 확인이 가능하다. 공식문서를 링크한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 클러스터 확인 kops get cluster # 클러스터 인스턴스 그룹 확인 kops get ig # 클러스터 인스턴스 확인 kops get instances # 클러스터 접근 정보 확인 kubectl get nodes -v6 # 클러스터 배포 파드 확인 kubectl get pods -A # 클러스터 정보 확인 k**ubectl cluster-info dump** 클러스터 세부 구성 확인\n클러스터 세부 구성 확인으로 스토리지, 네트워크, 파드, 마스터 노드 컴포넌트들의 구성 정보를 확인하였다. 추후 참고용을 위해 정리해둔다. 퍼블릭 도메인을 이용하여 구성 정보를 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # storage class 확인 kubectl get sc # 노드별 스토리지 확인 lsblk df -hT # 네트워크 확인 sudo iptables -t nat -S # 마스터 노드 컴포넌트 확인 **tree /etc/kubernetes/manifests/ # kubelet 작동 확인 systemctl status kubelet # 컨테이너 접근 방법 확인 ps axf | grep /usr/bin/containerd # ec2 메타데이터 확인 - IAM role 확인** TOKEN=`curl -s -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34;` echo $TOKEN curl -s -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; –v http://169.254.169.254/latest/meta-data/iam/security-credentials/\u0026lt;도메인\u0026gt; | jq kubectl get sc : kops로 클러스터 구성시 스토리지 CSI 가 기본 제공된다. 스토리지 CSI 는 클러스터 스토리지 관리 기능 플러그인이다. kops-csi 와 kops-ssd 가 존재하는데 kops-csi 는 AWS EBS(gp3), EFS, GCP 등의 스토리지를 사용할 수 있는 플러그인이고, kops-ssd 는 NVMe SSD 기반 인스턴스 스토리지를 사용할 수 있는 플러그인이다.\nsudo iptables -t nat -S : Kubernetes에서 사용하는 iptables rule 을 확인할 수 있다. 이번 장에서 구성한 네트워크 CNI는 VPC로 노드 IP와 서비스 IP의 할당별 iptables rule을 확인할 수 있다.\n파드 IP 설정 iptables rule\n임의의 애플리케이션 파드 IP를 확인했다. 체인을 따라가보면 들어오는 패킷 중 목적지 IP 주소가 100.64.59.121/32이고, 프로토콜이 TCP이며, 목적지 포트가 80인 패킷이 172.30.41.131:8080(파드 할당 IP) 으로 가는 것을 확인 할 수 있다. 노드 IP 설정 iptables rule\n임의의 노트 IP를 확인했다. 노드 IP에 대해NAT 규칙을 적용하는 것을 확인할 수 있었다. tree /etc/kubernetes/manifests/ : 마스터 노드에서 작동하는 컴포넌트 mainfest 정보들이다. kubelet에 의해 Static Pod로 배포되어 있으며 API 서버의 명령어(kubectl apply, delete) 등으로 관리가 불가능하다. kubelet을 통해 아래 컴포넌트의 manifest 를 모니터링하고 있으며 manifest 수정시 자동으로 업데이트된다.\n1 2 3 4 5 6 7 /etc/kubernetes/manifests/ ├── etcd-events-master-ap-northeast-2a.manifest ├── etcd-main-master-ap-northeast-2a.manifest ├── kube-apiserver.manifest ├── kube-controller-manager.manifest ├── kube-proxy.manifest └── kube-scheduler.manifest 3. External DNS, 게임 마리오 배포 실습 예제로는 게임 마리오를 배포하였다. External DNS은 쿠버네티스 addon으로, 내부에서 동작하는 마리오 서비스의 IP 주소를 외부의 DNS 서버에 자동으로 등록시켜주기 위해 배포하였다. 인상 깊은 점은 addon인 ExternalDNS 배포였는데, kops를 통해 클러스터 정보를 수정하니 자동으로 배포되었다.\n공식 문서1, 공식 문서2 에는 클러스터 스펙 수정시의 옵션이 잘 정리되어 있다. 추후 기능 테스트시 접근을 위해 남겨둔다.\n1 2 3 4 5 6 7 8 9 10 11 12 # 클러스터 수정 kops edit cluster # 아래 spec 에 ExternalDNS 정보 추가** -------------------------- spec: externalDns: provider: external-dns -------------------------- # 클러스터 업데이트 후, 롤링 업데이트 진행 kops update cluster --yes \u0026amp;\u0026amp; echo \u0026amp;\u0026amp; sleep 3 \u0026amp;\u0026amp; kops rolling-update cluster 트러블슈팅 API 서버 접근시, 아래의 에러 메세지(couldn’t get current server API group list: ~ dial tcp connect refused )가 확인되었다.\n원인은 클러스터 접근 토큰 만료였다. kops 명령어를 통해 자격 증명을 재발급(kops export kubeconfig) 받을 수 있다하여 시도해보았지만, 쿠버네티스 유저가 없어서 그런지 접근 토큰이 null 로 발급되었다. kops 클러스터 관리 저장소인 s3에도 접근 토큰 정보가 없었다. 필자는 마스터 노드에 접근(~/.kube/config)하여 토큰을 복사하니 해결하였다.\n마치며 kops 간단하다! 특히 마스터 노드를 직접 접근하여 컨트롤할 수 있어 클러스터 동작 이해에 좋은 툴이였다. 공식문서도 참고할 것이 많다. 공식 문서 kops operation 에 재밌는 주제들(카펜터)이 많던데 얼른 테스트해보고 싶은 마음이다. 시간나는대로 정리해서 올려보겠다.\n","date":"Mar 10","permalink":"https://HanHoRang31.github.io/post/pkos2-1-kops/","tags":["KANS","kops","cloud","AWS","kubernetes"],"title":"[PKOS] Kops로 클러스터 구축하기"},{"categories":null,"contents":"","date":"Jan 01","permalink":"https://HanHoRang31.github.io/articles/","tags":null,"title":"Articles"}]