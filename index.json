[{"categories":null,"contents":"","date":"Mar 05","permalink":"https://HanHoRang31.github.io/projects/a_project/","tags":null,"title":"HanHoRang Git Repo"},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. 2주차 스터디에서는 쿠버네티스의 네트워크와 스토리지를 중점적으로 공부하였다. 분량이 많아 네트워크와 스토리지를 나눠서 블로그를 작성할 예정이다. 이번 블로그 글에서는 로컬 스토리지에 대해 공유하겠다. 일반적으로 로컬 스토리지는 IOPS 성능이 특화되어 있지만 노드에 종속되어 있어 고가용성이나 스토리지 기능에 제한이 있다. 이러한 제한을 없애기 위한 과정으로 로컬 스토리지의 Hostpath, local 볼륨을 마운트하여 테스트를 진행할 것이고, 마지막으로는 로컬 볼륨에서 고가용성과 스토리지 기능(백업)을 가진 Mysql 데이터베이스를 구성하겠다. 추가로 스토리지 성능 측정과 모니터링 과정, QnA를 준비하였다. 본론으로 들어가서, 쿠버네티스에서 스토리지를 사용하는 이유는 무엇일까? 스토리지가 데이터를 저장하는 용도인 것처럼 데이터 저장을 위해서이다. 예를 들어, 데이터베이스같은 애플리케이션을 파드로 운영한다고 가정해보자, 파드 라이프사이클과 별개로 데이터가 보존되어야 한다. 이를 위해 쿠버네티스에서는 PV(Persistent Volume)과 PVC(Persistent Volume Claim) 리소스를 제공한다. 또한, 데이터 관리 방법(데이터 저장 위치, 데이터 공유, 확장성)에 따라 여러 스토리지 볼륨과 기능을 제공한다. 이처럼 데이터를 보존해야 하는 애플리케이션을 상태있는(Stateful) 애플리케이션이라 칭하며 스토리지를 통해 클러스터 내의 컨테이너에 안정적이고 지속적인 데이터를 제공할 수 있다.\n로컬 스토리지 로컬 스토리지는 말 그대로 파드의 스토리지로 서버 내부 볼륨의 스토리지를 사용하는 것이다. AWS EC2는 내부 볼륨인 인스토어 스토어를 사용한다. 로컬 스토리지 구현은 쿠버네티스에서 HostPath, Local 볼륨 마운트로 나뉘어 사용이 가능하다. HostPath 볼륨과 Local 볼륨의 차이는 쿠버네티스 볼륨 리소스(PV) 사용 유무에 따라 구분한다.\n일반적으로 내부 볼륨의 스토리지를 사용하는 만큼, 다른 원격 스토리지와 비교했을때 IOPS 성능이 뛰어나다. cncf 공식사이트에서 IOPS 기준 약 2~3배의 차이가 난다고 하니 성능 필요의 애플리케이션에서는 도입을 고려할 만하다. 그리고 로컬스토리지는 EC2 에 종속되어 있어 고가용성 구성과 백업같은 기능 사용에 추가 구성이 필요하다. 필자는 이를 해결하기 위해 볼륨프로비저닝 플러그인인 Local-path-provisioner 와 백업 솔루션인 Velero를 사용하였다.\n먼저 Local-path-provisioner 플러그인 설치 과정을 살펴볼 것이고, 로컬스토리지 이해를 위해 3가지의 케이스로 나뉘어 테스트를 진행하겠다.\nLocal-path-provisioner PV를 통한 고가용성 테스트 Hospath PV를 통한 고가용성 테스트 파드간 데이터 동기화 구성 local-path-provisioner 볼륨 프로비저닝 플러그인 중 하나로, 로컬 노드의 파일 시스템 경로를 사용하여 PVC(Persistent Volume Claim)를 만들어주는 역할을 한다. PVC를 통해 볼륨을 요청하면 PV가 자동으로 생성되어 연결된다고 보면 된다.\n설치시, 로컬 노드에 대한 볼륨 설정이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 curl -s -O https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml vim local-path-storage.yaml ---------------------------- apiVersion: apps/v1 kind: Deployment metadata: name: local-path-provisioner namespace: local-path-storage spec: replicas: 1 selector: matchLabels: app: local-path-provisioner template: metadata: labels: app: local-path-provisioner spec: # 추가 부분 **nodeSelector: kubernetes.io/hostname: \u0026#34;마스터 노드 이름 입력 \u0026#34; tolerations: - effect: NoSchedule key: node-role.kubernetes.io/control-plane operator: Exists** ... kind: ConfigMap apiVersion: v1 metadata: name: local-path-config namespace: local-path-storage data: config.json: |- { \u0026#34;nodePathMap\u0026#34;:[ { \u0026#34;node\u0026#34;:\u0026#34;DEFAULT_PATH_FOR_NON_LISTED_NODES\u0026#34;, \u0026#34;paths\u0026#34;:[\u0026#34;**/data/local-path**\u0026#34;] # 추가 부분 } ] } ---------------------------- Deployment / spec.spec 에서 nodeselector 와 tolerations 로 볼륨 배치 노드 설정(마스터 노드로 파드 배치) Configmap / data config.json에서 로컬 볼륨 PATH 설정 설치 확인\n1 2 3 4 kubectl get sc local-path ---------------------------- NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path rancher.io/local-path Delete WaitForFirstConsumer false 29m Case 1. Local-path-provisioner PV를 통한 고가용성 테스트 HostPath 볼륨이 IOPS 성능이 좋은 것을 앞서 확인하였다. 성능적으로 사용하기 좋은 볼륨이라 할 수 있으나 고가용성 구성이 필요하다.\nHostpath 볼륨의 문제점으로 노드간 마이그레이션에 자유롭지 못하기 때문이다. 이해를 위해 직접 예제 파드를 배포해보고 고가용성을 테스트 해보겠다.\nLocal-path-provisioner PV를 통한 고가용성 테스트\n앞서 배포한 Local-path-provisioner 를 통해 PV를 생성하여 파드를 배포하고 고가용성 구성을 테스트해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # 파드 예제 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: localpath-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi storageClassName: \u0026#34;local-path\u0026#34; --- apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: pod-persistent-volume mountPath: /data volumes: - name: pod-persistent-volume persistentVolumeClaim: claimName: localpath-claim 파드 배포 후 노드 드레인을 진행해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 배포 파드의 노드 확인 PODNODE=$(kubectl get pod -l app=date -o jsonpath={.items[0].spec.nodeName}) echo $PODNODE # 노드 드레인과 파드 모니터링 kubectl drain $PODNODE --force --ignore-daemonsets --delete-emptydir-data \u0026amp;\u0026amp; kubectl get pod -w --------------------------------- node/i-0c41dc0f6eeb01730 cordoned Warning: ignoring DaemonSet-managed Pods: kube-system/aws-node-w8bxm, kube-system/ebs-csi-node-thndq, kube-system/node-local-dns-v2hhc evicting pod default/date-pod-d95d6b8f-q9skb evicting pod kube-system/metrics-server-5f65d889cd-9btc7 pod/metrics-server-5f65d889cd-9btc7 evicted pod/date-pod-d95d6b8f-q9skb evicted node/i-0c41dc0f6eeb01730 drained NAME READY STATUS RESTARTS AGE date-pod-d95d6b8f-x5vrb 0/1 Pending 0 2m 노드 드레인시, 상태가 Pending 인 것을 확인할 수 있다. 이는 다른 노드에 PV볼륨이 없기 때문이다.\n마찬가지로 파드를 5개로 추가해도 볼륨이 있는 노드에만 파드가 올라온 것을 확인할 수 있다.\n1 2 # 예제 파드 개수 5개로 증가 kubectl scale deployment date-pod --replicas=5 Case2. HostPath를 통한 고가용성 테스트 볼륨 Hostpath로 노드 PATH를 직접 지정하여 고가용성을 테스트해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: log mountPath: /data volumes: - name: log hostPath: path: \u0026#34;/data\u0026#34; type: DirectoryOrCreate 파드 드레인과 개수 조절시 노드에 상관없이 배포가 진행된다.\n하지만, 볼륨별로 데이터가 쌓이는 것이 다르다. 각 노드에 들어가서 로그를 확인해보면 찍히는 것이 다른 것을 확인할 수 있다.\n고가용성이라 할 수 있지만 적재되어 있는 데이터가 달라 stateful 애플리케이션(ex. MySQL)을 운영하기엔 한계가 있다.\n참고 책에서는 이러한 제약사항을 해결하기 위해서는 애플리케이션 단에서 다른 노드의 파드와 데이터를 동기화해서 해결할 수 있다고 한다.\n동기화 방법을 찾아보니 NFS 볼륨(ex. AWS EFS)을 구성하여 HostPath 를 연결하거나, 볼륨간 rsync를 사용하라 나오지만, 성능(로컬SSD가 아님)이 떨어져 해결 방법은 아닌 것 같다.\nCase3. 파드간 데이터 동기화 구성 그렇다면 성능 좋고, 고가용성도 보장되고, 데이터 동기화를 보장할 수 있는 Stateful 애플리케이션을 구성할 수 있을까?\n쿠버네티스 공식문서 예제에서 이를 확인할 수 있는데 해당 예제를 구성해보고 테스트해보겠다. 해당 예제에서는 데이터베이스 리소스로 StatefulSet를 사용한다.\nStatefulSet 리소스는 이름처럼 Stateful한 애플리케이션을 위해 만든 리소스이다. StatefulSet 리소스의 특징은 다음과 같다.\nStatefulSet 리소스의 특징\nPod 이름\nStatefulSet에 의해 생성된 파드들은 {Pod 이름}-{순번} 식으로 이름이 정해진다. 이는 클러스터 내부 환경에서 데이터베이스에 접근할 때 사용하기 위해서이다.\n파드 순차적 배포\nPod 생성시 모든 Pod 가 동시에 생성되지 않고 순서대로 하나씩 생성된다. 이는 데이터베이스에서 마스터 파드 → 슬레이브 파드로 기동해야 하는 조건등에서 유용하게 사용 될 수 있다.\n파드별 볼륨 마운트\n일반적으로 PVC \u0026amp; PV에 중복적으로 Pod를 사용할 수 없다. 연결된 Pod가 존재하면 그 다음 파드들은 PVC를 얻지 못해 볼륨을 사용하지 못한다. 반면, Statefulset에서 PVC를 템플릿 형태로 정의하여 Pod마다 PVC, PV를 생성하여 파드별로 볼륨을 마운트할 수 있게 된다.\n다시 돌아가서 쿠버네티스 공식문서의 예제는 클러스터에 MySQL 스테이트풀셋이 배포되고 각 레플리카에 순서대로 배포되는 예제이다. 중요하게 볼 점은 스테이트 풀셋의 매니페스트이다.\n해당 StatefulSet 매니페스트는 3개의 replica를 가진 MySQL을 생성한다. init 컨테이너는 두개 배포되며, init-container는 MySQL init 설정을 수행하고, xtrabackup init-container는 MySQL 클러스터 복제를 위해 데이터를 클론하여 동기화를 진행한다. init 컨테이너 이후 MySQL 컨테이너는 데이터베이스 작업을 수행하며, xtrabackup 컨테이너는 클론 작업을 진행한다.\n공식 문서의 예제를 그대로 배포하면 스토리지 클래스가 default로 EBS 볼륨(외부)에 연결된다. 이대로 진행하면 local-path-provisioner에서 배포한 로컬 볼륨에서 마운트되지 않는다.\n로컬 볼륨에 마운트하기 위해서는 추가 작업이 필요하다.\nlocal-path-provisioner 배포 파일 수정\nlocal-path-provisioner 파드를 워크 노드에만 배포하도록 설정한다. 해당 설정을 통해 워크 노드에만 로컬 볼륨을 생성하고 파드에 연결할 수 있도록 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim local-path-storage.yaml ---------------------------- apiVersion: apps/v1 kind: Deployment metadata: name: local-path-provisioner namespace: local-path-storage spec: replicas: 1 selector: matchLabels: app: local-path-provisioner template: metadata: labels: app: local-path-provisioner spec: spec: # 아래 부분 추가 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/node operator: Exists 스테이트 풀셋의 매니페스트 내 스토리지클래스 지정 1 2 3 4 5 6 7 8 9 volumeClaimTemplates: - metadata: name: data spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: local-path # 로컬 볼륨 추가 resources: requests: storage: 10Gi 스테이트 풀셋의 매니페스트 replica count 를 워크노드(2) 개수 만큼 수정 1 2 3 serviceName: mysql replicas: 2 # 수정 template: 배포 확인\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl apply -f ./ ---------------------------- configmap/mysql created service/mysql created service/mysql-read created statefulset.apps/mysql created kubectl get pods ---------------------------- NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 31m mysql-1 2/2 Running 0 31m 동기화 테스트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # 파드 0에서 Mysql Data 생성 kubectl exec -it pod/mysql-0 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, init-mysql (init), clone-mysql (init) --------------------------------- bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 140 Server version: 5.7.41-log MySQL Community Server (GPL) Copyright (c) 2000, 2023, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. --------------------------------- mysql\u0026gt; create database testdb; --------------------------------- Query OK, 1 row affected (0.01 sec) --------------------------------- mysql\u0026gt; use testdb; --------------------------------- Database changed --------------------------------- mysql\u0026gt; create table test(name varchar(10), testdata varchar(50)); --------------------------------- Query OK, 0 rows affected (0.02 sec) --------------------------------- mysql\u0026gt; insert into test values(\u0026#39;han\u0026#39;, \u0026#39;mysql example test\u0026#39;); --------------------------------- Query OK, 1 row affected (0.01 sec) --------------------------------- mysql\u0026gt; select * from test; --------------------------------- +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.00 sec) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 파드 1에서 확인 kubectl exec -it pod/mysql-1 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, init-mysql (init), clone-mysql (init) --------------------------------- bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 306 Server version: 5.7.41 MySQL Community Server (GPL) Copyright (c) 2000, 2023, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. --------------------------------- mysql\u0026gt; use testdb --------------------------------- Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed --------------------------------- mysql\u0026gt; select * from test; --------------------------------- +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.00 sec) 됐다! 로컬 볼륨 기반으로 Mysql 예제를 배포하였고 고가용성 구성을 위해 동기화까지 진행을 완료했다!\n로컬 볼륨 백업과 복원 쿠버네티스에서는 HostPath 볼륨의 대한 백업과 복원 기능은 지원하지 않는다. 노드 별로 백업 스크립트를 작성하거나 써드파티 솔루션을 사용해야 한다. 이번 절에서는 local-path-provisioner 을 사용해서 볼륨을 프로비저닝한만큼, 해당 볼륨에 맞게 백업을 할 수 있는 써드파트 솔루션을 찾아보았다.\nlocal-path-provisioner 깃허브 이슈를 찾아보니 Velero 솔루션을 이용해서 백업을 할 수 있다고 하여 테스트를 진행해보고자 한다.\nVelero? 는 쿠버네티스 클러스터의 리소스와 퍼시스턴트 볼륨을 백업하고 복원하는 데 사용되는 오픈 소스 툴이다.\nVelero 을 사용하기전 local-path-provisioner 볼륨 타입을 Local로 수정해야 한다. Hostpath 볼륨을 지원하지 않으나 Local 볼륨은 Restic 과 연계하여 백업을 지원하기 때문이다. (공식문서)\n이는 Local 볼륨이 쿠버네티스의 자원으로 관리되며, 스토리지 클래스와 퍼시스턴트 볼륨 클레임(PVC)을 사용할 수 있기 때문으로 보인다.\nlocal-path-provisioner 사전 작업\nLocal 볼륨을 수정하기 위해서는 local-path-provisioner 파드의 수정 작업이 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 volumeClaimTemplates: - metadata: name: data annotations: volumeType: local # 추가 spec: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] storageClassName: local-path resources: requests: storage: 10Gi 로컬 볼륨 확인\n1 2 3 4 5 6 kubectl get pv \u0026lt;pv-name\u0026gt; -o yaml --------------------------------- spec: local: # local or HostPath 볼륨 path: /mnt/local-storage/ssd/vol1 ... Velero 설치\n필자는 Velero 백업 버킷을 AWS S3 설정하여 설치를 진행하였다.\n설치는 S3 버켓 생성 및 설정 / Veleo CLI 로 나뉜다.\nS3 버켓 지정 및 IAM 설정\nVelero 에서 S3 버킷을 접근하기 위한 IAM USER ID와 KEY 생성\n1 **aws s3 mb s3://\u0026lt;bucket-name\u0026gt; --region ap-northeast-2** IAM Policy 생성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 # 버킷 변수 설정 export BUCKET=**\u0026lt;bucket-name\u0026gt;** # IAM Policy 생성 cat \u0026gt; velero-policy.json \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:DescribeSnapshots\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateVolume\u0026#34;, \u0026#34;ec2:CreateSnapshot\u0026#34;, \u0026#34;ec2:DeleteSnapshot\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${BUCKET}/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${BUCKET}\u0026#34; ] } ] } EOF # IAM Policy Attach aws iam put-user-policy \\ --user-name velero \\ --policy-name velero \\ --policy-document file://velero-policy.json # IAM user 정보 가져오기 aws iam create-access-key --user-name velero --------------------------------- { \u0026#34;AccessKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;velero\u0026#34;, \u0026#34;AccessKeyId\u0026#34;: \u0026#34;{ID}\u0026#34;, # 밑의 credentials-velero ID에 저장 \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;{KEY}\u0026#34;, # 밑의 credentials-velero KEY에 저장 \u0026#34;CreateDate\u0026#34;: \u0026#34;2023-03-16T04:31:23+00:00\u0026#34; } } # credentials-velero 생성 및 IAM 정보 저장 cat \u0026lt;\u0026lt; EOF \u0026gt; credentials-velero [default] aws_access_key_id=\u0026lt;AWS_ACCESS_KEY_ID\u0026gt; aws_secret_access_key=\u0026lt;AWS_SECRET_ACCESS_KEY\u0026gt; EOF Velero CLI 설치 후 서버 설치\nrestic은 버전 velero 1.10(최신버전) 이상에서 더 이상 지원되지 않는다. 버전을 1.9.6으로 맞춰서 다운받아야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # arch 확인 uname -m --------------------------------- x86_64 # velero CLI 설치 wget https://github.com/vmware-tanzu/velero/releases/download/v1.9.6/velero-v1.9.6-linux-amd64.tar.gz tar xzvf velero-v1.9.6-linux-amd64.tar.gz cp velero-v1.9.6-linux-amd64/velero ~/bin # CLI 확인 velero --------------------------------- Velero is a tool for managing disaster recovery, specifically for Kubernetes cluster resources. It provides a simple, configurable, and operationally robust way to back up your application state and associated data. If you\u0026#39;re familiar with kubectl, Velero supports a similar model, allowing you to execute commands such as \u0026#39;velero get backup\u0026#39; and \u0026#39;velero create schedule\u0026#39;. The same operations can also be performed as \u0026#39;velero backup get\u0026#39; and \u0026#39;velero schedule create\u0026#39;. ... Velero 설치\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 export BUCKET=**\u0026lt;bucket-name\u0026gt;** export REGION=**ap-northeast-2** velero install \\ --provider aws \\ --bucket $BUCKET \\ --secret-file ./credentials-velero \\ --backup-location-config region=$REGION \\ --use-volume-snapshots=false \\ --plugins velero/velero-plugin-for-aws:v1.3.0 \\ --use-restic --------------------------------- ... Deployment/velero: created DaemonSet/restic: attempting to create resource DaemonSet/restic: attempting to create resource client DaemonSet/restic: created Velero is installed! ⛵ Use \u0026#39;kubectl logs deployment/velero -n velero\u0026#39; to view the status. # Velero 확인 kubectl get all -n velero NAME READY STATUS RESTARTS AGE pod/restic-f5ngz 1/1 Running 0 38s pod/restic-x9sk9 1/1 Running 0 37s pod/velero-5f6657d4c8-jttxv 1/1 Running 0 38s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/restic 2 2 2 2 2 \u0026lt;none\u0026gt; 38s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/velero 1/1 1 1 38s NAME DESIRED CURRENT READY AGE replicaset.apps/velero-5f6657d4c8 1 1 1 38s Velero 백업\nVelero는 Restic을 사용하여 PV 볼륨에 대해 백업하는 방법에 두 가지 접근 방식을 지원한다. (공식문서)\n옵트인 접근 방식(default): Restic을 사용하여 백업할 볼륨이 포함된 모든 포드에 annotation을 달아야 한다. 옵트아웃 접근 방식: 모든 포드 볼륨이 Restic을 사용하여 백업되고 백업되지 않아야 하는 볼륨을 옵트아웃할 수 있는 기능이 있다. 이번 절에서는 옵트인 접근 방식을 택할 것이고 Case3 의 Mysql 볼륨을 백업할 예정이다. 백업할 볼륨에 대해 backup.velero.io/backup-volumesannotation 달고 백업을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 주석 추가 pod에 볼륨 정보를 추가 kubectl annotate pod/mysql-0 backup.velero.io/backup-volumes=data # 백업 velero backup create mysql --include-namespaces default --wait --------------------------------- Backup request \u0026#34;mysql\u0026#34; submitted successfully. Waiting for backup to complete. You may safely press ctrl-c to stop waiting - your backup will continue in the background. .................. Backup completed with status: Completed. You may check for more information using the commands `velero backup describe mysql` and `velero backup logs mysql`. # 백업 목록 확인 velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-16 14:22:39 +0900 KST 29d default \u0026lt;none\u0026gt; S3 버킷 조회할 수 있다.\nS3 조회시, backups(쿠버네티스 리소스 저장 경로) 와 restic(PV 볼륨 데이터 저장 경로)에 데이터를 확인할 수 있다.\nVelero 복원\nmysql 배포 파일과 PV를 지우고 복원을 진행하겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #mysql 지우기 kubectl delete -f ./ kubectl delete pvc/\u0026lt;PVC 볼륨\u0026gt; #velero 복원 velero restore create --from-backup mysql --wait --------------------------------- Restore request \u0026#34;mysql-20230316155542\u0026#34; submitted successfully. Waiting for restore to complete. You may safely press ctrl-c to stop waiting - your restore will continue in the background. ........... Restore completed with status: Completed. You may check for more information using the commands `velero restore describe mysql-20230316155542` and `velero restore logs mysql-20230316155542`. # 쿠버네티스 리소스 복원 확인 kubectl get all --------------------------------- NAME READY STATUS RESTARTS AGE pod/mysql-0 2/2 Running 0 39s pod/mysql-1 0/2 Init:CrashLoopBackOff 2 (18s ago) 39s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 4h39m service/mysql ClusterIP None \u0026lt;none\u0026gt; 3306/TCP 39s service/mysql-read ClusterIP 100.69.52.194 \u0026lt;none\u0026gt; 3306/TCP 39s kubectl get pv --------------------------------- NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-601b919a-cf20-4478-9f28-10d541c66844 10Gi RWO Delete Bound default/data-mysql-0 local-path 71s pvc-b8a766a6-411f-47df-a548-d6b0ee091ea1 10Gi RWO Delete Bound default/data-mysql-1 local-path 70s # Mysql data 확인 kubectl exec -it pod/mysql-0 -- /bin/bash --------------------------------- Defaulted container \u0026#34;mysql\u0026#34; out of: mysql, xtrabackup, restic-wait (init), init-mysql (init), clone-mysql (init) bash-4.2# mysql -u root -p --------------------------------- Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 73 Server version: 5.7.41-log MySQL Community Server (GPL) mysql\u0026gt; use testdb; --------------------------------- Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed --------------------------------- mysql\u0026gt; select * from test; +------+--------------------+ | name | testdata | +------+--------------------+ | han | mysql example test | +------+--------------------+ 1 row in set (0.01 sec) 데이터가 그대로 보존되어 있다!\nVelero 스케쥴 백업\n크론탭처럼 백업도 Velero가 가능하다. 다음은 5분마다 백업을 진행하는 예제이다.\n백업된 오브젝트별 데이터 변화를 위해 데이터베이스의 데이터를 수정 후 복원을 해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 velero schedule create mysql-crontab --include-namespaces default --schedule=\u0026#34;*/5 * * * *\u0026#34; ****velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-16 15:51:44 +0900 KST 29d default \u0026lt;none\u0026gt; mysql-crontab-20230316072517 Completed 0 0 2023-03-16 16:25:17 +0900 KST 29d default \u0026lt;none\u0026gt; mysql-crontab-20230316072017 Completed 0 0 2023-03-16 16:20:17 +0900 KST 29d default \u0026lt;none\u0026gt; #DB 접속 후 데이터 삭제 select * from test; +------+---------------------+ | name | testdata | +------+---------------------+ | han | mysql example test | | han | mysql example test2 | | han | mysql example test3 | +------+---------------------+ 3 rows in set (0.00 sec) mysql\u0026gt; delete from test; Query OK, 3 rows affected (0.00 sec) mysql\u0026gt; select * from test; Empty set (0.00 sec) #velero 복원 velero restore create --from-backup mysql-crontab-20230316072017 --wait --------------------------------- Restore request \u0026#34;mysql-crontab-20230316072017-20230316163030\u0026#34; submitted successfully. Waiting for restore to complete. You may safely press ctrl-c to stop waiting - your restore will continue in the background. Restore completed with status: Completed. You may check for more information using the commands `velero restore describe mysql-crontab-20230316072017-20230316163030` and `velero restore logs mysql-crontab-20230316072017-20230316163030`. # Mysql DATA 확인 mysql\u0026gt; select * from test; Empty set (0.00 sec) Mysql DATA 확인 의 결과가 예상과 다르다. 백업 진행 이후 백업 데이터인 행3개가 있어야 하나, 최신 데이터가 조회된다.\n이는 Mysql 파드가 2개 있어 파드간 데이터 무결성이 보장되어 백업 데이터 파일을 옮긴다 한들 수정이 안되기 때문이다. 따라서 백업본의 결과를 얻기 위해서는 PV 볼륨과 Mysql 리소스를 지우고 다시 복원을 해야한다.\nVelero 클러스터 마이그레이션\n클러스터간 마이그레이션 방법으로 Velero를 활용할 수 있다. 공식문서를 참고하여 테스트를 진행해보겠다.\n진행 전 Velero 에서 클러스터 마이그레이션시 제약사항이 있으니 확인이 필요하다.\nVelero는 기본적으로 클라우드 공급자 간에 PV 스냅샷의 마이그레이션을 지원하지 않는다. 이를 위한 방안으로 restic을 이용하여 파일시스템 레벨의 마이그레이션을 진행해야 한다. Velero는 백업이 수행된 위치보다 낮은 Kubernetes 버전이 있는 클러스터로의 복원을 지원하지 않는다. 동일한 버전의 Kubernetes를 실행하지 않는 클러스터 간에 워크로드를 마이그레이션하는 것이 가능할 수 있지만 마이그레이션 전에 각 사용자 정의 리소스에 대한 클러스터 간 API 그룹의 호환성을 포함하여 몇 가지 요인을 고려해야 한다. AWS 및 Azure용 Velero 플러그인은 리전 간 데이터 마이그레이션을 지원하지 않는다. 이를 위한 방안으로 restic을 사용해야 한다. 우리는 restic을 사용하니 제약사항에 자유롭다. 클러스터 마이그레이션의 예에 대한 일환으로 클러스터를 재 구축하여 앞서 생성한 Mysql 애플리케이션을 복원해보겠다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # velero 설치 export BUCKET=**hanhorang-velero-s3** export REGION=**ap-northeast-2** velero install \\ --provider aws \\ --bucket $BUCKET \\ --secret-file ./credentials-velero \\ --backup-location-config region=$REGION \\ --use-volume-snapshots=false \\ --plugins velero/velero-plugin-for-aws:v1.3.0 \\ --use-restic --------------------------------- velero get backup --------------------------------- NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR mysql Completed 0 0 2023-03-17 00:31:21 +0900 KST 29d default \u0026lt;none\u0026gt; velero 백업 개체가 그대로 있다. 복원 과정은 앞 과정과 동일하기에 생략하였다. 클러스터를 재구축해도 Velero 백업 객체가 남아있는 이유가 무엇일까?\nVelero 리소스는 오브젝트 스토리지의 백업 파일과 동기화되기 때문이다. 설치 과정에서 삭제한 클러스터와 새로운 클러스터의 Velero 버킷이 동일하므로 백업 객체가 그대로 있음을 확인하였다. 클러스터1과 클러스터2가 병행 운영시에도 버킷 데이터에 따라 Velero 리소스가 동기화가 이루어진다는데 기본 동기화 간격이 1분으로 이 부분을 확인하여 마이그레이션을 진행하면 될 것 같다.\n로컬 볼륨 모니터링 PV 볼륨 성능 확인할 수 있는 krew df-pv 도구가 있으나, HostPath 볼륨은 인스토어스토어라서 확인되지 않는다. 하지만 Local 볼륨은 확인이 가능하다.\n1 2 3 4 5 kubectl krew df-pv \u0026amp;\u0026amp; kubectl df-pv --------------------------------- PV NAME PVC NAME NAMESPACE NODE NAME POD NAME VOLUME MOUNT NAME SIZE USED AVAILABLE %USED IUSED IFREE %IUSED pvc-678e7407-9f76-4fd1-a9ad-8c2581b8df36 data-mysql-0 default i-0314088c74eee3276 mysql-0 data 123Gi 4Gi 119Gi 3.68 119172 16395900 0.72 pvc-d2c3cbcb-13ec-46de-81e9-1178d25dd4ad data-mysql-1 default i-0ab66ac834dc8710d mysql-1 data 123Gi 4Gi 119Gi 3.74 121203 16393869 0.73 성능 측정 로컬 스토리지의 성능 측정 방법으로 iostat 명령어와 krew 툴인 kubestr을 사용하여 성능을 측정하겠다.\nkubestr 스토리지 IOPS 측정 툴이다. 스토리지 사용에 따른 검증용으로 사용하기 좋은 툴인 것 같다. 예제도 많으니 링크를 통해 확인하자. 이번 예제에서는 실습 참고용 책에서 제공해주신 예제 스크립트를 통해 성능을 측정할 것이다.\nfio-read.fio\nfio를 사용하여 4KB 블록 크기를 가지는 랜덤 읽기 및 쓰기\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [global] ioengine=libaio direct=1 bs=4k runtime=120 time_based=1 iodepth=16 numjobs=4 # numjobs=16 size=1g group_reporting rw=randrw rwmixread=100 rwmixwrite=0 [read] ioengine=libaio : Asynchronous I/O를 수행하기 위해 libaio 라이브러리를 사용합니다. direct=1 : Direct I/O를 사용합니다. bs=4k : I/O 요청에 사용되는 블록 크기는 4KB입니다. runtime=120 : 120초 동안 작업을 실행합니다. time_based=1 : 시간 기반으로 작업을 수행합니다. iodepth=16 : 각 작업에 대한 I/O 요청 수를 16개로 설정합니다. numjobs=4 : 4개의 작업을 수행합니다. size=1g : 각 작업에 대한 데이터 크기는 1GB입니다. group_reporting : 모든 작업 결과를 통합하여 보고합니다. rw=randrw : 랜덤 읽기 및 쓰기 작업을 수행합니다. rwmixread=100 : 작업 중 읽기 작업의 비율은 100%입니다. rwmixwrite=0 : 작업 중 쓰기 작업의 비율은 0%입니다. fio-write.fio\n루트 디렉토리에서 4KB 블록 크기로 16개의 job이 16개의 i/o depth로 실행되며, 실행 시간이 120초인 1GB 파일에 대해 100% 쓰기 랜덤 테스트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [global] ioengine=libaio numjobs=16 iodepth=16 direct=1 bs=4k runtime=120 time_based=1 size=1g group_reporting rw=randrw rwmixread=0 rwmixwrite=100 directory=/ [read] rwmixwrite=100: 100% 쓰기 테스트를 수행합니다. directory=/: 테스트할 디렉토리를 루트 디렉토리로 설정합니다. 성능 측정\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubestr fio -f fio-write.fio -s local-path --size 10G --------------------------------- PVC created kubestr-fio-pvc-hp69m Pod created kubestr-fio-pod-fcvcp Running FIO test (fio-write.fio) on StorageClass (local-path) with a PVC of Size (10G) Elapsed time- 4m11.664545514s FIO test results: FIO version - fio-3.30 Global options - ioengine=libaio verify= direct=1 gtod_reduce= JobName: blocksize= filesize= iodepth= rw= write: IOPS=3023.577881 BW(KiB/s)=12094 iops: min=992 max=8640 avg=3023.464355 bw(KiB/s): min=3968 max=34564 avg=12093.908203 Disk stats (read/write): nvme0n1: ios=0/362587 merge=0/173 ticks=0/6330627 in_queue=6330627, util=99.954132% fio-write 실행 결과, 쓰기 평균 iops가 3034인 것을 확인할 수 있다.\nQ. 테스트가 안될 경우, PV 상태 Pending?\n해당 경우는 PVC 요청에 맞는 볼륨의 PV가 없을때 발생한다. PVC 요청에 맞는 볼륨이 있는지 또는 Local-path-provisioner 설정을 확인하자. 필자의 경우 Local-path-provisioner 설정으로 Pending 이 발생했다.\n1 2 3 4 kubectl get pvc -A --------------------------------- NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default kubestr-fio-pvc-dl7vx Pending local-path 4m34 노드 볼륨 IO 성능 측정 iostat 명령어를 통해 실시간으로 스토리지의 성능과 사용량에 관한 정보를 확인할 수 있다. 클러스터 운영시 스토리지 트러블슈팅의 일환으로 사용하자.\n아래 예제는 fio-write 스크립트 실행 중 iostat 를 실행하여 스토리지 정보를 확인한 예제이다.\n1 2 3 4 5 6 7 # iostat 패키지 설치 sudo apt install -y **sysstat iostat -xmdz 1 -p nvme2n1** --------------------------------- **Device r/s rMB/s rrqm/s %rrqm r_await rareq-sz w/s wMB/s wrqm/s %wrqm w_await wareq-sz d/s dMB/s drqm/s %drqm d_await dareq-sz aqu-sz %util nvme2n1 0.00 0.00 0.00 0.00 0.00 0.00 24.00 0.10 2.00 7.69 0.75 4.33 0.00 0.00 0.00 0.00 0.00 0.00 0.02 4.00** w/s: 초당 쓰기 요청 수 wMB/s: 초당 쓴 데이터 양 (메가바이트/초) wrqm/s: 초당 쓰기 요청 큐에 들어간 요청 수 %wrqm: 쓰기 요청 큐에 들어간 요청 비율 w_await: 쓰기 요청 대기 시간, 드라이버 요청 대기열에서 기다린 시간과 장치의 I/O 응답시간을 모두 포함한다. (밀리초) wareq-sz: 평균 쓰기 요청 크기 (섹터) aqu-sz: 요청 대기열의 평균 길이 %util: 디스크 사용률 (0 ~ 100%) 로컬 스토리지 QnA Q1. AWS 인스토어 스토어에 대한 볼륨 스토리지 조절이 가능한가?\n인스턴스 스토어는 EC2 인스턴스의 로컬 디스크를 사용하는 것이기 때문에 크기 조정이 불가능하다. 인스턴스 스토어를 사용하는 EC2 인스턴스를 변경하거나 새로운 인스턴스를 시작하여 크기를 조정해야 한다. 일반적으로 DB(Mysql) 볼륨 사용량으로 10G~100G을 설정한다고 하지만, 애플리케이션 규모와 기간에 따라 사용량 예측이 힘들다. 필요시 백업을 통해 인스토어 스토어 볼륨을 변경할 수 있도록 하자. EC2 인스턴스에 따른 볼륨(SSD) 는 링크에서 확인이 가능하다.\nQ2. hostpath 볼륨을 여러개의 파드가 동시에 사용할 수 있을까?\n노드의 파일 시스템은 여러 개의 프로세스가 동시에 접근할 수 있는 공유 리소스이기 때문에 여러 개의 파드가 하나의 hostpath를 사용할 수 있다. 하지만, 데이터 손상이나 권한 오류가 발생할 수 있다. 하나의 파드가 파일을 쓴 후에 다른 파드가 동일한 파일을 읽을 경우나, 여러 개의 파드가 동시에 동일한 파일을 쓰는 경우가 있기 때문이다. 이를 위해 적절한 동기화 및 락 메커니즘을 구현이 필요하다.\n테스트로 스터디에서 공유해주신 localpath-fail.yaml 를 수정해서 로그를 확인해봤다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: apps/v1 kind: Deployment metadata: name: date-pod labels: app: date spec: replicas: 1 selector: matchLabels: app: date template: metadata: labels: app: date spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 0.01; done\u0026#34;] # 0.01 로 수정 volumeMounts: - name: pod-persistent-volume mountPath: /data volumes: - name: pod-persistent-volume persistentVolumeClaim: claimName: localpath-claim 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 파드 10개로 증가후 테스트 kubectl scale deployment date-pod --replicas=10 -------------------------------------- # 로그 확인 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:05 | wc -l 355 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:05 | wc -l 355 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:06 | wc -l 457 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:07 | wc -l 511 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:08 | wc -l 513 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:09 | wc -l 530 ubuntu@i-0993bfc0c3f4818c8:/data/local-path/pvc-9676527c-e860-4c13-acc3-cda3bd1a5f1d_default_localpath-claim$ cat out.txt | grep 03:10 | wc -l 506 0.01초 x 10 개 파드로 1초당 약 1000개의 결과가 나와야 하지만 350~530 개의 결과가 나오는 것을 확인하였다. 앞서 kubestr 측정에서 쓰기 IOPS를 측정하여 약 3000이 나왔음에도 턱없이 부족한 것을 알 수 있다.\nQ3. Velero 의 백업 용량은 몇 인가?\n사용 버킷에 따라 달라진다. S3 버킷 기준으로는 해당 버킷의 용량에 따라가는데 최대 5테라까지 지원이 가능하다.\n","date":"Mar 18","permalink":"https://HanHoRang31.github.io/post/pkos2-2-localstorage/","tags":["KANS","kops","cloud","AWS","kubernetes","Volume","velero","local-path-provisioner"],"title":"PKOS 2기 2주차 - 로컬스토리지 "},{"categories":null,"contents":" 1 2 Production Kubernetes Online Study (=PKOS)는 쿠버네티스 실무 실습 스터디입니다. CloudNet@ Gasida(가시다)이 진행하며, 책 \u0026#34;24단계 실습으로 정복하는 쿠버네티스\u0026#34;을 기반으로 진행하고 있습니다. PKOS 1주차 스터디 내용과 느낌점을 정리하고자 한다. 스터디하면서 매번 느끼는 거지만 정말 괴수분들 너무 많고, 배울 점이 정말 많다… 특히 모임장님이신 가시다님의 스터디 내용은 볼 때마다 감탄만 나온다. 쿠버네티스에 대한 원리와 세부 컴포넌트에 대한 명령어까지 배운 점이 많다. 본 글에서는 필자가 배운 내용을 정리함과 동시에 개념에 대해 몰랐던 부분이나, 트러블슈팅에 대한 내용을 중점적으로 정리하였다.\n추가로, 몰랐던 부분은 ChatGPT를 활용하여 작성하였다. ChatGPT가 주는 답변은 대체로 만족하지만, 공식 문서에 대한 내용과 비교하여 다른 내용이 일부 존재한다. ChatGPT 활용시, 공식 문서와 이중 검증이 필요하다고 본다. 본 글에서도 답변 내용을 일부 수정하여 작성한다.\nkops? kops는 쿠버네티스 클러스터를 생성, 업그레이드, 관리하는 데 사용되는 오픈 소스 도구이다. 명령어 툴로 쉽게 쿠버네티스 클러스터를 구성하고 관리할 수 있는 툴이라고 이해하면 되겠다. 쿠버네티스 클러스터의 인프라를 코드로 정의하고 관리하는데 사용할 수 있어 IaC 이며, 같은 IaC 툴인 Terraform(테라폼)과의 특징을 비교하면 다음과 같다.\nkops과 Terraform 특징 비교 표\n특징 kops Terraform 지원하는 클라우드 플랫폼(Provider) AWS, GCP, OpenStack, DigitalOcean 등 AWS, GCP, Azure, Oracle Cloud, Alibaba Cloud, VMware, OpenStack 등 관리 대상 Kubernetes 클러스터 인프라스트럭처 (서버, 네트워크, 데이터베이스 등) 코드 작성 방식 YAML 파일 HCL (HashiCorp Configuration Language) 상태 관리 상태를 지정된 저장소(state)에 저장 상태를 지정된 저장소(backend)에 저장 장점 간단하고 직관적인 클러스터 구성 AWS뿐만 아니라 다른 클라우드 프로바이더도 지원 단점 Kubernetes 클러스터만 지원 Kops에 비해 배우기가 어려움 그렇다면 kops는 언제 써야 할까?\n모임장님 의견과 동일하게 교육용이 적합하다고 본다. 쿠버네티스 클러스터 구축이 간단하고 배우기 쉽다. 또한, 배포 속도도 빠르다. 쿠버네티스 관리형 서비스인 EKS 와 비교했을 때, 마스터 노드들을 세부적으로 알 수 있어 세부 원리 이해에 좋고, 비용도 저렴하다. kops 공식문서도 정리가 잘 되어 있다. 눈여겨 볼 점은 공식문서의 addon과 Operation 부분이다. 클러스터 관리를 및 addon 배포를 간단하게 테스트할 수 있고 배포 yaml를 확인할 수 있기 때문이다.\n실습 스터디 내용으로 kops를 통해 AWS에 쿠버네티스 클러스터를 구축하고, 게임 마리오를 예제로 배포하였다. 과정은 크게 3가지로 진행하였다.\n베스천 서버(kops-ec2) 구성 kops 를 통한 클러스터 구축 및 확인 External DNS 와 게임, 슈퍼마리오 배포 실습 과정 전 사전 작업으로 퍼블릭 도메인 구입, 키 페어 생성, S3 버킷 생성, AWS IAM 자격 증명을 진행하였다. 사전 작업 내용은 공식 문서에서 참고가 가능하다.\nQ. 퍼블릭 도메인 구입 이유 ?\n퍼블릭 도메인은 쿠버네티스 클러스터 이름으로 사용하기 위하여 구입하였다. 클러스터 이름을 도메인으로 설정하면 외부에서 서비스 디스커버리 및 클러스터 액세스가 쉽게 가능해지기 때문이다. 스터디에서는 해당 도메인을 통해 클러스터 구성 확인과 게임 배포 후 접근을 위해 사용하였다.\n1. 베스천 서버(kops-ec2) 구성 베스천 서버 구성은 모임장님이 공유해주신 CloudFormation 템플릿을 통해 구성하였다. 템플릿 구성은 VPC 와 igw 구성같은 AWS 네트워크 구성과 EC2 서버 설정으로 되어있다. 그 중 EC2 서버 설정 스크립트를 확인할 필요가 있는데 kops와 필요 패키지를 같이 설치해주기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash # 호스트 이름 변경 hostnamectl --static set-hostname kops-ec2 # EC2 서버 시간을 서울로 변경 ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime # Install Packages cd /root yum -y install tree jq git htop ## kubectl 설치 curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl ## kops 설치 curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d \u0026#39;\u0026#34;\u0026#39; -f 4)/kops-linux-amd64 chmod +x kops mv kops /usr/local/bin/kops ## awscli 설치 curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install ## 환경 변수 설정 export PATH=/usr/local/bin:$PATH source ~/.bash_profile ## aws cli 자동 완성 설정 complete -C \u0026#39;/usr/local/bin/aws_completer\u0026#39; aws ## SSH 설정 ssh-keygen -t rsa -N \u0026#34;\u0026#34; -f /root/.ssh/id_rsa ## vi -\u0026gt; vim 으로 설정 echo \u0026#39;alias vi=vim\u0026#39; \u0026gt;\u0026gt; /etc/profile ## root 계정 변환 echo \u0026#39;sudo su -\u0026#39; \u0026gt;\u0026gt; /home/ec2-user/.bashrc ## helm 설치 curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash wget https://github.com/andreazorzetto/yh/releases/download/v0.4.0/yh-linux-amd64.zip unzip yh-linux-amd64.zip mv yh /usr/local/bin/ 베스천 서버 구성 과정 중 에러 발생 시, AWS Cloudformation에서 로그를 확인할 수 있다. 필자는 VPC가 최대여서 베스천 서버 구성이 안되었다. 해결을 위해 필요없는 VPC를 삭제하고, 템플릿으로 생성한 Cloudformtation 스택을 삭제하고 재실행하였다.\n2. kops를 통한 클러스터 구축 및 확인 쿠버네티스 클러스터 구축은 kops 명령어로 구축하였다. 클러스터 생성에 대한 옵션은 공식 문서에서 참고할 수 있었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 클러스터 생성 bash ## export KOPS_STATE_STORE=s3://( 클러스터 관리 저장소로 활용할 버킷 이름) ## export KOPS_CLUSTER_NAME=\u0026lt;도메인 메인 주소\u0026gt; ## export REGION=ap-northeast-2 지역 kops create cluster \\ --cloud aws \\ --name $KOPS_CLUSTER_NAME \\ --state s3://$KOPS_STATE_STORE \\ --zones \u0026#34;$REGION\u0026#34;a,\u0026#34;$REGION\u0026#34;c \\ --networking amazonvpc \\ --network-cidr 172.30.0.0/16 \\ --kubernetes-version \u0026#34;1.24.10\u0026#34; \\ --node-count 2 \\ --node-size t3.medium \\ --master-size t3.medium \\ --ssh-public-key ~/.ssh/id_rsa.pub \\ -y --state : 클러스터 관련 설정 파일들이 저장된다. 이렇게 저장된 설정 파일들은 나중에 클러스터를 업데이트하거나 삭제할 때 사용된다. 실습에서는 외부 스토리지인 S3를 이용하였다. —state 플래그를 사용하지 않으면 로컬 저장소인 ~/.kops 에 클러스터 설정 파일이 저장된다. 하지만 로컬 시스템에 대한 의존성이 높아지고 협업시에 대한 공유성이 떨어져 외부 스토리지를 사용하는 것을 권장한다. 필자는 클러스터 생성시 VPC 개수 이유(error creating VPC: VpcLimitExceeded) 로 클러스터 생성이 되지 않았다.\n재설치를 위해서는 기존 클러스터 삭제가 필요하다. 실습 내용의 구성 단계에 따라 삭제하면 된다.\n1 2 3 4 5 1. EC2 Auto Scaling 그룹 삭제 2. EC2 시작 템플릿 Launch Templates 삭제 3. S3 버킷 비우기 4. Route53에 추가된 A 레코드 3개 삭제 5. CloudFormation 삭제 VPC 문제는 S3 버킷만 지우면 됐었다. Cloudformtation 스택 생성시 에러가 발생하여 AWS 리소스들은 생성되지 않았기 때문이다. 아래는 kops 를 통한 클러스터 구성 과정을 정리하였다\n1 2 3 4 5 6 7 8 1. kops create cluster 명령어 실행 2. 클러스터 구성 정보를 S3에 저장 3. 클러스터 구성 정보를 기반으로 CloudFormation 스택 생성 4. VPC 및 관련 리소스 생성 5. 마스터 노드 EC2 인스턴스 생성 6. 노드 그룹 EC2 인스턴스 생성 7. 노드 그룹 EC2 인스턴스가 마스터 노드를 참조하여 클러스터에 가입 8. 클러스터가 실행되고 kubectl을 통해 액세스 가능해짐 클러스터 구성 확인 확인\n스터디 실습 내용으로 클러스터 구성 확인에 대한 것도 시간을 할당하여 확인하였다. 쿠버네티스 클러스터가 복잡한 만큼 확인할 것이 많았는데, 실습 내용을 참고로 하여 명령어를 정리해보았다.\n클러스터 도메인 확인\n클러스터 구성시 클러스터 이름을 퍼블릭 도메인으로 입력하였다. 필자는 퍼블릭 도메인을 AWS Route53 에서 구매하였는데, 이 같은 경우 route53에서 A레코드도메인이 추가된 것을 확인할 수 있다.\n클러스터 구성 정보 확인\n클러스터 구성 정보는 kubectl 와 kops 툴로 확인이 가능하다. kops 툴을 통해 클러스터 정보 뿐만 아니라 이미지 확인(assets), 보안 정보 확인이 가능하다. 공식문서를 링크한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 클러스터 확인 kops get cluster # 클러스터 인스턴스 그룹 확인 kops get ig # 클러스터 인스턴스 확인 kops get instances # 클러스터 접근 정보 확인 kubectl get nodes -v6 # 클러스터 배포 파드 확인 kubectl get pods -A # 클러스터 정보 확인 k**ubectl cluster-info dump** 클러스터 세부 구성 확인\n클러스터 세부 구성 확인으로 스토리지, 네트워크, 파드, 마스터 노드 컴포넌트들의 구성 정보를 확인하였다. 추후 참고용을 위해 정리해둔다. 퍼블릭 도메인을 이용하여 구성 정보를 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # storage class 확인 kubectl get sc # 노드별 스토리지 확인 lsblk df -hT # 네트워크 확인 sudo iptables -t nat -S # 마스터 노드 컴포넌트 확인 **tree /etc/kubernetes/manifests/ # kubelet 작동 확인 systemctl status kubelet # 컨테이너 접근 방법 확인 ps axf | grep /usr/bin/containerd # ec2 메타데이터 확인 - IAM role 확인** TOKEN=`curl -s -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34;` echo $TOKEN curl -s -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; –v http://169.254.169.254/latest/meta-data/iam/security-credentials/\u0026lt;도메인\u0026gt; | jq kubectl get sc : kops로 클러스터 구성시 스토리지 CSI 가 기본 제공된다. 스토리지 CSI 는 클러스터 스토리지 관리 기능 플러그인이다. kops-csi 와 kops-ssd 가 존재하는데 kops-csi 는 AWS EBS(gp3), EFS, GCP 등의 스토리지를 사용할 수 있는 플러그인이고, kops-ssd 는 NVMe SSD 기반 인스턴스 스토리지를 사용할 수 있는 플러그인이다.\nsudo iptables -t nat -S : Kubernetes에서 사용하는 iptables rule 을 확인할 수 있다. 이번 장에서 구성한 네트워크 CNI는 VPC로 노드 IP와 서비스 IP의 할당별 iptables rule을 확인할 수 있다.\n파드 IP 설정 iptables rule\n임의의 애플리케이션 파드 IP를 확인했다. 체인을 따라가보면 들어오는 패킷 중 목적지 IP 주소가 100.64.59.121/32이고, 프로토콜이 TCP이며, 목적지 포트가 80인 패킷이 172.30.41.131:8080(파드 할당 IP) 으로 가는 것을 확인 할 수 있다. 노드 IP 설정 iptables rule\n임의의 노트 IP를 확인했다. 노드 IP에 대해NAT 규칙을 적용하는 것을 확인할 수 있었다. tree /etc/kubernetes/manifests/ : 마스터 노드에서 작동하는 컴포넌트 mainfest 정보들이다. kubelet에 의해 Static Pod로 배포되어 있으며 API 서버의 명령어(kubectl apply, delete) 등으로 관리가 불가능하다. kubelet을 통해 아래 컴포넌트의 manifest 를 모니터링하고 있으며 manifest 수정시 자동으로 업데이트된다.\n1 2 3 4 5 6 7 /etc/kubernetes/manifests/ ├── etcd-events-master-ap-northeast-2a.manifest ├── etcd-main-master-ap-northeast-2a.manifest ├── kube-apiserver.manifest ├── kube-controller-manager.manifest ├── kube-proxy.manifest └── kube-scheduler.manifest 3. External DNS, 게임 마리오 배포 실습 예제로는 게임 마리오를 배포하였다. External DNS은 쿠버네티스 addon으로, 내부에서 동작하는 마리오 서비스의 IP 주소를 외부의 DNS 서버에 자동으로 등록시켜주기 위해 배포하였다. 인상 깊은 점은 addon인 ExternalDNS 배포였는데, kops를 통해 클러스터 정보를 수정하니 자동으로 배포되었다.\n공식 문서1, 공식 문서2 에는 클러스터 스펙 수정시의 옵션이 잘 정리되어 있다. 추후 기능 테스트시 접근을 위해 남겨둔다.\n1 2 3 4 5 6 7 8 9 10 11 12 # 클러스터 수정 kops edit cluster # 아래 spec 에 ExternalDNS 정보 추가** -------------------------- spec: externalDns: provider: external-dns -------------------------- # 클러스터 업데이트 후, 롤링 업데이트 진행 kops update cluster --yes \u0026amp;\u0026amp; echo \u0026amp;\u0026amp; sleep 3 \u0026amp;\u0026amp; kops rolling-update cluster 트러블슈팅 API 서버 접근시, 아래의 에러 메세지(couldn’t get current server API group list: ~ dial tcp connect refused )가 확인되었다.\n원인은 클러스터 접근 토큰 만료였다. kops 명령어를 통해 자격 증명을 재발급(kops export kubeconfig) 받을 수 있다하여 시도해보았지만, 쿠버네티스 유저가 없어서 그런지 접근 토큰이 null 로 발급되었다. kops 클러스터 관리 저장소인 s3에도 접근 토큰 정보가 없었다. 필자는 마스터 노드에 접근(~/.kube/config)하여 토큰을 복사하니 해결하였다.\n마치며 kops 간단하다! 특히 마스터 노드를 직접 접근하여 컨트롤할 수 있어 클러스터 동작 이해에 좋은 툴이였다. 공식문서도 참고할 것이 많다. 공식 문서 kops operation 에 재밌는 주제들(카펜터)이 많던데 얼른 테스트해보고 싶은 마음이다. 시간나는대로 정리해서 올려보겠다.\n","date":"Mar 10","permalink":"https://HanHoRang31.github.io/post/pkos2-1-kops/","tags":["KANS","kops","cloud","AWS","kubernetes"],"title":"PKOS 2기 1주차 - kops"},{"categories":null,"contents":"","date":"Jan 01","permalink":"https://HanHoRang31.github.io/articles/","tags":null,"title":"Articles"}]